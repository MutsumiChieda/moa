{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../script/')\n",
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gauss_rank_scaler import GaussRankScaler\n",
    "# from adabelief_pytorch import AdaBelief\n",
    "\n",
    "import utils\n",
    "import models\n",
    "import train as trainer\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", DEVICE)\n",
    "EPOCHS = 3000\n",
    "MODELNAME = \"ResNet1129_RG\"\n",
    "if not exists(MODELNAME):\n",
    "    os.makedirs(f\"{MODELNAME}/tensorboard\")\n",
    "now = datetime.now()\n",
    "now = str(now)[5:17].replace(\" \", \"_\").replace(\":\", \"\")\n",
    "writer = SummaryWriter(log_dir=f\"{MODELNAME}/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/folds/train.csv\")\n",
    "with open(\"../input/folds/targets\", \"r\") as f:\n",
    "    targets = f.read().split(\"\\n\")\n",
    "with open(\"../input/folds/features\", \"r\") as f:\n",
    "    features = f.read().split(\"\\n\")\n",
    "targets = targets[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>...</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "      <th>kfold</th>\n",
       "      <th>nsc_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>-1.9590</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>-0.2800</td>\n",
       "      <td>-0.1498</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1086 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  cp_time     g-0     g-1     g-2     g-3     g-4     g-5  \\\n",
       "0  id_000644bb2      0.0  1.0620  0.5577 -0.2479 -0.6208 -0.1944 -1.0120   \n",
       "1  id_000779bfc      1.0  0.0743  0.4087  0.2991  0.0604  1.0190  0.5207   \n",
       "2  id_000a6266a      0.5  0.6280  0.5817  1.5540 -0.0764 -0.0323  1.2390   \n",
       "3  id_0015fd391      0.5 -0.5138 -0.2491 -0.2656  0.5288  4.0620 -0.8095   \n",
       "4  id_001626bd3      1.0 -0.3254 -0.4009  0.9700  0.6919  1.4180 -0.8244   \n",
       "\n",
       "      g-6     g-7  ...  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0 -1.0220 -0.0326  ...                0                  0   \n",
       "1  0.2341  0.3372  ...                0                  0   \n",
       "2  0.1715  0.2155  ...                0                  0   \n",
       "3 -1.9590  0.1792  ...                0                  0   \n",
       "4 -0.2800 -0.1498  ...                0                  0   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                          0                                      0   \n",
       "1                          0                                      0   \n",
       "2                          0                                      0   \n",
       "3                          0                                      0   \n",
       "4                          0                                      0   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \\\n",
       "0                0          0                           0              0   \n",
       "1                0          0                           0              0   \n",
       "2                0          0                           0              0   \n",
       "3                0          0                           0              0   \n",
       "4                0          0                           0              0   \n",
       "\n",
       "   kfold  nsc_labels  \n",
       "0      4           0  \n",
       "1      2           0  \n",
       "2      4           0  \n",
       "3      0           0  \n",
       "4      1           0  \n",
       "\n",
       "[5 rows x 1086 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: クラスタごとに並べてConv1d\n",
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, num_layers=3, dropout=.2, hidden_size=256, activation=\"relu\", batchnorm=True, weight_norm=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            if weight_norm:\n",
    "                layers.append(nn.utils.weight_norm(nn.Linear(num_features if len(layers)==0 else hidden_size, hidden_size, bias=(not batchnorm))))\n",
    "            else:\n",
    "                layers.append(nn.Linear(num_features if len(layers)==0 else hidden_size, hidden_size, bias=(not batchnorm)))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            if activation == \"relu\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == \"prelu\":\n",
    "                layers.append(nn.PReLU())\n",
    "            else:\n",
    "                raise RuntimeError(f'{activation} is not implemented')\n",
    "        # layers.append(nn.utils.weight_norm(nn.Linear(hidden_size, num_targets)))\n",
    "        layers.append(nn.Linear(hidden_size, num_targets))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, num_layers=3, dropout=.2, hidden_size=256, activation=\"relu\", batchnorm=True, weight_norm=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if weight_norm:\n",
    "            self.fc1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size, bias=(not batchnorm)))\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(num_features, hidden_size, bias=(not batchnorm))\n",
    "        if batchnorm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        if activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == \"prelu\":\n",
    "            self.activation = nn.PReLU()\n",
    "        else:\n",
    "            raise RuntimeError(f'{activation} is not implemented')\n",
    "        \n",
    "        self.shortcut = nn.Sequential(*[nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size, bias=(not batchnorm))),\n",
    "                                      nn.Dropout(dropout),\n",
    "                                      self.activation])\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(num_layers-1):\n",
    "            if weight_norm:\n",
    "                layers.append(nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size, bias=(not batchnorm))))\n",
    "            else:\n",
    "                layers.append(nn.Linear(num_features if len(layers)==0 else hidden_size, hidden_size, bias=(not batchnorm)))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            if activation == \"relu\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == \"prelu\":\n",
    "                layers.append(nn.PReLU())\n",
    "            else:\n",
    "                raise RuntimeError(f'{activation} is not implemented')\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # layers.append(nn.utils.weight_norm(nn.Linear(hidden_size, num_targets)))\n",
    "        self.fc_out = nn.Linear(hidden_size, num_targets)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = self.dropout1(h)\n",
    "        h = self.activation(h)\n",
    "        \n",
    "        shortcut = self.shortcut(h)\n",
    "        h = self.encoder(h)\n",
    "        \n",
    "        h = self.activation(h + shortcut)\n",
    "        y = self.fc_out(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_output_bias(model, df, targets):   \n",
    "    init_bias = np.array([])\n",
    "    for target in targets:\n",
    "        try:\n",
    "            neg, pos = np.bincount(df[target])\n",
    "        except ValueError:\n",
    "            neg, pos = np.array([df.shape[0], 0.01])\n",
    "        init_bias_ = np.log([pos/neg])\n",
    "        init_bias = np.append(init_bias, init_bias_)\n",
    "    model.fc_out.bias.data = torch.tensor(init_bias, dtype=torch.float32)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "    def forward(self, x, target, smoothing=0.2):\n",
    "        confidence = 1. - smoothing\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "from utils import LabelSmoothingCrossEntropy\n",
    "\n",
    "# criterion = LabelSmoothingCrossEntropy()\n",
    "# loss = criterion(outputs, targets)\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rankgauss(df):\n",
    "    print(f'Applying RankGauss')\n",
    "    cols_numeric = [f'g-{g}' for g in range(772)] + [f'c-{c}' for c in range(100)]\n",
    "    cols_keys = set(df.columns) - set(cols_numeric)\n",
    "    mask = (df[cols_numeric].var() >= 0.7).values\n",
    "    tmp = df[cols_numeric].loc[:, mask]\n",
    "    df = pd.concat([df[cols_keys], tmp], axis=1)\n",
    "    cols_numeric = [feat for feat in list(df.columns) if feat not in cols_keys]\n",
    "    scaler = GaussRankScaler()\n",
    "    df[cols_numeric] = scaler.fit_transform(df[cols_numeric])\n",
    "    cols_keys = list(cols_keys - set(['sig_id']) - set(targets))\n",
    "    feats = cols_keys + cols_numeric\n",
    "    feats.remove('nsc_labels')\n",
    "    feats.remove('kfold')\n",
    "    return df, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, fold, params, hp_tune=False):\n",
    "    \n",
    "    df, features_ = apply_rankgauss(df)\n",
    "    \n",
    "    save_model = False if hp_tune else True\n",
    "    print(f'\\n[Fold No.{fold:>3}]')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    x_tr = train_df[features_].to_numpy()\n",
    "    x_va = valid_df[features_].to_numpy()\n",
    "\n",
    "    y_tr = train_df[targets].to_numpy()\n",
    "    y_va = valid_df[targets].to_numpy()\n",
    "\n",
    "    dataset_tr = utils.MoaDataset(x_tr, y_tr)\n",
    "    loader_tr = torch.utils.data.DataLoader(dataset_tr, batch_size=512, num_workers=2, pin_memory=True)\n",
    "    dataset_va = utils.MoaDataset(x_va, y_va)\n",
    "    loader_va = torch.utils.data.DataLoader(dataset_va, batch_size=512, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = ResNet(num_features=x_tr.shape[1], num_targets=y_tr.shape[1], **params['nn_params'])\n",
    "    model = set_output_bias(model, train_df, targets)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    if params[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), **params[\"optim_params\"])\n",
    "    elif params[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), **params[\"optim_params\"])\n",
    "    elif params[\"optimizer\"] == \"AdamW\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), **params[\"optim_params\"])\n",
    "    elif params[\"optimizer\"] == \"AdaBelief\":\n",
    "        optimizer = AdaBelief(model.parameters(), **params[\"optim_params\"])\n",
    "    else:\n",
    "        raise RuntimeError(f'{params[\"optimizer\"]} is not implemented')\n",
    "\n",
    "    if params[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", **params[\"scdl_params\"])\n",
    "    elif params[\"scheduler\"] == \"CosineAnnealingLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, **params[\"scdl_params\"])\n",
    "    elif params[\"scheduler\"] == \"none\": \n",
    "        print(\"No scheduling will be applied\")\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda ep: 1**ep)\n",
    "    else:\n",
    "        raise RuntimeError(f'{params[\"scheduler\"]} is not implemented')\n",
    "\n",
    "    eng = utils.Engine(model, optimizer, device=DEVICE)\n",
    "\n",
    "    del df, train_df, valid_df, x_tr, x_va, y_tr, y_va\n",
    "    gc.collect()\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    print(f'Training state is shown in {MODELNAME}/tensorboard')\n",
    "    filename = f\"{MODELNAME}/{now}_fold{fold}.pt\"\n",
    "\n",
    "    loss_best = np.inf\n",
    "    patience = 25\n",
    "    patience_cnt = 0\n",
    "    for ep in range(EPOCHS):\n",
    "        loss_tr = eng.train(loader_tr)\n",
    "        loss_tr_nodrop = eng.validate(loader_tr)\n",
    "        loss_va = eng.validate(loader_va)\n",
    "        scheduler.step(loss_va)\n",
    "        print(f'Ep.{ep:>3}/{EPOCHS:>3}, patience:{patience_cnt:>2}/{patience:>2}, train:{loss_tr:.6}, tr_nodrop:{loss_tr_nodrop:.6}, valid:{loss_va:.6}', end='\\r')\n",
    "        writer.add_scalars(f'{now}/fold{fold}', {'train':loss_tr, 'tr_nodrop':loss_tr_nodrop, 'valid':loss_va}, ep)\n",
    "        if loss_va < loss_best:\n",
    "            patience_cnt = 0\n",
    "            loss_best = loss_va\n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), filename)\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "        if patience_cnt > patience:\n",
    "            break\n",
    "\n",
    "    print(\"\\nmodel saved at:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"nn_params\": {\"dropout\": 0.5, \"num_layers\":4, \"hidden_size\": 512, \"activation\": \"relu\", \"batchnorm\": True, \"weight_norm\": True},\n",
    "    \"optimizer\": \"Adam\",\n",
    "    # # SGD\n",
    "    # \"optim_params\": {\"lr\":1e-4, \"momentum\": 0.3, \"weight_decay\": 0.2, \"dampening\": 0, \"nesterov\": False},\n",
    "    # Adam\n",
    "    \"optim_params\": {\"lr\":1e-2, \"betas\": (0.9, 0.999), \"eps\": 1e-08, \"weight_decay\": 1.2e-6, \"amsgrad\": False},\n",
    "    # # Adabelief \n",
    "    # \"optim_params\": {\"lr\": 1e-2, \"eps\":1e-16, \"betas\": (0.9,0.999), \"weight_decay\": 1.2e-6, \"weight_decouple\": False, \"rectify\": True, \"fixed_decay\": False, \"amsgrad\": False},\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"scdl_params\": {\"threshold\": 1e-5, \"patience\": 3}\n",
    "    # # CosineAnnealingLR\n",
    "    # \"scdl_params\": {\"T_max\":8, \"eta_min\":0, \"last_epoch\":-1}\n",
    "}\n",
    "# 0.02355, 0.03 on momentum:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELNAME}/{now}_params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying RankGauss\n",
      "\n",
      "[Fold No.  0]\n",
      "Training state is shown in ResNet1129_RG/tensorboard\n",
      "Ep. 44/3000, patience:25/25, train:0.0122544, tr_nodrop:0.0110445, valid:0.0166974\n",
      "model saved at: ResNet1129_RG/11-30_0323_fold0.pt\n",
      "Applying RankGauss\n",
      "\n",
      "[Fold No.  1]\n",
      "Training state is shown in ResNet1129_RG/tensorboard\n",
      "Ep. 44/3000, patience:25/25, train:0.0125354, tr_nodrop:0.0113435, valid:0.0164428\n",
      "model saved at: ResNet1129_RG/11-30_0323_fold1.pt\n",
      "Applying RankGauss\n",
      "\n",
      "[Fold No.  2]\n",
      "Training state is shown in ResNet1129_RG/tensorboard\n",
      "Ep. 42/3000, patience:25/25, train:0.0126981, tr_nodrop:0.0115333, valid:0.0164527\n",
      "model saved at: ResNet1129_RG/11-30_0323_fold2.pt\n",
      "Applying RankGauss\n",
      "\n",
      "[Fold No.  3]\n",
      "Training state is shown in ResNet1129_RG/tensorboard\n",
      "Ep. 41/3000, patience:25/25, train:0.0127156, tr_nodrop:0.0115574, valid:0.0161724\n",
      "model saved at: ResNet1129_RG/11-30_0323_fold3.pt\n",
      "Applying RankGauss\n",
      "\n",
      "[Fold No.  4]\n",
      "Training state is shown in ResNet1129_RG/tensorboard\n",
      "Ep. 44/3000, patience:25/25, train:0.0122596, tr_nodrop:0.0110059, valid:0.0166587\n",
      "model saved at: ResNet1129_RG/11-30_0323_fold4.pt\n",
      "Wall time: 32min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for fold in range(5):\n",
    "    run_training(df, fold, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Get CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold No.  4] Predicting...\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros((df.shape[0], len(targets)))\n",
    "for fold in range(5):\n",
    "    filename = f\"{MODELNAME}/{now}_fold{fold}.pt\"\n",
    "#     df, features_ = apply_rankgauss(df)\n",
    "    print(f'[Fold No.{fold:>3}] Predicting...', end='\\r')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    va_idx = df[df.kfold == fold].index\n",
    "    \n",
    "    x_tr = train_df[features_].to_numpy()\n",
    "    x_va = valid_df[features_].to_numpy()\n",
    "\n",
    "    y_tr = train_df[targets].to_numpy()\n",
    "    y_va = valid_df[targets].to_numpy()\n",
    "\n",
    "    dataset_tr = utils.MoaDataset(x_tr, y_tr)\n",
    "    loader_tr = torch.utils.data.DataLoader(dataset_tr, batch_size=512, num_workers=2, pin_memory=True)\n",
    "    dataset_va = utils.MoaDataset(x_va, y_va)\n",
    "    loader_va = torch.utils.data.DataLoader(dataset_va, batch_size=512, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    model = ResNet(num_features=x_tr.shape[1], num_targets=y_tr.shape[1], **params['nn_params'])\n",
    "    \n",
    "    weight = torch.load(filename, map_location=torch.device(DEVICE))\n",
    "    model.load_state_dict(weight)\n",
    "    \n",
    "    model.eval()\n",
    "    ps = []\n",
    "    for ind, batch in enumerate(loader_va):\n",
    "        ps.append(torch.sigmoid(model(batch[\"x\"])).detach().cpu().numpy())\n",
    "    ps = np.vstack(ps)\n",
    "    predictions[va_idx] += ps\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score               : 0.0161744\n"
     ]
    }
   ],
   "source": [
    "def log_loss_metric(y_true, y_pred):\n",
    "    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = - np.mean(np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip), axis = 1))\n",
    "    return loss\n",
    "print(f'CV score               : {log_loss_metric(df[targets].values, predictions):.6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "oof.iloc[:, 1:] = predictions\n",
    "oof.to_csv(f\"{MODELNAME}/oof.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score w/ postprocess: 0.01604\n"
     ]
    }
   ],
   "source": [
    "predictions_ = predictions.copy()\n",
    "predictions_ = np.clip(predictions_,0.0005,0.999)\n",
    "predictions_[df[\"cp_type_ctl_vehicle\"]==1] = 0\n",
    "print(f'CV score w/ postprocess: {log_loss_metric(df[targets].values, predictions_):.6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp_dose_D2',\n",
       " 'cp_type_ctl_vehicle',\n",
       " 'cp_type_trt_cp',\n",
       " 'cp_time',\n",
       " 'cp_dose_D1',\n",
       " 'g-0',\n",
       " 'g-2',\n",
       " 'g-3',\n",
       " 'g-4',\n",
       " 'g-5',\n",
       " 'g-6',\n",
       " 'g-7',\n",
       " 'g-8',\n",
       " 'g-9',\n",
       " 'g-10',\n",
       " 'g-11',\n",
       " 'g-12',\n",
       " 'g-13',\n",
       " 'g-14',\n",
       " 'g-16',\n",
       " 'g-17',\n",
       " 'g-19',\n",
       " 'g-20',\n",
       " 'g-22',\n",
       " 'g-24',\n",
       " 'g-25',\n",
       " 'g-26',\n",
       " 'g-27',\n",
       " 'g-28',\n",
       " 'g-29',\n",
       " 'g-30',\n",
       " 'g-31',\n",
       " 'g-32',\n",
       " 'g-33',\n",
       " 'g-34',\n",
       " 'g-35',\n",
       " 'g-36',\n",
       " 'g-37',\n",
       " 'g-38',\n",
       " 'g-39',\n",
       " 'g-40',\n",
       " 'g-41',\n",
       " 'g-42',\n",
       " 'g-43',\n",
       " 'g-44',\n",
       " 'g-45',\n",
       " 'g-46',\n",
       " 'g-47',\n",
       " 'g-48',\n",
       " 'g-49',\n",
       " 'g-50',\n",
       " 'g-51',\n",
       " 'g-52',\n",
       " 'g-53',\n",
       " 'g-54',\n",
       " 'g-55',\n",
       " 'g-56',\n",
       " 'g-57',\n",
       " 'g-58',\n",
       " 'g-59',\n",
       " 'g-60',\n",
       " 'g-61',\n",
       " 'g-62',\n",
       " 'g-63',\n",
       " 'g-64',\n",
       " 'g-65',\n",
       " 'g-66',\n",
       " 'g-67',\n",
       " 'g-68',\n",
       " 'g-69',\n",
       " 'g-70',\n",
       " 'g-71',\n",
       " 'g-72',\n",
       " 'g-73',\n",
       " 'g-74',\n",
       " 'g-75',\n",
       " 'g-76',\n",
       " 'g-77',\n",
       " 'g-78',\n",
       " 'g-79',\n",
       " 'g-80',\n",
       " 'g-81',\n",
       " 'g-82',\n",
       " 'g-83',\n",
       " 'g-84',\n",
       " 'g-85',\n",
       " 'g-86',\n",
       " 'g-87',\n",
       " 'g-88',\n",
       " 'g-89',\n",
       " 'g-90',\n",
       " 'g-91',\n",
       " 'g-92',\n",
       " 'g-93',\n",
       " 'g-95',\n",
       " 'g-96',\n",
       " 'g-97',\n",
       " 'g-98',\n",
       " 'g-99',\n",
       " 'g-100',\n",
       " 'g-101',\n",
       " 'g-102',\n",
       " 'g-103',\n",
       " 'g-105',\n",
       " 'g-106',\n",
       " 'g-107',\n",
       " 'g-108',\n",
       " 'g-109',\n",
       " 'g-110',\n",
       " 'g-111',\n",
       " 'g-112',\n",
       " 'g-113',\n",
       " 'g-114',\n",
       " 'g-115',\n",
       " 'g-116',\n",
       " 'g-117',\n",
       " 'g-118',\n",
       " 'g-119',\n",
       " 'g-120',\n",
       " 'g-121',\n",
       " 'g-122',\n",
       " 'g-123',\n",
       " 'g-124',\n",
       " 'g-125',\n",
       " 'g-126',\n",
       " 'g-127',\n",
       " 'g-128',\n",
       " 'g-129',\n",
       " 'g-130',\n",
       " 'g-131',\n",
       " 'g-132',\n",
       " 'g-133',\n",
       " 'g-134',\n",
       " 'g-135',\n",
       " 'g-136',\n",
       " 'g-137',\n",
       " 'g-138',\n",
       " 'g-139',\n",
       " 'g-140',\n",
       " 'g-141',\n",
       " 'g-142',\n",
       " 'g-143',\n",
       " 'g-144',\n",
       " 'g-145',\n",
       " 'g-146',\n",
       " 'g-147',\n",
       " 'g-148',\n",
       " 'g-149',\n",
       " 'g-150',\n",
       " 'g-151',\n",
       " 'g-152',\n",
       " 'g-154',\n",
       " 'g-155',\n",
       " 'g-156',\n",
       " 'g-157',\n",
       " 'g-158',\n",
       " 'g-159',\n",
       " 'g-160',\n",
       " 'g-161',\n",
       " 'g-162',\n",
       " 'g-163',\n",
       " 'g-164',\n",
       " 'g-165',\n",
       " 'g-166',\n",
       " 'g-167',\n",
       " 'g-168',\n",
       " 'g-169',\n",
       " 'g-170',\n",
       " 'g-171',\n",
       " 'g-172',\n",
       " 'g-173',\n",
       " 'g-174',\n",
       " 'g-175',\n",
       " 'g-176',\n",
       " 'g-177',\n",
       " 'g-178',\n",
       " 'g-179',\n",
       " 'g-180',\n",
       " 'g-181',\n",
       " 'g-182',\n",
       " 'g-183',\n",
       " 'g-185',\n",
       " 'g-186',\n",
       " 'g-187',\n",
       " 'g-188',\n",
       " 'g-189',\n",
       " 'g-190',\n",
       " 'g-191',\n",
       " 'g-192',\n",
       " 'g-193',\n",
       " 'g-194',\n",
       " 'g-195',\n",
       " 'g-196',\n",
       " 'g-197',\n",
       " 'g-198',\n",
       " 'g-199',\n",
       " 'g-200',\n",
       " 'g-201',\n",
       " 'g-202',\n",
       " 'g-203',\n",
       " 'g-204',\n",
       " 'g-205',\n",
       " 'g-206',\n",
       " 'g-207',\n",
       " 'g-208',\n",
       " 'g-209',\n",
       " 'g-210',\n",
       " 'g-211',\n",
       " 'g-212',\n",
       " 'g-213',\n",
       " 'g-214',\n",
       " 'g-215',\n",
       " 'g-217',\n",
       " 'g-218',\n",
       " 'g-220',\n",
       " 'g-221',\n",
       " 'g-222',\n",
       " 'g-223',\n",
       " 'g-224',\n",
       " 'g-225',\n",
       " 'g-226',\n",
       " 'g-227',\n",
       " 'g-228',\n",
       " 'g-229',\n",
       " 'g-230',\n",
       " 'g-231',\n",
       " 'g-232',\n",
       " 'g-233',\n",
       " 'g-234',\n",
       " 'g-235',\n",
       " 'g-236',\n",
       " 'g-237',\n",
       " 'g-238',\n",
       " 'g-239',\n",
       " 'g-240',\n",
       " 'g-241',\n",
       " 'g-242',\n",
       " 'g-243',\n",
       " 'g-244',\n",
       " 'g-245',\n",
       " 'g-246',\n",
       " 'g-247',\n",
       " 'g-248',\n",
       " 'g-249',\n",
       " 'g-250',\n",
       " 'g-251',\n",
       " 'g-252',\n",
       " 'g-253',\n",
       " 'g-254',\n",
       " 'g-255',\n",
       " 'g-256',\n",
       " 'g-257',\n",
       " 'g-258',\n",
       " 'g-259',\n",
       " 'g-260',\n",
       " 'g-261',\n",
       " 'g-262',\n",
       " 'g-263',\n",
       " 'g-264',\n",
       " 'g-265',\n",
       " 'g-266',\n",
       " 'g-268',\n",
       " 'g-269',\n",
       " 'g-270',\n",
       " 'g-271',\n",
       " 'g-272',\n",
       " 'g-273',\n",
       " 'g-274',\n",
       " 'g-275',\n",
       " 'g-276',\n",
       " 'g-277',\n",
       " 'g-278',\n",
       " 'g-280',\n",
       " 'g-281',\n",
       " 'g-282',\n",
       " 'g-283',\n",
       " 'g-284',\n",
       " 'g-285',\n",
       " 'g-286',\n",
       " 'g-287',\n",
       " 'g-288',\n",
       " 'g-289',\n",
       " 'g-290',\n",
       " 'g-291',\n",
       " 'g-293',\n",
       " 'g-294',\n",
       " 'g-295',\n",
       " 'g-296',\n",
       " 'g-297',\n",
       " 'g-298',\n",
       " 'g-299',\n",
       " 'g-300',\n",
       " 'g-301',\n",
       " 'g-302',\n",
       " 'g-304',\n",
       " 'g-305',\n",
       " 'g-306',\n",
       " 'g-308',\n",
       " 'g-309',\n",
       " 'g-310',\n",
       " 'g-311',\n",
       " 'g-312',\n",
       " 'g-313',\n",
       " 'g-314',\n",
       " 'g-315',\n",
       " 'g-316',\n",
       " 'g-317',\n",
       " 'g-318',\n",
       " 'g-319',\n",
       " 'g-320',\n",
       " 'g-321',\n",
       " 'g-322',\n",
       " 'g-323',\n",
       " 'g-324',\n",
       " 'g-325',\n",
       " 'g-326',\n",
       " 'g-327',\n",
       " 'g-328',\n",
       " 'g-329',\n",
       " 'g-330',\n",
       " 'g-332',\n",
       " 'g-333',\n",
       " 'g-334',\n",
       " 'g-335',\n",
       " 'g-336',\n",
       " 'g-337',\n",
       " 'g-338',\n",
       " 'g-339',\n",
       " 'g-341',\n",
       " 'g-342',\n",
       " 'g-343',\n",
       " 'g-344',\n",
       " 'g-345',\n",
       " 'g-346',\n",
       " 'g-347',\n",
       " 'g-348',\n",
       " 'g-349',\n",
       " 'g-350',\n",
       " 'g-351',\n",
       " 'g-352',\n",
       " 'g-353',\n",
       " 'g-354',\n",
       " 'g-355',\n",
       " 'g-356',\n",
       " 'g-357',\n",
       " 'g-358',\n",
       " 'g-359',\n",
       " 'g-360',\n",
       " 'g-361',\n",
       " 'g-362',\n",
       " 'g-363',\n",
       " 'g-364',\n",
       " 'g-365',\n",
       " 'g-366',\n",
       " 'g-367',\n",
       " 'g-368',\n",
       " 'g-369',\n",
       " 'g-370',\n",
       " 'g-371',\n",
       " 'g-372',\n",
       " 'g-373',\n",
       " 'g-374',\n",
       " 'g-375',\n",
       " 'g-376',\n",
       " 'g-377',\n",
       " 'g-378',\n",
       " 'g-379',\n",
       " 'g-380',\n",
       " 'g-381',\n",
       " 'g-383',\n",
       " 'g-384',\n",
       " 'g-385',\n",
       " 'g-386',\n",
       " 'g-387',\n",
       " 'g-388',\n",
       " 'g-389',\n",
       " 'g-390',\n",
       " 'g-391',\n",
       " 'g-392',\n",
       " 'g-393',\n",
       " 'g-394',\n",
       " 'g-395',\n",
       " 'g-396',\n",
       " 'g-397',\n",
       " 'g-398',\n",
       " 'g-399',\n",
       " 'g-400',\n",
       " 'g-401',\n",
       " 'g-402',\n",
       " 'g-403',\n",
       " 'g-404',\n",
       " 'g-405',\n",
       " 'g-406',\n",
       " 'g-407',\n",
       " 'g-408',\n",
       " 'g-409',\n",
       " 'g-410',\n",
       " 'g-411',\n",
       " 'g-412',\n",
       " 'g-413',\n",
       " 'g-414',\n",
       " 'g-415',\n",
       " 'g-416',\n",
       " 'g-417',\n",
       " 'g-418',\n",
       " 'g-419',\n",
       " 'g-421',\n",
       " 'g-422',\n",
       " 'g-423',\n",
       " 'g-424',\n",
       " 'g-425',\n",
       " 'g-426',\n",
       " 'g-427',\n",
       " 'g-428',\n",
       " 'g-429',\n",
       " 'g-430',\n",
       " 'g-431',\n",
       " 'g-432',\n",
       " 'g-433',\n",
       " 'g-434',\n",
       " 'g-436',\n",
       " 'g-437',\n",
       " 'g-438',\n",
       " 'g-439',\n",
       " 'g-440',\n",
       " 'g-441',\n",
       " 'g-442',\n",
       " 'g-443',\n",
       " 'g-444',\n",
       " 'g-445',\n",
       " 'g-446',\n",
       " 'g-447',\n",
       " 'g-448',\n",
       " 'g-449',\n",
       " 'g-450',\n",
       " 'g-451',\n",
       " 'g-452',\n",
       " 'g-453',\n",
       " 'g-454',\n",
       " 'g-455',\n",
       " 'g-456',\n",
       " 'g-457',\n",
       " 'g-458',\n",
       " 'g-459',\n",
       " 'g-460',\n",
       " 'g-461',\n",
       " 'g-462',\n",
       " 'g-463',\n",
       " 'g-464',\n",
       " 'g-465',\n",
       " 'g-466',\n",
       " 'g-467',\n",
       " 'g-468',\n",
       " 'g-469',\n",
       " 'g-470',\n",
       " 'g-471',\n",
       " 'g-472',\n",
       " 'g-473',\n",
       " 'g-474',\n",
       " 'g-475',\n",
       " 'g-476',\n",
       " 'g-477',\n",
       " 'g-478',\n",
       " 'g-479',\n",
       " 'g-480',\n",
       " 'g-482',\n",
       " 'g-483',\n",
       " 'g-484',\n",
       " 'g-485',\n",
       " 'g-486',\n",
       " 'g-487',\n",
       " 'g-488',\n",
       " 'g-489',\n",
       " 'g-490',\n",
       " 'g-491',\n",
       " 'g-492',\n",
       " 'g-493',\n",
       " 'g-494',\n",
       " 'g-495',\n",
       " 'g-496',\n",
       " 'g-497',\n",
       " 'g-498',\n",
       " 'g-499',\n",
       " 'g-500',\n",
       " 'g-501',\n",
       " 'g-502',\n",
       " 'g-503',\n",
       " 'g-504',\n",
       " 'g-505',\n",
       " 'g-506',\n",
       " 'g-508',\n",
       " 'g-509',\n",
       " 'g-510',\n",
       " 'g-511',\n",
       " 'g-512',\n",
       " 'g-513',\n",
       " 'g-514',\n",
       " 'g-515',\n",
       " 'g-516',\n",
       " 'g-517',\n",
       " 'g-518',\n",
       " 'g-519',\n",
       " 'g-520',\n",
       " 'g-521',\n",
       " 'g-522',\n",
       " 'g-523',\n",
       " 'g-524',\n",
       " 'g-525',\n",
       " 'g-526',\n",
       " 'g-527',\n",
       " 'g-528',\n",
       " 'g-529',\n",
       " 'g-530',\n",
       " 'g-531',\n",
       " 'g-532',\n",
       " 'g-533',\n",
       " 'g-534',\n",
       " 'g-535',\n",
       " 'g-537',\n",
       " 'g-538',\n",
       " 'g-539',\n",
       " 'g-540',\n",
       " 'g-541',\n",
       " 'g-542',\n",
       " 'g-543',\n",
       " 'g-544',\n",
       " 'g-545',\n",
       " 'g-546',\n",
       " 'g-547',\n",
       " 'g-548',\n",
       " 'g-549',\n",
       " 'g-551',\n",
       " 'g-553',\n",
       " 'g-554',\n",
       " 'g-555',\n",
       " 'g-556',\n",
       " 'g-557',\n",
       " 'g-558',\n",
       " 'g-559',\n",
       " 'g-561',\n",
       " 'g-562',\n",
       " 'g-563',\n",
       " 'g-564',\n",
       " 'g-565',\n",
       " 'g-566',\n",
       " 'g-567',\n",
       " 'g-568',\n",
       " 'g-569',\n",
       " 'g-570',\n",
       " 'g-571',\n",
       " 'g-572',\n",
       " 'g-573',\n",
       " 'g-574',\n",
       " 'g-575',\n",
       " 'g-576',\n",
       " 'g-577',\n",
       " 'g-578',\n",
       " 'g-579',\n",
       " 'g-580',\n",
       " 'g-581',\n",
       " 'g-582',\n",
       " 'g-584',\n",
       " 'g-585',\n",
       " 'g-586',\n",
       " 'g-587',\n",
       " 'g-588',\n",
       " 'g-589',\n",
       " 'g-590',\n",
       " 'g-591',\n",
       " 'g-592',\n",
       " 'g-593',\n",
       " 'g-594',\n",
       " 'g-595',\n",
       " 'g-596',\n",
       " 'g-597',\n",
       " 'g-598',\n",
       " 'g-599',\n",
       " 'g-600',\n",
       " 'g-601',\n",
       " 'g-602',\n",
       " 'g-603',\n",
       " 'g-604',\n",
       " 'g-605',\n",
       " 'g-606',\n",
       " 'g-607',\n",
       " 'g-608',\n",
       " 'g-609',\n",
       " 'g-610',\n",
       " 'g-612',\n",
       " 'g-613',\n",
       " 'g-614',\n",
       " 'g-615',\n",
       " 'g-616',\n",
       " 'g-617',\n",
       " 'g-618',\n",
       " 'g-619',\n",
       " 'g-620',\n",
       " 'g-621',\n",
       " 'g-622',\n",
       " 'g-623',\n",
       " 'g-624',\n",
       " 'g-625',\n",
       " 'g-626',\n",
       " 'g-627',\n",
       " 'g-628',\n",
       " 'g-629',\n",
       " 'g-630',\n",
       " 'g-631',\n",
       " 'g-632',\n",
       " 'g-633',\n",
       " 'g-634',\n",
       " 'g-635',\n",
       " 'g-636',\n",
       " 'g-637',\n",
       " 'g-638',\n",
       " 'g-639',\n",
       " 'g-640',\n",
       " 'g-641',\n",
       " 'g-642',\n",
       " 'g-643',\n",
       " 'g-644',\n",
       " 'g-645',\n",
       " 'g-646',\n",
       " 'g-647',\n",
       " 'g-648',\n",
       " 'g-649',\n",
       " 'g-650',\n",
       " 'g-651',\n",
       " 'g-652',\n",
       " 'g-653',\n",
       " 'g-654',\n",
       " 'g-655',\n",
       " 'g-656',\n",
       " 'g-657',\n",
       " 'g-658',\n",
       " 'g-659',\n",
       " 'g-660',\n",
       " 'g-661',\n",
       " 'g-662',\n",
       " 'g-663',\n",
       " 'g-664',\n",
       " 'g-665',\n",
       " 'g-666',\n",
       " 'g-667',\n",
       " 'g-668',\n",
       " 'g-669',\n",
       " 'g-670',\n",
       " 'g-671',\n",
       " 'g-672',\n",
       " 'g-673',\n",
       " 'g-674',\n",
       " 'g-675',\n",
       " 'g-676',\n",
       " 'g-677',\n",
       " 'g-678',\n",
       " 'g-679',\n",
       " 'g-680',\n",
       " 'g-681',\n",
       " 'g-682',\n",
       " 'g-683',\n",
       " 'g-684',\n",
       " 'g-685',\n",
       " 'g-686',\n",
       " 'g-688',\n",
       " 'g-689',\n",
       " 'g-690',\n",
       " 'g-691',\n",
       " 'g-692',\n",
       " 'g-693',\n",
       " 'g-694',\n",
       " 'g-695',\n",
       " 'g-696',\n",
       " 'g-697',\n",
       " 'g-698',\n",
       " 'g-699',\n",
       " 'g-700',\n",
       " 'g-701',\n",
       " 'g-702',\n",
       " 'g-703',\n",
       " 'g-704',\n",
       " 'g-705',\n",
       " 'g-706',\n",
       " 'g-707',\n",
       " 'g-708',\n",
       " 'g-709',\n",
       " 'g-710',\n",
       " 'g-711',\n",
       " 'g-712',\n",
       " 'g-713',\n",
       " 'g-714',\n",
       " 'g-715',\n",
       " 'g-716',\n",
       " 'g-717',\n",
       " 'g-719',\n",
       " 'g-720',\n",
       " 'g-721',\n",
       " 'g-722',\n",
       " 'g-723',\n",
       " 'g-724',\n",
       " 'g-725',\n",
       " 'g-726',\n",
       " 'g-727',\n",
       " 'g-728',\n",
       " 'g-729',\n",
       " 'g-730',\n",
       " 'g-731',\n",
       " 'g-732',\n",
       " 'g-733',\n",
       " 'g-734',\n",
       " 'g-735',\n",
       " 'g-736',\n",
       " 'g-737',\n",
       " 'g-738',\n",
       " 'g-739',\n",
       " 'g-740',\n",
       " 'g-741',\n",
       " 'g-742',\n",
       " 'g-743',\n",
       " 'g-744',\n",
       " 'g-745',\n",
       " 'g-746',\n",
       " 'g-747',\n",
       " 'g-748',\n",
       " 'g-749',\n",
       " 'g-750',\n",
       " 'g-751',\n",
       " 'g-752',\n",
       " 'g-753',\n",
       " 'g-754',\n",
       " 'g-755',\n",
       " 'g-757',\n",
       " 'g-758',\n",
       " 'g-759',\n",
       " 'g-760',\n",
       " 'g-761',\n",
       " 'g-762',\n",
       " 'g-763',\n",
       " 'g-764',\n",
       " 'g-765',\n",
       " 'g-766',\n",
       " 'g-767',\n",
       " 'g-768',\n",
       " 'g-769',\n",
       " 'g-770',\n",
       " 'g-771',\n",
       " 'c-0',\n",
       " 'c-1',\n",
       " 'c-2',\n",
       " 'c-3',\n",
       " 'c-4',\n",
       " 'c-5',\n",
       " 'c-6',\n",
       " 'c-7',\n",
       " 'c-8',\n",
       " 'c-9',\n",
       " 'c-10',\n",
       " 'c-11',\n",
       " 'c-12',\n",
       " 'c-13',\n",
       " 'c-14',\n",
       " 'c-15',\n",
       " 'c-16',\n",
       " 'c-17',\n",
       " 'c-18',\n",
       " 'c-19',\n",
       " 'c-20',\n",
       " 'c-21',\n",
       " 'c-22',\n",
       " 'c-23',\n",
       " 'c-24',\n",
       " 'c-25',\n",
       " 'c-26',\n",
       " 'c-27',\n",
       " 'c-28',\n",
       " 'c-29',\n",
       " 'c-30',\n",
       " 'c-31',\n",
       " 'c-32',\n",
       " 'c-33',\n",
       " 'c-34',\n",
       " 'c-35',\n",
       " 'c-36',\n",
       " 'c-37',\n",
       " 'c-38',\n",
       " 'c-39',\n",
       " 'c-40',\n",
       " 'c-41',\n",
       " 'c-42',\n",
       " 'c-43',\n",
       " 'c-44',\n",
       " 'c-45',\n",
       " 'c-46',\n",
       " 'c-47',\n",
       " 'c-48',\n",
       " 'c-49',\n",
       " 'c-50',\n",
       " 'c-51',\n",
       " 'c-52',\n",
       " 'c-53',\n",
       " 'c-54',\n",
       " 'c-55',\n",
       " 'c-56',\n",
       " 'c-57',\n",
       " 'c-58',\n",
       " 'c-59',\n",
       " 'c-60',\n",
       " 'c-61',\n",
       " 'c-62',\n",
       " 'c-63',\n",
       " 'c-64',\n",
       " 'c-65',\n",
       " 'c-66',\n",
       " 'c-67',\n",
       " 'c-68',\n",
       " 'c-69',\n",
       " 'c-70',\n",
       " 'c-71',\n",
       " 'c-72',\n",
       " 'c-73',\n",
       " 'c-74',\n",
       " 'c-75',\n",
       " 'c-76',\n",
       " 'c-77',\n",
       " 'c-78',\n",
       " 'c-79',\n",
       " 'c-80',\n",
       " 'c-81',\n",
       " 'c-82',\n",
       " 'c-83',\n",
       " 'c-84',\n",
       " 'c-85',\n",
       " 'c-86',\n",
       " 'c-87',\n",
       " 'c-88',\n",
       " 'c-89',\n",
       " 'c-90',\n",
       " 'c-91',\n",
       " 'c-92',\n",
       " 'c-93',\n",
       " 'c-94',\n",
       " 'c-95',\n",
       " 'c-96',\n",
       " 'c-97',\n",
       " 'c-98',\n",
       " 'c-99']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
