{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../script/')\n",
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "import utils\n",
    "import models\n",
    "import train as trainer\n",
    "DEVICE = \"cuda\"\n",
    "EPOCHS = 1000\n",
    "MODELNAME = \"TabNet1017\"\n",
    "if not exists(f\"{MODELNAME}/scores\"):\n",
    "    os.makedirs(f\"{MODELNAME}/scores\")\n",
    "if not exists(f\"{MODELNAME}/weight\"):\n",
    "    os.makedirs(f\"{MODELNAME}/weight\")\n",
    "now = datetime.now()\n",
    "now = str(now)[5:17].replace(\" \", \"_\").replace(\":\", \"\")\n",
    "# writer = SummaryWriter(log_dir=f\"{MODELNAME}/tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../script/tabnet-develop\")\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from pytorch_tabnet import tab_network\n",
    "from pytorch_tabnet.multiclass_utils import unique_labels\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pytorch_tabnet.utils import (PredictDataset,\n",
    "                                  create_dataloaders,\n",
    "                                  create_explain_matrix)\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import io\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "class TabModel(BaseEstimator):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n",
    "                 lambda_sparse=1e-3, seed=0,\n",
    "                 clip_value=1, verbose=1,\n",
    "                 optimizer_fn=torch.optim.Adam,\n",
    "                 optimizer_params=dict(lr=2e-2),\n",
    "                 scheduler_params=None, scheduler_fn=None,\n",
    "                 mask_type=\"sparsemax\",\n",
    "                 input_dim=None, output_dim=None,\n",
    "                 device_name='auto'):\n",
    "        \"\"\" Class for TabNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "            device_name: str\n",
    "                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.cat_idxs = cat_idxs\n",
    "        self.cat_dims = cat_dims\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.device_name = device_name\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.batch_size = 1024\n",
    "\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        print(f\"Device used : {self.device}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n",
    "                          weights, batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define construct_loaders to use this base class')\n",
    "\n",
    "    def init_network(\n",
    "                     self,\n",
    "                     input_dim,\n",
    "                     output_dim,\n",
    "                     n_d,\n",
    "                     n_a,\n",
    "                     n_steps,\n",
    "                     gamma,\n",
    "                     cat_idxs,\n",
    "                     cat_dims,\n",
    "                     cat_emb_dim,\n",
    "                     n_independent,\n",
    "                     n_shared,\n",
    "                     epsilon,\n",
    "                     virtual_batch_size,\n",
    "                     momentum,\n",
    "                     device_name,\n",
    "                     mask_type,\n",
    "                     ):\n",
    "        self.network = tab_network.TabNet(\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            device_name=device_name,\n",
    "            mask_type=mask_type).to(self.device)\n",
    "\n",
    "        self.reducing_matrix = create_explain_matrix(\n",
    "            self.network.input_dim,\n",
    "            self.network.cat_emb_dim,\n",
    "            self.network.cat_idxs,\n",
    "            self.network.post_embed_dim)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n",
    "            weights=0, max_epochs=100, patience=10, batch_size=1024,\n",
    "            virtual_batch_size=128, num_workers=0, drop_last=False):\n",
    "        \"\"\"Train a neural network stored in self.network\n",
    "        Using train_dataloader for training data and\n",
    "        valid_dataloader for validation.\n",
    "        Parameters\n",
    "        ----------\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            weights : bool or dictionnary\n",
    "                0 for no balancing\n",
    "                1 for automated balancing\n",
    "                dict for custom weights per class\n",
    "            max_epochs : int\n",
    "                Maximum number of epochs during training\n",
    "            patience : int\n",
    "                Number of consecutive non improving epoch before early stopping\n",
    "            batch_size : int\n",
    "                Training batch size\n",
    "            virtual_batch_size : int\n",
    "                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n",
    "            num_workers : int\n",
    "                Number of workers used in torch.utils.data.DataLoader\n",
    "            drop_last : bool\n",
    "                Whether to drop last batch during training\n",
    "        \"\"\"\n",
    "        # update model name\n",
    "\n",
    "        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                               weights, max_epochs, patience, batch_size,\n",
    "                               virtual_batch_size, num_workers, drop_last)\n",
    "\n",
    "        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n",
    "                                                                    y_train,\n",
    "                                                                    X_valid,\n",
    "                                                                    y_valid,\n",
    "                                                                    self.updated_weights,\n",
    "                                                                    self.batch_size,\n",
    "                                                                    self.num_workers,\n",
    "                                                                    self.drop_last)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "\n",
    "        self.optimizer = self.optimizer_fn(self.network.parameters(),\n",
    "                                           **self.optimizer_params)\n",
    "\n",
    "        if self.scheduler_fn:\n",
    "            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "        self.learning_rates = []\n",
    "        self.metrics_train = []\n",
    "        self.metrics_valid = []\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Will train until validation stopping metric\",\n",
    "                  f\"hasn't improved in {self.patience} rounds.\")\n",
    "            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n",
    "            print('---------------------------------------')\n",
    "            print(msg_epoch)\n",
    "\n",
    "        total_time = 0\n",
    "        while (self.epoch < self.max_epochs and\n",
    "               self.patience_counter < self.patience):\n",
    "            starting_time = time.time()\n",
    "            # updates learning rate history\n",
    "            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n",
    "\n",
    "            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n",
    "\n",
    "            # leaving it here, may be used for callbacks later\n",
    "            self.losses_train.append(fit_metrics['train']['loss_avg'])\n",
    "            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n",
    "            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n",
    "            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n",
    "\n",
    "            stopping_loss = fit_metrics['valid']['stopping_loss']\n",
    "            if stopping_loss < self.best_cost:\n",
    "                self.best_cost = stopping_loss\n",
    "                self.patience_counter = 0\n",
    "                # Saving model\n",
    "                self.best_network = deepcopy(self.network)\n",
    "                has_improved = True\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                has_improved=False\n",
    "            self.epoch += 1\n",
    "            total_time += time.time() - starting_time\n",
    "            if self.verbose > 0:\n",
    "                if self.epoch % self.verbose == 0:\n",
    "                    separator = \"|\"\n",
    "                    msg_epoch = f\"| {self.epoch:<5} | \"\n",
    "                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n",
    "                    msg_epoch += f\" {has_improved}\"\n",
    "                    print(msg_epoch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopping occured at epoch {self.epoch}\")\n",
    "            print(f\"Training done in {total_time:.3f} seconds.\")\n",
    "            print('---------------------------------------')\n",
    "\n",
    "        self.history = {\"train\": {\"loss\": self.losses_train,\n",
    "                                  \"metric\": self.metrics_train,\n",
    "                                  \"lr\": self.learning_rates},\n",
    "                        \"valid\": {\"loss\": self.losses_valid,\n",
    "                                  \"metric\": self.metrics_valid}}\n",
    "        # load best models post training\n",
    "        self.load_best_model()\n",
    "\n",
    "        # compute feature importance once the best model is defined\n",
    "        self._compute_feature_importances(train_dataloader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saving model with two distinct files.\n",
    "        \"\"\"\n",
    "        saved_params = {}\n",
    "        for key, val in self.get_params().items():\n",
    "            if isinstance(val, type):\n",
    "                # Don't save torch specific params\n",
    "                continue\n",
    "            else:\n",
    "                saved_params[key] = val\n",
    "\n",
    "        # Create folder\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save models params\n",
    "        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(saved_params, f)\n",
    "\n",
    "        # Save state_dict\n",
    "        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n",
    "        shutil.make_archive(path, 'zip', path)\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Successfully saved model at {path}.zip\")\n",
    "        return f\"{path}.zip\"\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(filepath) as z:\n",
    "                with z.open(\"model_params.json\") as f:\n",
    "                    loaded_params = json.load(f)\n",
    "                with z.open(\"network.pt\") as f:\n",
    "                    try:\n",
    "                        saved_state_dict = torch.load(f)\n",
    "                    except io.UnsupportedOperation:\n",
    "                        # In Python <3.7, the returned file object is not seekable (which at least\n",
    "                        # some versions of PyTorch require) - so we'll try buffering it in to a\n",
    "                        # BytesIO instead:\n",
    "                        saved_state_dict = torch.load(io.BytesIO(f.read()))\n",
    "        except KeyError:\n",
    "            raise KeyError(\"Your zip file is missing at least one component\")\n",
    "\n",
    "        self.__init__(**loaded_params)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=1024,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "        self.network.load_state_dict(saved_state_dict)\n",
    "        self.network.eval()\n",
    "        return\n",
    "\n",
    "    def fit_epoch(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates and updates network for one epoch.\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with valid set\n",
    "        \"\"\"\n",
    "        train_metrics = self.train_epoch(train_dataloader)\n",
    "        valid_metrics = self.predict_epoch(valid_dataloader)\n",
    "\n",
    "        fit_metrics = {'train': train_metrics,\n",
    "                       'valid': valid_metrics}\n",
    "\n",
    "        return fit_metrics\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_batch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_batch to use this base class')\n",
    "\n",
    "    def load_best_model(self):\n",
    "        if self.best_network is not None:\n",
    "            self.network = self.best_network\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem or the last class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict to use this base class')\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Return local explanation\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            M_explain: matrix\n",
    "                Importance per sample, per columns.\n",
    "            masks: matrix\n",
    "                Sparse matrix showing attention masks used by network.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            for key, value in masks.items():\n",
    "                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n",
    "                                            self.reducing_matrix)\n",
    "\n",
    "            if batch_nb == 0:\n",
    "                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                             self.reducing_matrix)\n",
    "                res_masks = masks\n",
    "            else:\n",
    "                res_explain = np.vstack([res_explain,\n",
    "                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                                        self.reducing_matrix)])\n",
    "                for key, value in masks.items():\n",
    "                    res_masks[key] = np.vstack([res_masks[key], value])\n",
    "        return res_explain, res_masks\n",
    "\n",
    "    def _compute_feature_importances(self, loader):\n",
    "        self.network.eval()\n",
    "        feature_importances_ = np.zeros((self.network.post_embed_dim))\n",
    "        for data, targets in loader:\n",
    "            data = data.to(self.device).float()\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n",
    "\n",
    "        feature_importances_ = csc_matrix.dot(feature_importances_,\n",
    "                                              self.reducing_matrix)\n",
    "        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n",
    "        \n",
    "        \n",
    "class TabNetRegressor(TabModel):\n",
    "\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n",
    "                          batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(weights, int):\n",
    "            if weights == 1:\n",
    "                raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "        if isinstance(weights, dict):\n",
    "            raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "\n",
    "        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n",
    "                                                                y_train,\n",
    "                                                                X_valid,\n",
    "                                                                y_valid,\n",
    "                                                                weights,\n",
    "                                                                batch_size,\n",
    "                                                                num_workers,\n",
    "                                                                drop_last)\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                          weights, max_epochs, patience,\n",
    "                          batch_size, virtual_batch_size, num_workers, drop_last):\n",
    "\n",
    "        if loss_fn is None:\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        if len(y_train.shape) == 1:\n",
    "            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n",
    "                                if doing single regression.\"\"\")\n",
    "        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n",
    "        self.output_dim = y_train.shape[1]\n",
    "\n",
    "        self.updated_weights = weights\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        # Initialize counters and histories.\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        self.best_cost = np.inf\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.train()\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            batch_outs = self.train_batch(data, targets)\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n",
    "        total_loss = total_loss / len(train_loader)\n",
    "        epoch_metrics = {'loss_avg': total_loss,\n",
    "                         'stopping_loss': total_loss,\n",
    "                         }\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step(total_loss)\n",
    "        return epoch_metrics\n",
    "\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        data = data.to(self.device).float()\n",
    "\n",
    "        targets = targets.to(self.device).float()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "\n",
    "        loss = self.loss_fn(output, targets)\n",
    "        \n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "\n",
    "        loss.backward()\n",
    "        if self.clip_value:\n",
    "            clip_grad_norm_(self.network.parameters(), self.clip_value)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        self.network.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in loader:\n",
    "            batch_outs = self.predict_batch(data, targets)\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "\n",
    "        total_loss = total_loss / len(loader)\n",
    "        epoch_metrics = {'total_loss': total_loss,\n",
    "                         'stopping_loss': stopping_loss}\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        data = data.to(self.device).float()\n",
    "        targets = targets.to(self.device).float()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "       \n",
    "        loss = self.loss_fn(output, targets)\n",
    "        #print(self.loss_fn, loss)\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "        #print(loss)\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        results = []\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            output, M_loss = self.network(data)\n",
    "            predictions = output.cpu().detach().numpy()\n",
    "            results.append(predictions)\n",
    "        res = np.vstack(results)\n",
    "        return res\n",
    "\n",
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "    \"\"\"\n",
    "    :param predicted:   The predicted probabilities as floats between 0-1\n",
    "    :param actual:      The binary labels. Either 0 or 1.\n",
    "    :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "    :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "    \"\"\"\n",
    "    p1 = actual * np.log(predicted+eps)\n",
    "    p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "    loss = p0 + p1\n",
    "\n",
    "    return -loss.mean()\n",
    "    \n",
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data\n",
      "Skipped: already exists\n"
     ]
    }
   ],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "def split_data():\n",
    "    print(\"Split data\")\n",
    "    path_fold = \"../input/folds/train_folds.csv\"\n",
    "    if not exists(path_fold):\n",
    "        df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
    "        df.loc[:, \"kfold\"] = -1\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        targets = df.drop(\"sig_id\", axis=1).values\n",
    "\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "        for fold_, (tr_, va_) in enumerate(mskf.split(X=df, y=targets)):\n",
    "            df.loc[va_, \"kfold\"] = fold_\n",
    "        df.to_csv(path_fold, index=False)\n",
    "        print(f\"Created: {path_fold}\")\n",
    "    else:\n",
    "        print(\"Skipped: already exists\")\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
    "    \n",
    "    # Label encoding\n",
    "    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n",
    "               \"cp_time\":{24:0, 48:1, 72:2},\n",
    "               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n",
    "    for col in ['cp_type', 'cp_time', 'cp_dose']:\n",
    "        df[col] = df[col].map(mapping[col])\n",
    "    # df = utils.process_data(df)\n",
    "    \n",
    "    folds = pd.read_csv(\"../input/folds/train_folds.csv\")\n",
    "\n",
    "    # Create aux target\n",
    "    # `nsc_labels` means # of labels found in non-scored train set\n",
    "    non_scored_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
    "    targets_non_scored = non_scored_df.drop(\"sig_id\", axis=1).to_numpy().sum(axis=1)\n",
    "    non_scored_df.loc[:, \"nsc_labels\"] = targets_non_scored\n",
    "    drop_cols = [c for c in non_scored_df.columns if c not in (\"nsc_labels\", \"sig_id\")]\n",
    "    non_scored_df = non_scored_df.drop(drop_cols, axis=1)\n",
    "    folds = folds.merge(non_scored_df, on=\"sig_id\", how=\"left\")\n",
    "\n",
    "    targets = folds.drop([\"sig_id\", \"kfold\"], axis=1).columns\n",
    "    features = df.drop(\"sig_id\", axis=1).columns\n",
    "    df = df.merge(folds, on=\"sig_id\", how=\"left\")\n",
    "    \n",
    "    return df, features, targets\n",
    "\n",
    "split_data()\n",
    "df, features, targets = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=0\n",
    "train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "x_tr = train_df[features].to_numpy()\n",
    "x_va = valid_df[features].to_numpy()\n",
    "\n",
    "y_tr = train_df[targets].to_numpy()\n",
    "y_va = valid_df[targets].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, fold, params, hp_tune=False):\n",
    "\n",
    "    cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n",
    "    # Cardinaliry of Label Encoded features\n",
    "    cat_dims = [len(np.unique(df[col])) for col in cat_cols]\n",
    "    # Indices of Label Encoded features, decremented once for deletion of 'sig_id'\n",
    "    cat_idx = [df.columns.get_loc(col)-1 for col in cat_cols]\n",
    "\n",
    "    save_model = False if hp_tune else True\n",
    "    print(f'\\n[Fold No.{fold:>3}]')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    x_tr = train_df[features].to_numpy()\n",
    "    x_va = valid_df[features].to_numpy()\n",
    "\n",
    "    y_tr = train_df[targets].to_numpy()\n",
    "    y_va = valid_df[targets].to_numpy()\n",
    "\n",
    "    # for hp tuning, see https://www.kaggle.com/gogo827jz/moa-stacked-tabnet-baseline-tensorflow-2-0\n",
    "    # to get the clue.\n",
    "\n",
    "    model = TabNetRegressor(n_d=32, n_a=32, n_steps=3, gamma=1.3, \n",
    "                            optimizer_fn=torch.optim.AdamW,\n",
    "                            optimizer_params=dict(lr=1e-2, amsgrad=True), \n",
    "#                             scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "#                             scheduler_params={\"patience\":3, \"mode\":\"min\"},\n",
    "                            scheduler_fn=torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "                            scheduler_params={\"T_max\":20},\n",
    "                            cat_dims=cat_dims, cat_emb_dim=len(cat_dims), cat_idxs=cat_idx, \n",
    "                            mask_type=\"sparsemax\", device_name=DEVICE)\n",
    "\n",
    "    del df, train_df, valid_df\n",
    "    gc.collect()\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    filename = f\"{MODELNAME}/weight/{now}_fold{fold}\"\n",
    "\n",
    "    model.fit(X_train=x_tr, y_train=y_tr, X_valid=x_va, y_valid=y_va, \n",
    "              max_epochs=EPOCHS, patience=50, \n",
    "              batch_size=1024, virtual_batch_size=256,\n",
    "              num_workers=2, drop_last=False, \n",
    "              loss_fn=nn.functional.binary_cross_entropy_with_logits)\n",
    "\n",
    "    model.load_best_model()\n",
    "    preds_tr = torch.as_tensor(model.predict(x_tr)).detach().cpu()\n",
    "    preds_va = torch.as_tensor(model.predict(x_va)).detach().cpu()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = criterion(preds_tr, torch.as_tensor(y_tr).type_as(preds_tr))\n",
    "    loss_va = criterion(preds_va, torch.as_tensor(y_va).type_as(preds_va))\n",
    "    with open(f\"{MODELNAME}/scores/{now}.txt\", \"a\") as f:\n",
    "        f.write(f\"[fold{fold:>2}] {loss_tr.item():.5}, {loss_va.item():.5}\\n\")\n",
    "\n",
    "    if save_model:\n",
    "        model.save_model(filename)\n",
    "\n",
    "    print(\"\\nmodel saved at:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No variable params for now \n",
    "params = {\n",
    "#     \"nn_params\": {\"dropout\": 0.2, \"num_layers\": 3, \"hidden_size\": 512, \"activation\": \"prelu\", \"batchnorm\": True},\n",
    "#     \"optimizer\": \"SGD\",\n",
    "#     \"optim_params\": {\"lr\": 1e-2, \"momentum\": 0.3},\n",
    "#     \"scheduler\": \"ReduceLROnPlateau\",\n",
    "#     \"scdl_params\": {\"threshold\": 0.00001}\n",
    "}\n",
    "# 0.02355, 0.03 on momentum:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold No.  0]\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s3616\\anaconda3\\envs\\moa\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1     |  0.54029 |   0.25108 |   4.7        True\n",
      "| 2     |  0.07329 |   0.02787 |   8.4        True\n",
      "| 3     |  0.03107 |   0.02668 |   12.0       True\n",
      "| 4     |  0.02763 |   0.02472 |   15.6       True\n",
      "| 5     |  0.02588 |   0.02363 |   19.3       True\n",
      "| 6     |  0.02521 |   0.02356 |   22.9       True\n",
      "| 7     |  0.02470 |   0.02326 |   26.6       True\n",
      "| 8     |  0.02438 |   0.02281 |   30.2       True\n",
      "| 9     |  0.02395 |   0.02267 |   33.9       True\n",
      "| 10    |  0.02362 |   0.02244 |   37.6       True\n",
      "| 11    |  0.02340 |   0.02216 |   41.2       True\n",
      "| 12    |  0.02303 |   0.02189 |   44.9       True\n",
      "| 13    |  0.02279 |   0.02182 |   48.6       True\n",
      "| 14    |  0.02273 |   0.02188 |   52.2       False\n",
      "| 15    |  0.02259 |   0.02185 |   55.9       False\n",
      "| 16    |  0.02243 |   0.02171 |   59.5       True\n",
      "| 17    |  0.02234 |   0.02182 |   63.1       False\n",
      "| 18    |  0.02218 |   0.02153 |   66.8       True\n",
      "| 19    |  0.02203 |   0.02148 |   70.5       True\n",
      "| 20    |  0.02203 |   0.02144 |   74.1       True\n",
      "| 21    |  0.02180 |   0.02130 |   77.8       True\n",
      "| 22    |  0.02172 |   0.02123 |   81.4       True\n",
      "| 23    |  0.02167 |   0.02114 |   85.0       True\n",
      "| 24    |  0.02156 |   0.02117 |   88.6       False\n",
      "| 25    |  0.02155 |   0.02110 |   92.3       True\n",
      "| 26    |  0.02141 |   0.02105 |   95.9       True\n",
      "| 27    |  0.02134 |   0.02101 |   99.6       True\n",
      "| 28    |  0.02125 |   0.02098 |   103.3      True\n",
      "| 29    |  0.02121 |   0.02098 |   106.9      True\n",
      "| 30    |  0.02115 |   0.02092 |   110.5      True\n",
      "| 31    |  0.02113 |   0.02085 |   114.2      True\n",
      "| 32    |  0.02105 |   0.02086 |   117.8      False\n",
      "| 33    |  0.02101 |   0.02084 |   121.5      True\n",
      "| 34    |  0.02095 |   0.02072 |   125.1      True\n",
      "| 35    |  0.02095 |   0.02070 |   128.8      True\n",
      "| 36    |  0.02088 |   0.02063 |   132.4      True\n",
      "| 37    |  0.02075 |   0.02056 |   136.1      True\n",
      "| 38    |  0.02071 |   0.02054 |   139.8      True\n",
      "| 39    |  0.02067 |   0.02050 |   143.4      True\n",
      "| 40    |  0.02061 |   0.02045 |   147.1      True\n",
      "| 41    |  0.02059 |   0.02038 |   150.7      True\n",
      "| 42    |  0.02051 |   0.02033 |   154.4      True\n",
      "| 43    |  0.02048 |   0.02033 |   158.0      True\n",
      "| 44    |  0.02039 |   0.02030 |   161.7      True\n",
      "| 45    |  0.02038 |   0.02023 |   165.3      True\n",
      "| 46    |  0.02031 |   0.02017 |   168.9      True\n",
      "| 47    |  0.02028 |   0.02016 |   172.6      True\n",
      "| 48    |  0.02023 |   0.02015 |   176.3      True\n",
      "| 49    |  0.02033 |   0.02021 |   180.0      False\n",
      "| 50    |  0.02033 |   0.02018 |   183.7      False\n",
      "| 51    |  0.02025 |   0.02010 |   187.3      True\n",
      "| 52    |  0.02013 |   0.02008 |   191.0      True\n",
      "| 53    |  0.02003 |   0.01997 |   194.7      True\n",
      "| 54    |  0.02003 |   0.01994 |   198.3      True\n",
      "| 55    |  0.01994 |   0.01985 |   202.0      True\n",
      "| 56    |  0.01986 |   0.01980 |   205.7      True\n",
      "| 57    |  0.01981 |   0.01979 |   209.4      True\n",
      "| 58    |  0.01978 |   0.01971 |   213.1      True\n",
      "| 59    |  0.01975 |   0.01969 |   216.8      True\n",
      "| 60    |  0.01970 |   0.01964 |   220.5      True\n",
      "| 61    |  0.01967 |   0.01968 |   224.2      False\n",
      "| 62    |  0.01961 |   0.01961 |   227.9      True\n",
      "| 63    |  0.01956 |   0.01958 |   231.7      True\n",
      "| 64    |  0.01953 |   0.01955 |   235.4      True\n",
      "| 65    |  0.01957 |   0.01952 |   239.1      True\n",
      "| 66    |  0.01949 |   0.01950 |   242.8      True\n",
      "| 67    |  0.01941 |   0.01949 |   246.5      True\n",
      "| 68    |  0.01947 |   0.01947 |   250.2      True\n",
      "| 69    |  0.01940 |   0.01946 |   253.9      True\n",
      "| 70    |  0.01936 |   0.01947 |   257.7      False\n",
      "| 71    |  0.01931 |   0.01942 |   261.4      True\n",
      "| 72    |  0.01934 |   0.01937 |   265.1      True\n",
      "| 73    |  0.01928 |   0.01938 |   268.8      False\n",
      "| 74    |  0.01928 |   0.01937 |   272.6      True\n",
      "| 75    |  0.01924 |   0.01935 |   276.3      True\n",
      "| 76    |  0.01921 |   0.01934 |   280.0      True\n",
      "| 77    |  0.01920 |   0.01929 |   283.7      True\n",
      "| 78    |  0.01918 |   0.01926 |   287.4      True\n",
      "| 79    |  0.01911 |   0.01927 |   291.1      False\n",
      "| 80    |  0.01911 |   0.01926 |   294.8      False\n",
      "| 81    |  0.01913 |   0.01924 |   298.6      True\n",
      "| 82    |  0.01907 |   0.01921 |   302.3      True\n",
      "| 83    |  0.01904 |   0.01922 |   305.9      False\n",
      "| 84    |  0.01904 |   0.01920 |   309.6      True\n",
      "| 85    |  0.01904 |   0.01919 |   313.1      True\n",
      "| 86    |  0.01903 |   0.01918 |   316.7      True\n",
      "| 87    |  0.01899 |   0.01922 |   320.3      False\n",
      "| 88    |  0.01898 |   0.01918 |   323.9      True\n",
      "| 89    |  0.01893 |   0.01915 |   327.5      True\n",
      "| 90    |  0.01892 |   0.01913 |   331.0      True\n",
      "| 91    |  0.01888 |   0.01914 |   334.6      False\n",
      "| 92    |  0.01883 |   0.01912 |   338.2      True\n",
      "| 93    |  0.01888 |   0.01910 |   341.7      True\n",
      "| 94    |  0.01884 |   0.01907 |   345.4      True\n",
      "| 95    |  0.01878 |   0.01911 |   348.9      False\n",
      "| 96    |  0.01879 |   0.01905 |   352.5      True\n",
      "| 97    |  0.01875 |   0.01908 |   356.1      False\n",
      "| 98    |  0.01875 |   0.01905 |   359.6      True\n",
      "| 99    |  0.01874 |   0.01904 |   363.2      True\n",
      "| 100   |  0.01866 |   0.01904 |   366.7      True\n",
      "| 101   |  0.01866 |   0.01899 |   370.3      True\n",
      "| 102   |  0.01864 |   0.01904 |   373.8      False\n",
      "| 103   |  0.01866 |   0.01904 |   377.4      False\n",
      "| 104   |  0.01861 |   0.01903 |   380.9      False\n",
      "| 105   |  0.01858 |   0.01899 |   384.5      False\n",
      "| 106   |  0.01855 |   0.01907 |   388.1      False\n",
      "| 107   |  0.01858 |   0.01899 |   391.7      False\n",
      "| 108   |  0.01856 |   0.01899 |   395.3      False\n",
      "| 109   |  0.01851 |   0.01901 |   398.8      False\n",
      "| 110   |  0.01848 |   0.01898 |   402.4      True\n",
      "| 111   |  0.01847 |   0.01898 |   406.0      True\n",
      "| 112   |  0.01848 |   0.01896 |   409.5      True\n",
      "| 113   |  0.01843 |   0.01897 |   413.1      False\n",
      "| 114   |  0.01846 |   0.01895 |   416.7      True\n",
      "| 115   |  0.01843 |   0.01896 |   420.2      False\n",
      "| 116   |  0.01840 |   0.01894 |   423.8      True\n",
      "| 117   |  0.01839 |   0.01894 |   427.4      False\n",
      "| 118   |  0.01840 |   0.01892 |   431.0      True\n",
      "| 119   |  0.01835 |   0.01893 |   434.6      False\n",
      "| 120   |  0.01831 |   0.01892 |   438.2      False\n",
      "| 121   |  0.01833 |   0.01894 |   441.8      False\n",
      "| 122   |  0.01830 |   0.01890 |   445.4      True\n",
      "| 123   |  0.01830 |   0.01895 |   449.0      False\n",
      "| 124   |  0.01827 |   0.01894 |   452.6      False\n",
      "| 125   |  0.01826 |   0.01889 |   456.2      True\n",
      "| 126   |  0.01826 |   0.01892 |   459.8      False\n",
      "| 127   |  0.01826 |   0.01894 |   463.3      False\n",
      "| 128   |  0.01819 |   0.01900 |   466.9      False\n",
      "| 129   |  0.01823 |   0.01898 |   470.5      False\n",
      "| 130   |  0.01818 |   0.01897 |   474.1      False\n",
      "| 131   |  0.01819 |   0.01899 |   477.6      False\n",
      "| 132   |  0.01815 |   0.01896 |   481.2      False\n",
      "| 133   |  0.01819 |   0.01897 |   484.8      False\n",
      "| 134   |  0.01813 |   0.01894 |   488.3      False\n",
      "| 135   |  0.01807 |   0.01897 |   492.0      False\n",
      "| 136   |  0.01809 |   0.01898 |   495.5      False\n",
      "| 137   |  0.01806 |   0.01892 |   499.0      False\n",
      "| 138   |  0.01809 |   0.01892 |   502.6      False\n",
      "| 139   |  0.01819 |   0.01903 |   506.1      False\n",
      "| 140   |  0.01827 |   0.01893 |   509.7      False\n",
      "| 141   |  0.01820 |   0.01892 |   513.3      False\n",
      "| 142   |  0.01822 |   0.01952 |   516.9      False\n",
      "| 143   |  0.01832 |   0.01898 |   520.4      False\n",
      "| 144   |  0.01822 |   0.01907 |   524.0      False\n",
      "| 145   |  0.01817 |   0.01894 |   527.6      False\n",
      "| 146   |  0.01811 |   0.01891 |   531.2      False\n",
      "| 147   |  0.01805 |   0.01885 |   534.8      True\n",
      "| 148   |  0.01808 |   0.01884 |   538.4      True\n",
      "| 149   |  0.01800 |   0.01888 |   542.0      False\n",
      "| 150   |  0.01797 |   0.01885 |   545.6      False\n",
      "| 151   |  0.01800 |   0.01879 |   549.2      True\n",
      "| 152   |  0.01792 |   0.01880 |   552.8      False\n",
      "| 153   |  0.01790 |   0.01883 |   556.3      False\n",
      "| 154   |  0.01791 |   0.01879 |   559.9      True\n",
      "| 155   |  0.01790 |   0.01884 |   563.4      False\n",
      "| 156   |  0.01789 |   0.01879 |   567.0      False\n",
      "| 157   |  0.01787 |   0.01879 |   570.6      True\n",
      "| 158   |  0.01783 |   0.01879 |   574.1      False\n",
      "| 159   |  0.01783 |   0.01881 |   577.7      False\n",
      "| 160   |  0.01779 |   0.01879 |   581.2      False\n",
      "| 161   |  0.01779 |   0.01879 |   584.7      False\n",
      "| 162   |  0.01775 |   0.01879 |   588.3      False\n",
      "| 163   |  0.01772 |   0.01883 |   591.9      False\n",
      "| 164   |  0.01770 |   0.01884 |   595.4      False\n",
      "| 165   |  0.01767 |   0.01882 |   599.0      False\n",
      "| 166   |  0.01769 |   0.01881 |   602.5      False\n",
      "| 167   |  0.01768 |   0.01881 |   606.1      False\n",
      "| 168   |  0.01766 |   0.01881 |   609.7      False\n",
      "| 169   |  0.01763 |   0.01886 |   613.3      False\n",
      "| 170   |  0.01764 |   0.01882 |   616.9      False\n",
      "| 171   |  0.01762 |   0.01883 |   620.5      False\n",
      "| 172   |  0.01758 |   0.01886 |   624.2      False\n",
      "| 173   |  0.01766 |   0.01878 |   627.8      True\n",
      "| 174   |  0.01758 |   0.01883 |   631.4      False\n",
      "| 175   |  0.01757 |   0.01885 |   634.9      False\n",
      "| 176   |  0.01762 |   0.01886 |   638.5      False\n",
      "| 177   |  0.01757 |   0.01883 |   642.1      False\n",
      "| 178   |  0.01752 |   0.01880 |   645.8      False\n",
      "| 179   |  0.01751 |   0.01888 |   649.4      False\n",
      "| 180   |  0.01752 |   0.01885 |   653.0      False\n",
      "| 181   |  0.01747 |   0.01887 |   656.9      False\n",
      "| 182   |  0.01749 |   0.01893 |   660.6      False\n",
      "| 183   |  0.01741 |   0.01894 |   664.3      False\n",
      "| 184   |  0.01743 |   0.01892 |   668.0      False\n",
      "| 185   |  0.01745 |   0.01886 |   671.7      False\n",
      "| 186   |  0.01741 |   0.01894 |   675.3      False\n",
      "| 187   |  0.01741 |   0.01892 |   679.0      False\n",
      "| 188   |  0.01745 |   0.01892 |   682.7      False\n",
      "| 189   |  0.01741 |   0.01887 |   686.4      False\n",
      "| 190   |  0.01734 |   0.01889 |   690.1      False\n",
      "| 191   |  0.01733 |   0.01891 |   693.7      False\n",
      "| 192   |  0.01733 |   0.01889 |   697.3      False\n",
      "| 193   |  0.01734 |   0.01887 |   700.8      False\n",
      "| 194   |  0.01731 |   0.01891 |   704.4      False\n",
      "| 195   |  0.01732 |   0.01886 |   708.0      False\n",
      "| 196   |  0.01728 |   0.01894 |   711.6      False\n",
      "| 197   |  0.01729 |   0.01889 |   715.2      False\n",
      "| 198   |  0.01726 |   0.01892 |   718.8      False\n",
      "| 199   |  0.01722 |   0.01895 |   722.5      False\n",
      "| 200   |  0.01718 |   0.01891 |   726.1      False\n",
      "| 201   |  0.01713 |   0.01899 |   729.7      False\n",
      "| 202   |  0.01721 |   0.01891 |   733.3      False\n",
      "| 203   |  0.01718 |   0.01895 |   736.9      False\n",
      "| 204   |  0.01716 |   0.01898 |   740.4      False\n",
      "| 205   |  0.01715 |   0.01913 |   744.0      False\n",
      "| 206   |  0.01719 |   0.01906 |   747.6      False\n",
      "| 207   |  0.01712 |   0.01908 |   751.2      False\n",
      "| 208   |  0.01706 |   0.01904 |   754.8      False\n",
      "| 209   |  0.01711 |   0.01897 |   758.3      False\n",
      "| 210   |  0.01713 |   0.01895 |   761.9      False\n",
      "| 211   |  0.01706 |   0.01901 |   765.5      False\n",
      "| 212   |  0.01707 |   0.01908 |   769.1      False\n",
      "| 213   |  0.01706 |   0.01900 |   772.7      False\n",
      "| 214   |  0.01704 |   0.01909 |   776.2      False\n",
      "| 215   |  0.01706 |   0.01896 |   779.9      False\n",
      "| 216   |  0.01700 |   0.01903 |   783.5      False\n",
      "| 217   |  0.01696 |   0.01914 |   787.1      False\n",
      "| 218   |  0.01703 |   0.01911 |   790.6      False\n",
      "| 219   |  0.01698 |   0.01914 |   794.2      False\n",
      "| 220   |  0.01702 |   0.01908 |   797.8      False\n",
      "| 221   |  0.01702 |   0.01901 |   801.4      False\n",
      "| 222   |  0.01699 |   0.01908 |   805.0      False\n",
      "| 223   |  0.01693 |   0.01910 |   808.6      False\n",
      "Early stopping occured at epoch 223\n",
      "Training done in 808.551 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at TabNet1017/weight/10-18_1714_fold0.zip\n",
      "\n",
      "model saved at: TabNet1017/weight/10-18_1714_fold0\n",
      "\n",
      "[Fold No.  1]\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s3616\\anaconda3\\envs\\moa\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1     |  0.52681 |   0.22083 |   3.6        True\n",
      "| 2     |  0.06872 |   0.02914 |   7.2        True\n",
      "| 3     |  0.03122 |   0.02613 |   10.9       True\n",
      "| 4     |  0.02744 |   0.02505 |   14.5       True\n",
      "| 5     |  0.02606 |   0.02385 |   18.1       True\n",
      "| 6     |  0.02527 |   0.02365 |   21.7       True\n",
      "| 7     |  0.02465 |   0.02323 |   25.3       True\n",
      "| 8     |  0.02418 |   0.02299 |   28.9       True\n",
      "| 9     |  0.02377 |   0.02272 |   32.5       True\n",
      "| 10    |  0.02351 |   0.02237 |   36.0       True\n",
      "| 11    |  0.02327 |   0.02226 |   39.6       True\n",
      "| 12    |  0.02296 |   0.02206 |   43.2       True\n",
      "| 13    |  0.02280 |   0.02223 |   46.7       False\n",
      "| 14    |  0.02253 |   0.02172 |   50.3       True\n",
      "| 15    |  0.02211 |   0.02146 |   53.9       True\n",
      "| 16    |  0.02190 |   0.02137 |   57.6       True\n",
      "| 17    |  0.02173 |   0.02131 |   61.2       True\n",
      "| 18    |  0.02163 |   0.02123 |   64.8       True\n",
      "| 19    |  0.02151 |   0.02109 |   68.4       True\n",
      "| 20    |  0.02139 |   0.02107 |   72.0       True\n",
      "| 21    |  0.02126 |   0.02099 |   75.6       True\n",
      "| 22    |  0.02127 |   0.02098 |   79.2       True\n",
      "| 23    |  0.02121 |   0.02092 |   82.8       True\n",
      "| 24    |  0.02111 |   0.02082 |   86.4       True\n",
      "| 25    |  0.02103 |   0.02088 |   90.1       False\n",
      "| 26    |  0.02096 |   0.02096 |   93.7       False\n",
      "| 27    |  0.02094 |   0.02081 |   97.3       True\n",
      "| 28    |  0.02082 |   0.02070 |   100.9      True\n",
      "| 29    |  0.02084 |   0.02066 |   104.6      True\n",
      "| 30    |  0.02076 |   0.02062 |   108.2      True\n",
      "| 31    |  0.02075 |   0.02062 |   111.8      True\n",
      "| 32    |  0.02070 |   0.02069 |   115.4      False\n",
      "| 33    |  0.02064 |   0.02054 |   118.9      True\n",
      "| 34    |  0.02057 |   0.02053 |   122.5      True\n",
      "| 35    |  0.02057 |   0.02052 |   126.1      True\n",
      "| 36    |  0.02050 |   0.02050 |   129.7      True\n",
      "| 37    |  0.02048 |   0.02044 |   133.4      True\n",
      "| 38    |  0.02043 |   0.02043 |   137.0      True\n",
      "| 39    |  0.02039 |   0.02042 |   140.6      True\n",
      "| 40    |  0.02038 |   0.02035 |   144.2      True\n",
      "| 41    |  0.02034 |   0.02036 |   147.8      False\n",
      "| 42    |  0.02035 |   0.02028 |   151.4      True\n",
      "| 43    |  0.02030 |   0.02029 |   155.0      False\n",
      "| 44    |  0.02028 |   0.02028 |   158.6      True\n",
      "| 45    |  0.02029 |   0.02022 |   162.2      True\n",
      "| 46    |  0.02019 |   0.02016 |   165.7      True\n",
      "| 47    |  0.02011 |   0.02011 |   169.4      True\n",
      "| 48    |  0.02009 |   0.02008 |   173.1      True\n",
      "| 49    |  0.02004 |   0.02010 |   176.7      False\n",
      "| 50    |  0.02002 |   0.02006 |   180.3      True\n",
      "| 51    |  0.01994 |   0.02006 |   183.8      False\n",
      "| 52    |  0.01997 |   0.01998 |   187.5      True\n",
      "| 53    |  0.01989 |   0.01993 |   191.2      True\n",
      "| 54    |  0.01994 |   0.01991 |   194.8      True\n",
      "| 55    |  0.01984 |   0.01986 |   198.5      True\n",
      "| 56    |  0.01976 |   0.01985 |   202.1      True\n",
      "| 57    |  0.01979 |   0.01983 |   205.7      True\n",
      "| 58    |  0.01972 |   0.01982 |   209.3      True\n",
      "| 59    |  0.01970 |   0.01977 |   212.9      True\n",
      "| 60    |  0.01967 |   0.01979 |   216.5      False\n",
      "| 61    |  0.01966 |   0.01980 |   220.1      False\n",
      "| 62    |  0.01959 |   0.01973 |   223.8      True\n",
      "| 63    |  0.01964 |   0.01972 |   227.3      True\n",
      "| 64    |  0.01955 |   0.01972 |   230.9      True\n",
      "| 65    |  0.01956 |   0.01968 |   234.6      True\n",
      "| 66    |  0.01956 |   0.01970 |   238.2      False\n",
      "| 67    |  0.01950 |   0.01970 |   241.7      False\n",
      "| 68    |  0.01953 |   0.01972 |   245.3      False\n",
      "| 69    |  0.01950 |   0.01969 |   248.9      False\n",
      "| 70    |  0.01947 |   0.01966 |   252.5      True\n",
      "| 71    |  0.01945 |   0.01974 |   256.1      False\n",
      "| 72    |  0.01947 |   0.01969 |   259.9      False\n",
      "| 73    |  0.01943 |   0.01967 |   263.5      False\n",
      "| 74    |  0.01936 |   0.01960 |   267.1      True\n",
      "| 75    |  0.01934 |   0.01956 |   270.7      True\n",
      "| 76    |  0.01930 |   0.01955 |   274.5      True\n",
      "| 77    |  0.01935 |   0.01955 |   278.1      False\n",
      "| 78    |  0.01928 |   0.01950 |   281.7      True\n",
      "| 79    |  0.01922 |   0.01949 |   285.4      True\n",
      "| 80    |  0.01920 |   0.01942 |   289.1      True\n",
      "| 81    |  0.01917 |   0.01944 |   292.7      False\n",
      "| 82    |  0.01914 |   0.01944 |   296.4      False\n",
      "| 83    |  0.01915 |   0.01946 |   300.0      False\n",
      "| 84    |  0.01907 |   0.01943 |   303.6      False\n",
      "| 85    |  0.01906 |   0.01946 |   307.2      False\n",
      "| 86    |  0.01901 |   0.01946 |   310.8      False\n",
      "| 87    |  0.01902 |   0.01932 |   314.4      True\n",
      "| 88    |  0.01902 |   0.01938 |   318.0      False\n",
      "| 89    |  0.01899 |   0.01934 |   321.6      False\n",
      "| 90    |  0.01897 |   0.01935 |   325.1      False\n",
      "| 91    |  0.01895 |   0.01945 |   328.7      False\n",
      "| 92    |  0.01892 |   0.01928 |   332.3      True\n",
      "| 93    |  0.01888 |   0.01931 |   335.9      False\n",
      "| 94    |  0.01889 |   0.01929 |   339.6      False\n",
      "| 95    |  0.01884 |   0.01925 |   343.3      True\n",
      "| 96    |  0.01877 |   0.01935 |   346.9      False\n",
      "| 97    |  0.01878 |   0.01936 |   350.5      False\n",
      "| 98    |  0.01873 |   0.01950 |   354.1      False\n",
      "| 99    |  0.01876 |   0.01953 |   357.7      False\n",
      "| 100   |  0.01872 |   0.01924 |   361.3      True\n",
      "| 101   |  0.01871 |   0.01927 |   364.9      False\n",
      "| 102   |  0.01872 |   0.01937 |   368.4      False\n",
      "| 103   |  0.01872 |   0.01929 |   372.0      False\n",
      "| 104   |  0.01869 |   0.01916 |   375.6      True\n",
      "| 105   |  0.01866 |   0.01918 |   379.2      False\n",
      "| 106   |  0.01861 |   0.01913 |   382.9      True\n",
      "| 107   |  0.01866 |   0.01925 |   386.4      False\n",
      "| 108   |  0.01859 |   0.01923 |   390.0      False\n",
      "| 109   |  0.01860 |   0.01921 |   393.6      False\n",
      "| 110   |  0.01867 |   0.01908 |   397.3      True\n",
      "| 111   |  0.01869 |   0.01914 |   400.9      False\n",
      "| 112   |  0.01860 |   0.01907 |   404.5      True\n",
      "| 113   |  0.01857 |   0.01907 |   408.1      False\n",
      "| 114   |  0.01853 |   0.01908 |   411.8      False\n",
      "| 115   |  0.01852 |   0.01903 |   415.4      True\n",
      "| 116   |  0.01845 |   0.01905 |   419.0      False\n",
      "| 117   |  0.01844 |   0.01900 |   422.6      True\n",
      "| 118   |  0.01844 |   0.01900 |   426.2      False\n",
      "| 119   |  0.01834 |   0.01907 |   429.8      False\n",
      "| 120   |  0.01836 |   0.01960 |   433.4      False\n",
      "| 121   |  0.01830 |   0.01904 |   437.0      False\n",
      "| 122   |  0.01834 |   0.01898 |   440.6      True\n",
      "| 123   |  0.01829 |   0.01897 |   444.3      True\n",
      "| 124   |  0.01829 |   0.01908 |   448.0      False\n",
      "| 125   |  0.01823 |   0.01902 |   451.6      False\n",
      "| 126   |  0.01821 |   0.01905 |   455.3      False\n",
      "| 127   |  0.01830 |   0.01904 |   458.9      False\n",
      "| 128   |  0.01822 |   0.01928 |   462.6      False\n",
      "| 129   |  0.01821 |   0.01911 |   466.2      False\n",
      "| 130   |  0.01815 |   0.01905 |   469.8      False\n",
      "| 131   |  0.01817 |   0.01901 |   473.4      False\n",
      "| 132   |  0.01818 |   0.01898 |   476.9      False\n",
      "| 133   |  0.01815 |   0.01895 |   480.5      True\n",
      "| 134   |  0.01809 |   0.01896 |   484.1      False\n",
      "| 135   |  0.01808 |   0.01900 |   487.7      False\n",
      "| 136   |  0.01808 |   0.01897 |   491.3      False\n",
      "| 137   |  0.01804 |   0.01894 |   495.0      True\n",
      "| 138   |  0.01804 |   0.01897 |   498.6      False\n",
      "| 139   |  0.01800 |   0.01905 |   502.2      False\n",
      "| 140   |  0.01805 |   0.01902 |   505.7      False\n",
      "| 141   |  0.01800 |   0.01900 |   509.3      False\n",
      "| 142   |  0.01799 |   0.01896 |   512.9      False\n",
      "| 143   |  0.01797 |   0.01895 |   516.5      False\n",
      "| 144   |  0.01796 |   0.01893 |   520.2      True\n",
      "| 145   |  0.01793 |   0.01892 |   523.8      True\n",
      "| 146   |  0.01787 |   0.01895 |   527.5      False\n",
      "| 147   |  0.01787 |   0.01890 |   531.1      True\n",
      "| 148   |  0.01788 |   0.01898 |   534.7      False\n",
      "| 149   |  0.01785 |   0.01894 |   538.4      False\n",
      "| 150   |  0.01783 |   0.01894 |   542.0      False\n",
      "| 151   |  0.01782 |   0.01894 |   545.5      False\n",
      "| 152   |  0.01778 |   0.01897 |   549.1      False\n",
      "| 153   |  0.01773 |   0.01886 |   552.7      True\n",
      "| 154   |  0.01773 |   0.01887 |   556.3      False\n",
      "| 155   |  0.01771 |   0.01889 |   559.8      False\n",
      "| 156   |  0.01768 |   0.01897 |   563.5      False\n",
      "| 157   |  0.01774 |   0.01895 |   567.1      False\n",
      "| 158   |  0.01773 |   0.01887 |   570.7      False\n",
      "| 159   |  0.01768 |   0.01887 |   574.3      False\n",
      "| 160   |  0.01766 |   0.01885 |   577.8      True\n",
      "| 161   |  0.01763 |   0.01885 |   581.4      True\n",
      "| 162   |  0.01759 |   0.01887 |   585.0      False\n",
      "| 163   |  0.01758 |   0.01882 |   588.6      True\n",
      "| 164   |  0.01755 |   0.01886 |   592.2      False\n",
      "| 165   |  0.01754 |   0.01887 |   595.8      False\n",
      "| 166   |  0.01752 |   0.01881 |   599.4      True\n",
      "| 167   |  0.01750 |   0.01882 |   603.0      False\n",
      "| 168   |  0.01753 |   0.01898 |   606.5      False\n",
      "| 169   |  0.01754 |   0.01903 |   610.1      False\n",
      "| 170   |  0.01751 |   0.01892 |   613.7      False\n",
      "| 171   |  0.01749 |   0.01892 |   617.3      False\n",
      "| 172   |  0.01744 |   0.01887 |   621.0      False\n",
      "| 173   |  0.01742 |   0.01896 |   624.6      False\n",
      "| 174   |  0.01738 |   0.01892 |   628.2      False\n",
      "| 175   |  0.01741 |   0.01883 |   631.7      False\n",
      "| 176   |  0.01738 |   0.01900 |   635.3      False\n",
      "| 177   |  0.01747 |   0.01901 |   638.9      False\n",
      "| 178   |  0.01738 |   0.01884 |   642.5      False\n",
      "| 179   |  0.01733 |   0.01882 |   646.1      False\n",
      "| 180   |  0.01734 |   0.01883 |   649.7      False\n",
      "| 181   |  0.01728 |   0.01889 |   653.5      False\n",
      "| 182   |  0.01726 |   0.01881 |   657.2      False\n",
      "| 183   |  0.01730 |   0.01885 |   660.8      False\n",
      "| 184   |  0.01724 |   0.01947 |   664.4      False\n",
      "| 185   |  0.01721 |   0.01889 |   668.0      False\n",
      "| 186   |  0.01724 |   0.01891 |   671.6      False\n",
      "| 187   |  0.01720 |   0.01892 |   675.3      False\n",
      "| 188   |  0.01718 |   0.01885 |   678.8      False\n",
      "| 189   |  0.01717 |   0.01894 |   682.6      False\n",
      "| 190   |  0.01719 |   0.01899 |   686.2      False\n",
      "| 191   |  0.01715 |   0.01907 |   689.9      False\n",
      "| 192   |  0.01713 |   0.01903 |   693.5      False\n",
      "| 193   |  0.01713 |   0.01920 |   697.2      False\n",
      "| 194   |  0.01714 |   0.01965 |   700.8      False\n",
      "| 195   |  0.01721 |   0.01916 |   704.4      False\n",
      "| 196   |  0.01706 |   0.01917 |   708.0      False\n",
      "| 197   |  0.01706 |   0.01909 |   711.6      False\n",
      "| 198   |  0.01706 |   0.01917 |   715.3      False\n",
      "| 199   |  0.01702 |   0.01907 |   719.0      False\n",
      "| 200   |  0.01702 |   0.01899 |   722.8      False\n",
      "| 201   |  0.01700 |   0.01899 |   726.6      False\n",
      "| 202   |  0.01694 |   0.01897 |   730.4      False\n",
      "| 203   |  0.01695 |   0.01903 |   734.2      False\n",
      "| 204   |  0.01693 |   0.01903 |   737.9      False\n",
      "| 205   |  0.01690 |   0.01898 |   741.7      False\n",
      "| 206   |  0.01684 |   0.01902 |   745.3      False\n",
      "| 207   |  0.01688 |   0.01902 |   749.0      False\n",
      "| 208   |  0.01682 |   0.01902 |   752.7      False\n",
      "| 209   |  0.01678 |   0.01907 |   756.5      False\n",
      "| 210   |  0.01681 |   0.01906 |   760.2      False\n",
      "| 211   |  0.01677 |   0.01905 |   763.8      False\n",
      "| 212   |  0.01678 |   0.01903 |   767.5      False\n",
      "| 213   |  0.01676 |   0.01900 |   771.2      False\n",
      "| 214   |  0.01680 |   0.01947 |   774.9      False\n",
      "| 215   |  0.01686 |   0.01970 |   778.6      False\n",
      "| 216   |  0.01680 |   0.01955 |   782.2      False\n",
      "Early stopping occured at epoch 216\n",
      "Training done in 782.216 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at TabNet1017/weight/10-18_1714_fold1.zip\n",
      "\n",
      "model saved at: TabNet1017/weight/10-18_1714_fold1\n",
      "\n",
      "[Fold No.  2]\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s3616\\anaconda3\\envs\\moa\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1     |  0.52790 |   0.22184 |   3.7        True\n",
      "| 2     |  0.06599 |   0.02827 |   7.4        True\n",
      "| 3     |  0.03067 |   0.02624 |   11.1       True\n",
      "| 4     |  0.02694 |   0.02501 |   14.7       True\n",
      "| 5     |  0.02551 |   0.02369 |   18.4       True\n",
      "| 6     |  0.02450 |   0.02318 |   22.2       True\n",
      "| 7     |  0.02421 |   0.02311 |   25.9       True\n",
      "| 8     |  0.02398 |   0.02268 |   29.5       True\n",
      "| 9     |  0.02352 |   0.02257 |   33.2       True\n",
      "| 10    |  0.02311 |   0.02228 |   37.0       True\n",
      "| 11    |  0.02285 |   0.02218 |   40.9       True\n",
      "| 12    |  0.02264 |   0.02207 |   44.7       True\n",
      "| 13    |  0.02249 |   0.02192 |   48.4       True\n",
      "| 14    |  0.02227 |   0.02197 |   52.1       False\n",
      "| 15    |  0.02218 |   0.02169 |   55.8       True\n",
      "| 16    |  0.02198 |   0.02160 |   59.6       True\n",
      "| 17    |  0.02183 |   0.02145 |   63.3       True\n",
      "| 18    |  0.02170 |   0.02128 |   67.0       True\n",
      "| 19    |  0.02157 |   0.02131 |   70.6       False\n",
      "| 20    |  0.02151 |   0.02129 |   74.3       False\n",
      "| 21    |  0.02144 |   0.02124 |   78.0       True\n",
      "| 22    |  0.02142 |   0.02125 |   81.6       False\n",
      "| 23    |  0.02135 |   0.02128 |   85.3       False\n",
      "| 24    |  0.02127 |   0.02113 |   89.0       True\n",
      "| 25    |  0.02118 |   0.02110 |   92.6       True\n",
      "| 26    |  0.02116 |   0.02108 |   96.4       True\n",
      "| 27    |  0.02114 |   0.02102 |   100.1      True\n",
      "| 28    |  0.02103 |   0.02120 |   103.8      False\n",
      "| 29    |  0.02104 |   0.02103 |   107.5      False\n",
      "| 30    |  0.02099 |   0.02096 |   111.2      True\n",
      "| 31    |  0.02096 |   0.02094 |   114.9      True\n",
      "| 32    |  0.02093 |   0.02090 |   118.6      True\n",
      "| 33    |  0.02087 |   0.02089 |   122.3      True\n",
      "| 34    |  0.02077 |   0.02082 |   126.0      True\n",
      "| 35    |  0.02079 |   0.02077 |   129.8      True\n",
      "| 36    |  0.02073 |   0.02073 |   133.5      True\n",
      "| 37    |  0.02071 |   0.02070 |   137.2      True\n",
      "| 38    |  0.02066 |   0.02068 |   140.9      True\n",
      "| 39    |  0.02060 |   0.02067 |   144.6      True\n",
      "| 40    |  0.02055 |   0.02066 |   148.3      True\n",
      "| 41    |  0.02050 |   0.02062 |   152.0      True\n",
      "| 42    |  0.02049 |   0.02061 |   155.7      True\n",
      "| 43    |  0.02045 |   0.02056 |   159.3      True\n",
      "| 44    |  0.02045 |   0.02056 |   163.0      True\n",
      "| 45    |  0.02035 |   0.02056 |   166.7      True\n",
      "| 46    |  0.02037 |   0.02052 |   170.4      True\n",
      "| 47    |  0.02031 |   0.02049 |   174.1      True\n",
      "| 48    |  0.02025 |   0.02049 |   177.7      True\n",
      "| 49    |  0.02027 |   0.02046 |   181.4      True\n",
      "| 50    |  0.02021 |   0.02042 |   185.2      True\n",
      "| 51    |  0.02016 |   0.02040 |   189.0      True\n",
      "| 52    |  0.02016 |   0.02041 |   192.7      False\n",
      "| 53    |  0.02016 |   0.02040 |   196.4      False\n",
      "| 54    |  0.02009 |   0.02035 |   200.1      True\n",
      "| 55    |  0.02004 |   0.02033 |   203.7      True\n",
      "| 56    |  0.02006 |   0.02036 |   207.4      False\n",
      "| 57    |  0.02001 |   0.02035 |   211.0      False\n",
      "| 58    |  0.01996 |   0.02032 |   214.7      True\n",
      "| 59    |  0.01994 |   0.02025 |   218.4      True\n",
      "| 60    |  0.01992 |   0.02025 |   222.1      True\n",
      "| 61    |  0.01990 |   0.02023 |   225.8      True\n",
      "| 62    |  0.01985 |   0.02018 |   229.4      True\n",
      "| 63    |  0.01982 |   0.02022 |   233.1      False\n",
      "| 64    |  0.01979 |   0.02019 |   236.8      False\n",
      "| 65    |  0.01977 |   0.02019 |   240.5      False\n",
      "| 66    |  0.01972 |   0.02017 |   244.2      True\n",
      "| 67    |  0.01972 |   0.02015 |   247.9      True\n",
      "| 68    |  0.01970 |   0.02011 |   251.7      True\n",
      "| 69    |  0.01962 |   0.02011 |   255.4      False\n",
      "| 70    |  0.01961 |   0.02009 |   259.1      True\n",
      "| 71    |  0.01961 |   0.02007 |   262.8      True\n",
      "| 72    |  0.01958 |   0.02013 |   266.4      False\n",
      "| 73    |  0.01958 |   0.02014 |   270.1      False\n",
      "| 74    |  0.01956 |   0.02014 |   273.7      False\n",
      "| 75    |  0.01958 |   0.02011 |   277.4      False\n",
      "| 76    |  0.01954 |   0.02011 |   281.3      False\n",
      "| 77    |  0.01951 |   0.02003 |   285.1      True\n",
      "| 78    |  0.01949 |   0.02003 |   288.9      False\n",
      "| 79    |  0.01943 |   0.02005 |   292.7      False\n",
      "| 80    |  0.01939 |   0.02002 |   296.4      True\n",
      "| 81    |  0.01932 |   0.01998 |   300.2      True\n",
      "| 82    |  0.01933 |   0.01994 |   303.9      True\n",
      "| 83    |  0.01927 |   0.01995 |   307.6      False\n",
      "| 84    |  0.01930 |   0.01997 |   311.3      False\n",
      "| 85    |  0.01930 |   0.02001 |   314.9      False\n",
      "| 86    |  0.01948 |   0.02107 |   318.6      False\n",
      "| 87    |  0.01956 |   0.02004 |   322.3      False\n",
      "| 88    |  0.01936 |   0.01999 |   326.0      False\n",
      "| 89    |  0.01929 |   0.01996 |   329.6      False\n",
      "| 90    |  0.01928 |   0.01994 |   333.3      False\n",
      "| 91    |  0.01922 |   0.01995 |   337.0      False\n",
      "| 92    |  0.01918 |   0.01994 |   340.9      True\n",
      "| 93    |  0.01914 |   0.01991 |   344.6      True\n",
      "| 94    |  0.01912 |   0.01990 |   348.3      True\n",
      "| 95    |  0.01914 |   0.01986 |   352.0      True\n",
      "| 96    |  0.01912 |   0.01982 |   355.7      True\n",
      "| 97    |  0.01905 |   0.01981 |   359.4      True\n",
      "| 98    |  0.01906 |   0.01983 |   363.1      False\n",
      "| 99    |  0.01901 |   0.01983 |   366.7      False\n",
      "| 100   |  0.01902 |   0.01980 |   370.4      True\n",
      "| 101   |  0.01894 |   0.01978 |   374.1      True\n",
      "| 102   |  0.01894 |   0.01973 |   377.9      True\n",
      "| 103   |  0.01891 |   0.01974 |   381.5      False\n",
      "| 104   |  0.01890 |   0.01971 |   385.2      True\n",
      "| 105   |  0.01891 |   0.01972 |   388.9      False\n",
      "| 106   |  0.01890 |   0.01972 |   392.6      False\n",
      "| 107   |  0.01888 |   0.01972 |   396.3      False\n",
      "| 108   |  0.01884 |   0.01972 |   400.0      False\n",
      "| 109   |  0.01880 |   0.01971 |   403.6      True\n",
      "| 110   |  0.01879 |   0.01969 |   407.3      True\n",
      "| 111   |  0.01878 |   0.01966 |   411.0      True\n",
      "| 112   |  0.01878 |   0.01968 |   414.6      False\n",
      "| 113   |  0.01875 |   0.01972 |   418.3      False\n",
      "| 114   |  0.01875 |   0.01965 |   422.0      True\n",
      "| 115   |  0.01877 |   0.01972 |   425.6      False\n",
      "| 116   |  0.01872 |   0.01969 |   429.3      False\n",
      "| 117   |  0.01873 |   0.01970 |   433.0      False\n",
      "| 118   |  0.01874 |   0.01976 |   436.6      False\n",
      "| 119   |  0.01867 |   0.01963 |   440.3      True\n",
      "| 120   |  0.01865 |   0.01966 |   443.9      False\n",
      "| 121   |  0.01868 |   0.01967 |   447.6      False\n",
      "| 122   |  0.01861 |   0.01966 |   451.2      False\n",
      "| 123   |  0.01860 |   0.01968 |   454.9      False\n",
      "| 124   |  0.01858 |   0.01964 |   458.6      False\n",
      "| 125   |  0.01857 |   0.01967 |   462.3      False\n",
      "| 126   |  0.01858 |   0.01966 |   466.0      False\n",
      "| 127   |  0.01857 |   0.01967 |   469.6      False\n",
      "| 128   |  0.01856 |   0.01967 |   473.3      False\n",
      "| 129   |  0.01850 |   0.01971 |   476.9      False\n",
      "| 130   |  0.01848 |   0.01968 |   480.6      False\n",
      "| 131   |  0.01845 |   0.01969 |   484.3      False\n",
      "| 132   |  0.01847 |   0.01965 |   487.9      False\n",
      "| 133   |  0.01849 |   0.01961 |   491.6      True\n",
      "| 134   |  0.01843 |   0.01962 |   495.4      False\n",
      "| 135   |  0.01841 |   0.01968 |   499.0      False\n",
      "| 136   |  0.01834 |   0.01961 |   502.6      True\n",
      "| 137   |  0.01837 |   0.01961 |   506.3      False\n",
      "| 138   |  0.01842 |   0.01961 |   509.9      False\n",
      "| 139   |  0.01838 |   0.01966 |   513.6      False\n",
      "| 140   |  0.01837 |   0.01978 |   517.3      False\n",
      "| 141   |  0.01844 |   0.01968 |   521.0      False\n",
      "| 142   |  0.01835 |   0.01963 |   524.6      False\n",
      "| 143   |  0.01828 |   0.01963 |   528.3      False\n",
      "| 144   |  0.01831 |   0.01966 |   532.0      False\n",
      "| 145   |  0.01829 |   0.01960 |   535.7      True\n",
      "| 146   |  0.01827 |   0.01958 |   539.3      True\n",
      "| 147   |  0.01827 |   0.01964 |   543.0      False\n",
      "| 148   |  0.01831 |   0.01959 |   546.6      False\n",
      "| 149   |  0.01833 |   0.01959 |   550.3      False\n",
      "| 150   |  0.01836 |   0.01957 |   554.0      True\n",
      "| 151   |  0.01824 |   0.01956 |   557.7      True\n",
      "| 152   |  0.01818 |   0.01960 |   561.5      False\n",
      "| 153   |  0.01819 |   0.01962 |   565.2      False\n",
      "| 154   |  0.01817 |   0.01958 |   568.9      False\n",
      "| 155   |  0.01813 |   0.01960 |   572.6      False\n",
      "| 156   |  0.01813 |   0.01950 |   576.3      True\n",
      "| 157   |  0.01810 |   0.01959 |   579.9      False\n",
      "| 158   |  0.01812 |   0.01957 |   583.6      False\n",
      "| 159   |  0.01812 |   0.01963 |   587.3      False\n",
      "| 160   |  0.01809 |   0.01960 |   591.0      False\n",
      "| 161   |  0.01804 |   0.01955 |   594.7      False\n",
      "| 162   |  0.01803 |   0.01961 |   598.4      False\n",
      "| 163   |  0.01802 |   0.01956 |   602.1      False\n",
      "| 164   |  0.01798 |   0.01960 |   605.8      False\n",
      "| 165   |  0.01799 |   0.01959 |   609.5      False\n",
      "| 166   |  0.01794 |   0.01955 |   613.2      False\n",
      "| 167   |  0.01792 |   0.01959 |   617.0      False\n",
      "| 168   |  0.01793 |   0.01959 |   620.7      False\n",
      "| 169   |  0.01789 |   0.01959 |   624.4      False\n",
      "| 170   |  0.01790 |   0.01964 |   628.1      False\n",
      "| 171   |  0.01788 |   0.01956 |   631.7      False\n",
      "| 172   |  0.01787 |   0.01959 |   635.3      False\n",
      "| 173   |  0.01785 |   0.01950 |   639.0      False\n",
      "| 174   |  0.01783 |   0.01955 |   642.7      False\n",
      "| 175   |  0.01784 |   0.01953 |   646.3      False\n",
      "| 176   |  0.01778 |   0.01954 |   650.0      False\n",
      "| 177   |  0.01774 |   0.01958 |   653.6      False\n",
      "| 178   |  0.01779 |   0.01960 |   657.2      False\n",
      "| 179   |  0.01774 |   0.01954 |   660.9      False\n",
      "| 180   |  0.01777 |   0.01957 |   664.6      False\n",
      "| 181   |  0.01776 |   0.01966 |   668.2      False\n",
      "| 182   |  0.01771 |   0.01951 |   671.9      False\n",
      "| 183   |  0.01775 |   0.01970 |   675.6      False\n",
      "| 184   |  0.01773 |   0.01954 |   679.4      False\n",
      "| 185   |  0.01768 |   0.01961 |   683.1      False\n",
      "| 186   |  0.01769 |   0.01954 |   686.9      False\n",
      "| 187   |  0.01768 |   0.01958 |   690.7      False\n",
      "| 188   |  0.01763 |   0.01961 |   694.4      False\n",
      "| 189   |  0.01762 |   0.01958 |   698.2      False\n",
      "| 190   |  0.01759 |   0.01957 |   701.9      False\n",
      "| 191   |  0.01756 |   0.01964 |   705.5      False\n",
      "| 192   |  0.01759 |   0.01964 |   709.2      False\n",
      "| 193   |  0.01757 |   0.01966 |   712.8      False\n",
      "| 194   |  0.01755 |   0.01955 |   716.5      False\n",
      "| 195   |  0.01753 |   0.01960 |   720.2      False\n",
      "| 196   |  0.01753 |   0.01957 |   723.7      False\n",
      "| 197   |  0.01748 |   0.01958 |   727.3      False\n",
      "| 198   |  0.01750 |   0.01960 |   730.9      False\n",
      "| 199   |  0.01744 |   0.01961 |   734.4      False\n",
      "| 200   |  0.01744 |   0.01961 |   738.1      False\n",
      "| 201   |  0.01741 |   0.01971 |   741.7      False\n",
      "| 202   |  0.01737 |   0.01964 |   745.3      False\n",
      "| 203   |  0.01737 |   0.01961 |   748.9      False\n",
      "| 204   |  0.01738 |   0.01960 |   752.5      False\n",
      "| 205   |  0.01737 |   0.01952 |   756.1      False\n",
      "| 206   |  0.01731 |   0.01962 |   759.6      False\n",
      "Early stopping occured at epoch 206\n",
      "Training done in 759.638 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at TabNet1017/weight/10-18_1714_fold2.zip\n",
      "\n",
      "model saved at: TabNet1017/weight/10-18_1714_fold2\n",
      "\n",
      "[Fold No.  3]\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s3616\\anaconda3\\envs\\moa\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1     |  0.51504 |   0.19968 |   3.6        True\n",
      "| 2     |  0.06112 |   0.02844 |   7.4        True\n",
      "| 3     |  0.03047 |   0.02606 |   11.0       True\n",
      "| 4     |  0.02676 |   0.02456 |   14.7       True\n",
      "| 5     |  0.02537 |   0.02394 |   18.4       True\n",
      "| 6     |  0.02449 |   0.02321 |   22.2       True\n",
      "| 7     |  0.02397 |   0.02302 |   26.0       True\n",
      "| 8     |  0.02351 |   0.02262 |   29.7       True\n",
      "| 9     |  0.02286 |   0.02197 |   33.4       True\n",
      "| 10    |  0.02248 |   0.02169 |   37.2       True\n",
      "| 11    |  0.02219 |   0.02149 |   40.9       True\n",
      "| 12    |  0.02197 |   0.02140 |   44.7       True\n",
      "| 13    |  0.02180 |   0.02131 |   48.4       True\n",
      "| 14    |  0.02166 |   0.02125 |   52.1       True\n",
      "| 15    |  0.02155 |   0.02113 |   55.8       True\n",
      "| 16    |  0.02149 |   0.02118 |   59.5       False\n",
      "| 17    |  0.02137 |   0.02109 |   63.2       True\n",
      "| 18    |  0.02135 |   0.02102 |   67.0       True\n",
      "| 19    |  0.02128 |   0.02096 |   70.7       True\n",
      "| 20    |  0.02130 |   0.02103 |   74.4       False\n",
      "| 21    |  0.02124 |   0.02102 |   78.1       False\n",
      "| 22    |  0.02122 |   0.02094 |   81.8       True\n",
      "| 23    |  0.02123 |   0.02088 |   85.6       True\n",
      "| 24    |  0.02108 |   0.02099 |   89.3       False\n",
      "| 25    |  0.02101 |   0.02087 |   93.0       True\n",
      "| 26    |  0.02098 |   0.02081 |   96.7       True\n",
      "| 27    |  0.02087 |   0.02081 |   100.4      True\n",
      "| 28    |  0.02083 |   0.02075 |   104.1      True\n",
      "| 29    |  0.02083 |   0.02089 |   107.8      False\n",
      "| 30    |  0.02086 |   0.02089 |   111.5      False\n",
      "| 31    |  0.02079 |   0.02084 |   115.2      False\n",
      "| 32    |  0.02075 |   0.02072 |   118.9      True\n",
      "| 33    |  0.02074 |   0.02094 |   122.6      False\n",
      "| 34    |  0.02074 |   0.02078 |   126.3      False\n",
      "| 35    |  0.02064 |   0.02069 |   130.0      True\n",
      "| 36    |  0.02061 |   0.02060 |   133.8      True\n",
      "| 37    |  0.02061 |   0.02061 |   137.5      False\n",
      "| 38    |  0.02057 |   0.02061 |   141.2      False\n",
      "| 39    |  0.02060 |   0.02060 |   144.9      True\n",
      "| 40    |  0.02053 |   0.02059 |   148.6      True\n",
      "| 41    |  0.02051 |   0.02057 |   152.3      True\n",
      "| 42    |  0.02046 |   0.02105 |   156.0      False\n",
      "| 43    |  0.02046 |   0.02048 |   159.7      True\n",
      "| 44    |  0.02044 |   0.02051 |   163.5      False\n",
      "| 45    |  0.02038 |   0.02048 |   167.2      True\n",
      "| 46    |  0.02037 |   0.02047 |   171.0      True\n",
      "| 47    |  0.02036 |   0.02041 |   174.7      True\n",
      "| 48    |  0.02040 |   0.02037 |   178.5      True\n",
      "| 49    |  0.02037 |   0.02040 |   182.2      False\n",
      "| 50    |  0.02045 |   0.02152 |   185.9      False\n",
      "| 51    |  0.02061 |   0.02072 |   189.6      False\n",
      "| 52    |  0.02059 |   0.02079 |   193.3      False\n",
      "| 53    |  0.02050 |   0.02076 |   197.1      False\n",
      "| 54    |  0.02050 |   0.02063 |   200.8      False\n",
      "| 55    |  0.02036 |   0.02048 |   204.5      False\n",
      "| 56    |  0.02038 |   0.02045 |   208.3      False\n",
      "| 57    |  0.02033 |   0.02042 |   212.0      False\n",
      "| 58    |  0.02026 |   0.02035 |   215.8      True\n",
      "| 59    |  0.02019 |   0.02032 |   219.5      True\n",
      "| 60    |  0.02016 |   0.02033 |   223.1      False\n",
      "| 61    |  0.02010 |   0.02029 |   226.7      True\n",
      "| 62    |  0.02013 |   0.02026 |   230.4      True\n",
      "| 63    |  0.02010 |   0.02023 |   234.1      True\n",
      "| 64    |  0.01999 |   0.02024 |   237.7      False\n",
      "| 65    |  0.02002 |   0.02028 |   241.3      False\n",
      "| 66    |  0.02002 |   0.02029 |   245.0      False\n",
      "| 67    |  0.01997 |   0.02023 |   248.7      False\n",
      "| 68    |  0.01997 |   0.02020 |   252.5      True\n",
      "| 69    |  0.01992 |   0.02015 |   256.2      True\n",
      "| 70    |  0.01987 |   0.02011 |   260.0      True\n",
      "| 71    |  0.01983 |   0.02011 |   263.7      False\n",
      "| 72    |  0.01981 |   0.02008 |   267.5      True\n",
      "| 73    |  0.01980 |   0.02009 |   271.2      False\n",
      "| 74    |  0.01974 |   0.02003 |   274.9      True\n",
      "| 75    |  0.01975 |   0.02008 |   278.6      False\n",
      "| 76    |  0.01967 |   0.02001 |   282.4      True\n",
      "| 77    |  0.01965 |   0.02004 |   286.1      False\n",
      "| 78    |  0.01966 |   0.01997 |   289.8      True\n",
      "| 79    |  0.01961 |   0.01995 |   293.6      True\n",
      "| 80    |  0.01958 |   0.01997 |   297.3      False\n",
      "| 81    |  0.01959 |   0.01993 |   301.0      True\n",
      "| 82    |  0.01958 |   0.01992 |   304.8      True\n",
      "| 83    |  0.01956 |   0.01992 |   308.6      True\n",
      "| 84    |  0.01950 |   0.01987 |   312.3      True\n",
      "| 85    |  0.01954 |   0.01985 |   315.9      True\n",
      "| 86    |  0.01946 |   0.01983 |   319.6      True\n",
      "| 87    |  0.01944 |   0.01988 |   323.3      False\n",
      "| 88    |  0.01943 |   0.01983 |   327.0      False\n",
      "| 89    |  0.01941 |   0.01988 |   330.7      False\n",
      "| 90    |  0.01941 |   0.01980 |   334.4      True\n",
      "| 91    |  0.01940 |   0.01978 |   338.2      True\n",
      "| 92    |  0.01938 |   0.01974 |   341.9      True\n",
      "| 93    |  0.01934 |   0.01977 |   345.6      False\n",
      "| 94    |  0.01935 |   0.01977 |   349.3      False\n",
      "| 95    |  0.01932 |   0.01978 |   353.0      False\n",
      "| 96    |  0.01935 |   0.01972 |   356.7      True\n",
      "| 97    |  0.01928 |   0.01972 |   360.4      False\n",
      "| 98    |  0.01923 |   0.01971 |   364.2      True\n",
      "| 99    |  0.01926 |   0.01968 |   367.9      True\n",
      "| 100   |  0.01923 |   0.01971 |   371.6      False\n",
      "| 101   |  0.01925 |   0.01971 |   375.3      False\n",
      "| 102   |  0.01924 |   0.01971 |   379.0      False\n",
      "| 103   |  0.01920 |   0.01968 |   382.7      True\n",
      "| 104   |  0.01918 |   0.01968 |   386.4      False\n",
      "| 105   |  0.01913 |   0.01969 |   390.4      False\n",
      "| 106   |  0.01913 |   0.01966 |   394.1      True\n",
      "| 107   |  0.01915 |   0.01966 |   397.8      True\n",
      "| 108   |  0.01909 |   0.01966 |   401.5      True\n",
      "| 109   |  0.01910 |   0.01964 |   405.3      True\n",
      "| 110   |  0.01911 |   0.01966 |   409.0      False\n",
      "| 111   |  0.01908 |   0.01967 |   412.7      False\n",
      "| 112   |  0.01905 |   0.01963 |   416.4      True\n",
      "| 113   |  0.01903 |   0.01965 |   420.1      False\n",
      "| 114   |  0.01907 |   0.01964 |   423.9      False\n",
      "| 115   |  0.01903 |   0.01961 |   427.6      True\n",
      "| 116   |  0.01899 |   0.01960 |   431.3      True\n",
      "| 117   |  0.01901 |   0.01959 |   435.0      True\n",
      "| 118   |  0.01901 |   0.01958 |   438.8      True\n",
      "| 119   |  0.01902 |   0.01958 |   442.5      False\n",
      "| 120   |  0.01896 |   0.01957 |   446.3      True\n",
      "| 121   |  0.01899 |   0.01958 |   450.0      False\n",
      "| 122   |  0.01891 |   0.01958 |   453.7      False\n",
      "| 123   |  0.01894 |   0.01957 |   457.4      False\n",
      "| 124   |  0.01888 |   0.01963 |   461.2      False\n",
      "| 125   |  0.01888 |   0.01952 |   464.9      True\n",
      "| 126   |  0.01887 |   0.01958 |   468.6      False\n",
      "| 127   |  0.01888 |   0.01954 |   472.3      False\n",
      "| 128   |  0.01885 |   0.01951 |   476.1      True\n",
      "| 129   |  0.01880 |   0.01951 |   479.8      False\n",
      "| 130   |  0.01884 |   0.01948 |   483.5      True\n",
      "| 131   |  0.01882 |   0.01946 |   487.2      True\n",
      "| 132   |  0.01880 |   0.01944 |   490.9      True\n",
      "| 133   |  0.01881 |   0.01947 |   494.7      False\n",
      "| 134   |  0.01877 |   0.01947 |   498.4      False\n",
      "| 135   |  0.01880 |   0.01947 |   502.1      False\n",
      "| 136   |  0.01875 |   0.01948 |   505.8      False\n",
      "| 137   |  0.01874 |   0.01944 |   509.5      False\n",
      "| 138   |  0.01870 |   0.01943 |   513.3      True\n",
      "| 139   |  0.01870 |   0.01941 |   517.0      True\n",
      "| 140   |  0.01867 |   0.01939 |   520.7      True\n",
      "| 141   |  0.01861 |   0.01942 |   524.4      False\n",
      "| 142   |  0.01862 |   0.01942 |   528.2      False\n",
      "| 143   |  0.01863 |   0.01940 |   531.9      False\n",
      "| 144   |  0.01862 |   0.01939 |   535.6      False\n",
      "| 145   |  0.01862 |   0.01938 |   539.3      True\n",
      "| 146   |  0.01856 |   0.01941 |   543.0      False\n",
      "| 147   |  0.01855 |   0.01938 |   546.7      True\n",
      "| 148   |  0.01854 |   0.01939 |   550.4      False\n",
      "| 149   |  0.01851 |   0.01939 |   554.1      False\n",
      "| 150   |  0.01850 |   0.01937 |   557.9      True\n",
      "| 151   |  0.01852 |   0.01930 |   561.7      True\n",
      "| 152   |  0.01848 |   0.01934 |   565.4      False\n",
      "| 153   |  0.01846 |   0.01937 |   569.1      False\n",
      "| 154   |  0.01850 |   0.01932 |   572.9      False\n",
      "| 155   |  0.01851 |   0.01928 |   576.6      True\n",
      "| 156   |  0.01844 |   0.01929 |   580.2      False\n",
      "| 157   |  0.01844 |   0.01933 |   584.0      False\n",
      "| 158   |  0.01844 |   0.01928 |   587.7      False\n",
      "| 159   |  0.01840 |   0.01931 |   591.4      False\n",
      "| 160   |  0.01839 |   0.01931 |   595.1      False\n",
      "| 161   |  0.01838 |   0.01930 |   598.8      False\n",
      "| 162   |  0.01839 |   0.01929 |   602.5      False\n",
      "| 163   |  0.01840 |   0.01930 |   606.2      False\n",
      "| 164   |  0.01835 |   0.01931 |   609.9      False\n",
      "| 165   |  0.01837 |   0.01931 |   613.6      False\n",
      "| 166   |  0.01835 |   0.01931 |   617.3      False\n",
      "| 167   |  0.01833 |   0.01929 |   621.0      False\n",
      "| 168   |  0.01834 |   0.01929 |   624.7      False\n",
      "| 169   |  0.01830 |   0.01928 |   628.4      False\n",
      "| 170   |  0.01830 |   0.01927 |   632.1      True\n",
      "| 171   |  0.01826 |   0.01928 |   635.8      False\n",
      "| 172   |  0.01827 |   0.01932 |   639.5      False\n",
      "| 173   |  0.01824 |   0.01929 |   643.2      False\n",
      "| 174   |  0.01823 |   0.01923 |   646.9      True\n",
      "| 175   |  0.01822 |   0.01926 |   650.6      False\n",
      "| 176   |  0.01827 |   0.01930 |   654.3      False\n",
      "| 177   |  0.01831 |   0.01928 |   658.1      False\n",
      "| 178   |  0.01822 |   0.01921 |   661.8      True\n",
      "| 179   |  0.01821 |   0.01922 |   665.5      False\n",
      "| 180   |  0.01821 |   0.01921 |   669.3      True\n",
      "| 181   |  0.01822 |   0.01918 |   673.0      True\n",
      "| 182   |  0.01819 |   0.01923 |   676.7      False\n",
      "| 183   |  0.01812 |   0.01922 |   680.4      False\n",
      "| 184   |  0.01813 |   0.01922 |   684.1      False\n",
      "| 185   |  0.01807 |   0.01921 |   687.8      False\n",
      "| 186   |  0.01807 |   0.01922 |   691.5      False\n",
      "| 187   |  0.01810 |   0.01927 |   695.2      False\n",
      "| 188   |  0.01809 |   0.01918 |   698.9      False\n",
      "| 189   |  0.01810 |   0.01918 |   702.5      True\n",
      "| 190   |  0.01810 |   0.01920 |   706.2      False\n",
      "| 191   |  0.01807 |   0.01927 |   709.9      False\n",
      "| 192   |  0.01809 |   0.01928 |   713.6      False\n",
      "| 193   |  0.01802 |   0.01926 |   717.3      False\n",
      "| 194   |  0.01808 |   0.01965 |   721.0      False\n",
      "| 195   |  0.01812 |   0.01933 |   724.7      False\n",
      "| 196   |  0.01809 |   0.01927 |   728.4      False\n",
      "| 197   |  0.01806 |   0.01924 |   732.1      False\n",
      "| 198   |  0.01802 |   0.01924 |   735.7      False\n",
      "| 199   |  0.01803 |   0.01926 |   739.5      False\n",
      "| 200   |  0.01800 |   0.01935 |   743.2      False\n",
      "| 201   |  0.01800 |   0.01930 |   746.9      False\n",
      "| 202   |  0.01795 |   0.01922 |   750.6      False\n",
      "| 203   |  0.01797 |   0.01923 |   754.2      False\n",
      "| 204   |  0.01797 |   0.01922 |   758.0      False\n",
      "| 205   |  0.01796 |   0.01918 |   761.7      False\n",
      "| 206   |  0.01793 |   0.01919 |   765.4      False\n",
      "| 207   |  0.01788 |   0.01931 |   769.1      False\n",
      "| 208   |  0.01784 |   0.01924 |   772.8      False\n",
      "| 209   |  0.01786 |   0.01924 |   776.5      False\n",
      "| 210   |  0.01781 |   0.01928 |   780.2      False\n",
      "| 211   |  0.01781 |   0.01924 |   783.9      False\n",
      "| 212   |  0.01780 |   0.01924 |   787.6      False\n",
      "| 213   |  0.01779 |   0.01924 |   791.3      False\n",
      "| 214   |  0.01790 |   0.02035 |   795.0      False\n",
      "| 215   |  0.01800 |   0.01968 |   798.7      False\n",
      "| 216   |  0.01790 |   0.01915 |   802.4      True\n",
      "| 217   |  0.01785 |   0.01916 |   806.1      False\n",
      "| 218   |  0.01782 |   0.01913 |   809.8      True\n",
      "| 219   |  0.01775 |   0.01911 |   813.5      True\n",
      "| 220   |  0.01770 |   0.01919 |   817.2      False\n",
      "| 221   |  0.01771 |   0.01915 |   820.9      False\n",
      "| 222   |  0.01773 |   0.01917 |   824.6      False\n",
      "| 223   |  0.01765 |   0.01916 |   828.3      False\n",
      "| 224   |  0.01770 |   0.01920 |   832.0      False\n",
      "| 225   |  0.01774 |   0.01919 |   835.7      False\n",
      "| 226   |  0.01773 |   0.01912 |   839.4      False\n",
      "| 227   |  0.01768 |   0.01914 |   843.1      False\n",
      "| 228   |  0.01763 |   0.01915 |   846.8      False\n",
      "| 229   |  0.01764 |   0.01917 |   850.5      False\n",
      "| 230   |  0.01765 |   0.01917 |   854.2      False\n",
      "| 231   |  0.01756 |   0.01920 |   857.9      False\n",
      "| 232   |  0.01759 |   0.01913 |   861.6      False\n",
      "| 233   |  0.01757 |   0.01915 |   865.3      False\n",
      "| 234   |  0.01760 |   0.01914 |   869.0      False\n",
      "| 235   |  0.01755 |   0.01925 |   872.8      False\n",
      "| 236   |  0.01753 |   0.01923 |   876.5      False\n",
      "| 237   |  0.01761 |   0.01925 |   880.2      False\n",
      "| 238   |  0.01769 |   0.01921 |   883.9      False\n",
      "| 239   |  0.01761 |   0.01925 |   887.7      False\n",
      "| 240   |  0.01764 |   0.01923 |   891.4      False\n",
      "| 241   |  0.01758 |   0.01923 |   895.1      False\n",
      "| 242   |  0.01761 |   0.01933 |   898.7      False\n",
      "| 243   |  0.01767 |   0.01921 |   902.4      False\n",
      "| 244   |  0.01761 |   0.01922 |   906.2      False\n",
      "| 245   |  0.01760 |   0.01924 |   909.9      False\n",
      "| 246   |  0.01758 |   0.01916 |   913.5      False\n",
      "| 247   |  0.01749 |   0.01921 |   917.3      False\n",
      "| 248   |  0.01753 |   0.01919 |   921.0      False\n",
      "| 249   |  0.01749 |   0.01916 |   924.7      False\n",
      "| 250   |  0.01747 |   0.01920 |   928.4      False\n",
      "| 251   |  0.01746 |   0.01929 |   932.1      False\n",
      "| 252   |  0.01749 |   0.01924 |   935.8      False\n",
      "| 253   |  0.01743 |   0.01919 |   939.5      False\n",
      "| 254   |  0.01745 |   0.01923 |   943.3      False\n",
      "| 255   |  0.01748 |   0.01923 |   947.0      False\n",
      "| 256   |  0.01743 |   0.01925 |   950.7      False\n",
      "| 257   |  0.01742 |   0.01919 |   954.4      False\n",
      "| 258   |  0.01738 |   0.01932 |   958.3      False\n",
      "| 259   |  0.01736 |   0.01931 |   962.0      False\n",
      "| 260   |  0.01735 |   0.01935 |   965.7      False\n",
      "| 261   |  0.01735 |   0.01933 |   969.4      False\n",
      "| 262   |  0.01727 |   0.01932 |   973.0      False\n",
      "| 263   |  0.01726 |   0.01938 |   976.6      False\n",
      "| 264   |  0.01723 |   0.01940 |   980.3      False\n",
      "| 265   |  0.01731 |   0.01939 |   984.0      False\n",
      "| 266   |  0.01723 |   0.01944 |   987.8      False\n",
      "| 267   |  0.01723 |   0.01944 |   991.6      False\n",
      "| 268   |  0.01728 |   0.01944 |   995.3      False\n",
      "| 269   |  0.01724 |   0.01949 |   998.9      False\n",
      "Early stopping occured at epoch 269\n",
      "Training done in 998.944 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at TabNet1017/weight/10-18_1714_fold3.zip\n",
      "\n",
      "model saved at: TabNet1017/weight/10-18_1714_fold3\n",
      "Wall time: 56min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for fold in range(4):\n",
    "    run_training(df, fold, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELNAME}/{now}_params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"nn_params\": {\"dropout\": trial.suggest_uniform(\"dropout\", 0.1, 0.8), \n",
    "                      \"num_layers\": trial.suggest_int(\"num_layers\", 1, 7),\n",
    "                      \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 2048),\n",
    "                      \"activation\": trial.suggest_categorical(\"activation\", [\"relu\", \"prelu\"]),\n",
    "                      \"batchnorm\": trial.suggest_categorical(\"batchnorm\", [True, False])},\n",
    "        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\"]),\n",
    "        \"optim_params\": {\"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3)},\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"scdl_params\": {\"threshold\": 0.00001},\n",
    "    }\n",
    "    loss_all = []\n",
    "    for fold_ in range(4):\n",
    "        loss_tmp = run_training(df, fold, params, save_model=False)\n",
    "        loss_all.append(loss_tmp)\n",
    "    return np.mean(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
