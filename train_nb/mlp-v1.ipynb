{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../script/')\n",
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from adabelief_pytorch import AdaBelief\n",
    "\n",
    "import utils\n",
    "import models\n",
    "import train as trainer\n",
    "DEVICE = \"cuda\"\n",
    "# DEVICE = \"cpu\"\n",
    "EPOCHS = 3000\n",
    "MODELNAME = \"Baseline1122\"\n",
    "if not exists(MODELNAME):\n",
    "    os.makedirs(f\"{MODELNAME}/tensorboard\")\n",
    "now = datetime.now()\n",
    "now = str(now)[5:17].replace(\" \", \"_\").replace(\":\", \"\")\n",
    "writer = SummaryWriter(log_dir=f\"{MODELNAME}/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/folds/train.csv\")\n",
    "with open(\"../input/folds/targets\", \"r\") as f:\n",
    "    targets = f.read().split(\"\\n\")\n",
    "with open(\"../input/folds/features\", \"r\") as f:\n",
    "    features = f.read().split(\"\\n\")\n",
    "targets = targets[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: クラスタごとに並べてConv1d\n",
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, num_layers=3, dropout=.2, hidden_size=256, activation=\"relu\", batchnorm=True, weight_norm=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.utils.weight_norm(nn.Linear(num_features if len(layers)==0 else hidden_size, hidden_size, bias=(not batchnorm))))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            if activation == \"relu\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == \"prelu\":\n",
    "                layers.append(nn.PReLU())\n",
    "            else:\n",
    "                raise RuntimeError(f'{activation} is not implemented')\n",
    "        # layers.append(nn.utils.weight_norm(nn.Linear(hidden_size, num_targets)))\n",
    "        layers.append(nn.Linear(hidden_size, num_targets))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_output_bias(model, df, targets):   \n",
    "    init_bias = np.array([])\n",
    "    for target in targets:\n",
    "        try:\n",
    "            neg, pos = np.bincount(df[target])\n",
    "        except ValueError:\n",
    "            neg, pos = np.array([df.shape[0], 0.01])\n",
    "        init_bias_ = np.log([pos/neg])\n",
    "        init_bias = np.append(init_bias, init_bias_)\n",
    "    model.model[-1].bias.data = torch.tensor(init_bias, dtype=torch.float32)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "    def forward(self, x, target, smoothing=0.2):\n",
    "        confidence = 1. - smoothing\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "from utils import LabelSmoothingCrossEntropy\n",
    "\n",
    "# criterion = LabelSmoothingCrossEntropy()\n",
    "# loss = criterion(outputs, targets)\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, fold, params, hp_tune=False):\n",
    "\n",
    "    save_model = False if hp_tune else True\n",
    "    print(f'\\n[Fold No.{fold:>3}]')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    x_tr = train_df[features].to_numpy()\n",
    "    x_va = valid_df[features].to_numpy()\n",
    "\n",
    "    y_tr = train_df[targets].to_numpy()\n",
    "    y_va = valid_df[targets].to_numpy()\n",
    "\n",
    "    dataset_tr = utils.MoaDataset(x_tr, y_tr)\n",
    "    loader_tr = torch.utils.data.DataLoader(dataset_tr, batch_size=512, num_workers=2, pin_memory=True)\n",
    "    dataset_va = utils.MoaDataset(x_va, y_va)\n",
    "    loader_va = torch.utils.data.DataLoader(dataset_va, batch_size=512, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = BaseLine(num_features=x_tr.shape[1], num_targets=y_tr.shape[1], **params['nn_params'])\n",
    "    model = set_output_bias(model, train_df, targets)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    if params[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), **params[\"optim_params\"])\n",
    "    elif params[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), **params[\"optim_params\"])\n",
    "    elif params[\"optimizer\"] == \"AdamW\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), **params[\"optim_params\"])\n",
    "    elif params[\"optimizer\"] == \"AdaBelief\":\n",
    "        optimizer = AdaBelief(model.parameters(), **params[\"optim_params\"])\n",
    "    else:\n",
    "        raise RuntimeError(f'{params[\"optimizer\"]} is not implemented')\n",
    "\n",
    "    if params[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", **params[\"scdl_params\"])\n",
    "    elif params[\"scheduler\"] == \"CosineAnnealingLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, **params[\"scdl_params\"])\n",
    "    elif params[\"scheduler\"] == \"none\": \n",
    "        print(\"No scheduling will be applied\")\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda ep: 1**ep)\n",
    "    else:\n",
    "        raise RuntimeError(f'{params[\"scheduler\"]} is not implemented')\n",
    "\n",
    "    eng = utils.Engine(model, optimizer, device=DEVICE)\n",
    "\n",
    "    del df, train_df, valid_df, x_tr, x_va, y_tr, y_va\n",
    "    gc.collect()\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    print(f'Training state is shown in {MODELNAME}/tensorboard')\n",
    "    filename = f\"{MODELNAME}/{now}_fold{fold}.pt\"\n",
    "\n",
    "    loss_best = np.inf\n",
    "    patience = 25\n",
    "    patience_cnt = 0\n",
    "    for ep in range(EPOCHS):\n",
    "        loss_tr = eng.train(loader_tr)\n",
    "        loss_tr_nodrop = eng.validate(loader_tr)\n",
    "        loss_va = eng.validate(loader_va)\n",
    "        scheduler.step(loss_va)\n",
    "        print(f'Ep.{ep:>3}/{EPOCHS:>3}, patience:{patience_cnt:>2}/{patience:>2}, train:{loss_tr:.6}, tr_nodrop:{loss_tr_nodrop:.6}, valid:{loss_va:.6}', end='\\r')\n",
    "        writer.add_scalars(f'{now}/fold{fold}', {'train':loss_tr, 'tr_nodrop':loss_tr_nodrop, 'valid':loss_va}, ep)\n",
    "        if loss_va < loss_best:\n",
    "            patience_cnt = 0\n",
    "            loss_best = loss_va\n",
    "            if save_model:\n",
    "                torch.save(model.model.state_dict(), filename)\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "        if patience_cnt > patience:\n",
    "            break\n",
    "\n",
    "    print(\"\\nmodel saved at:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"nn_params\": {\"dropout\": 0.5, \"num_layers\":4, \"hidden_size\": 512, \"activation\": \"relu\", \"batchnorm\": True, \"weight_norm\": True},\n",
    "    \"optimizer\": \"Adam\",\n",
    "    # # SGD\n",
    "    # \"optim_params\": {\"lr\":1e-4, \"momentum\": 0.3, \"weight_decay\": 0.2, \"dampening\": 0, \"nesterov\": False},\n",
    "    # Adam\n",
    "    \"optim_params\": {\"lr\":1e-2, \"betas\": (0.9, 0.999), \"eps\": 1e-08, \"weight_decay\": 1.2e-6, \"amsgrad\": False},\n",
    "    # # Adabelief \n",
    "    # \"optim_params\": {\"lr\": 1e-2, \"eps\":1e-16, \"betas\": (0.9,0.999), \"weight_decay\": 1.2e-6, \"weight_decouple\": False, \"rectify\": True, \"fixed_decay\": False, \"amsgrad\": False},\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"scdl_params\": {\"threshold\": 1e-5, \"patience\": 3}\n",
    "    # # ReduceLROnPlateau\n",
    "    # \"scdl_params\": {\"threshold\": 1e-5, \"patience\": 3}\n",
    "    # # CosineAnnealingLR\n",
    "    # \"scdl_params\": {\"T_max\":8, \"eta_min\":0, \"last_epoch\":-1}\n",
    "}\n",
    "# 0.02355, 0.03 on momentum:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELNAME}/{now}_params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold No.  0]\n",
      "Training state is shown in Baseline1122/tensorboard\n",
      "Ep. 47/3000, patience:25/25, train:0.0148142, tr_nodrop:0.0140421, valid:0.0164846\n",
      "model saved at: Baseline1122/11-26_0045_fold0.pt\n",
      "\n",
      "[Fold No.  1]\n",
      "Training state is shown in Baseline1122/tensorboard\n",
      "Ep. 43/3000, patience:25/25, train:0.0147418, tr_nodrop:0.0139523, valid:0.0160776\n",
      "model saved at: Baseline1122/11-26_0045_fold1.pt\n",
      "\n",
      "[Fold No.  2]\n",
      "Training state is shown in Baseline1122/tensorboard\n",
      "Ep. 45/3000, patience:25/25, train:0.0144253, tr_nodrop:0.0136617, valid:0.0164775\n",
      "model saved at: Baseline1122/11-26_0045_fold2.pt\n",
      "\n",
      "[Fold No.  3]\n",
      "Training state is shown in Baseline1122/tensorboard\n",
      "Ep. 51/3000, patience:25/25, train:0.0144511, tr_nodrop:0.0136164, valid:0.0159169\n",
      "model saved at: Baseline1122/11-26_0045_fold3.pt\n",
      "\n",
      "[Fold No.  4]\n",
      "Training state is shown in Baseline1122/tensorboard\n",
      "Ep. 48/3000, patience:25/25, train:0.0145504, tr_nodrop:0.0138211, valid:0.0165258\n",
      "model saved at: Baseline1122/11-26_0045_fold4.pt\n",
      "Wall time: 33min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for fold in range(5):\n",
    "    run_training(df, fold, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Get CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold No.  4] Predicting...\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros((df.shape[0], len(targets)))\n",
    "for fold in range(5):\n",
    "    filename = f\"{MODELNAME}/{now}_fold{fold}.pt\"\n",
    "    print(f'[Fold No.{fold:>3}] Predicting...', end='\\r')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    va_idx = df[df.kfold == fold].index\n",
    "    \n",
    "    x_tr = train_df[features].to_numpy()\n",
    "    x_va = valid_df[features].to_numpy()\n",
    "\n",
    "    y_tr = train_df[targets].to_numpy()\n",
    "    y_va = valid_df[targets].to_numpy()\n",
    "\n",
    "    dataset_tr = utils.MoaDataset(x_tr, y_tr)\n",
    "    loader_tr = torch.utils.data.DataLoader(dataset_tr, batch_size=512, num_workers=2, pin_memory=True)\n",
    "    dataset_va = utils.MoaDataset(x_va, y_va)\n",
    "    loader_va = torch.utils.data.DataLoader(dataset_va, batch_size=512, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    model = BaseLine(num_features=x_tr.shape[1], num_targets=y_tr.shape[1], **params['nn_params'])\n",
    "    \n",
    "    weight = torch.load(filename, map_location=torch.device(DEVICE))\n",
    "    weight = OrderedDict([(f'model.{k}', v) for k, v in weight.items()])\n",
    "    model.load_state_dict(weight)\n",
    "    \n",
    "    model.eval()\n",
    "    ps = []\n",
    "    for ind, batch in enumerate(loader_va):\n",
    "        ps.append(torch.sigmoid(model(batch[\"x\"])).detach().cpu().numpy())\n",
    "    ps = np.vstack(ps)\n",
    "    predictions[va_idx] += ps\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score               : 0.016269\n"
     ]
    }
   ],
   "source": [
    "def log_loss_metric(y_true, y_pred):\n",
    "    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = - np.mean(np.mean(y_true * np.log(y_pred_clip) + (1 - y_true) * np.log(1 - y_pred_clip), axis = 1))\n",
    "    return loss\n",
    "print(f'CV score               : {log_loss_metric(df[targets].values, predictions):.6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "oof.iloc[:, 1:] = predictions\n",
    "oof.to_csv(f\"{MODELNAME}/oof.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score w/ postprocess: 0.0161582\n"
     ]
    }
   ],
   "source": [
    "predictions_ = predictions.copy()\n",
    "predictions_ = np.clip(predictions_,0.0005,0.999)\n",
    "predictions_[df[\"cp_type_ctl_vehicle\"]==1] = 0\n",
    "print(f'CV score w/ postprocess: {log_loss_metric(df[targets].values, predictions_):.6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score w/ postprocess: 0.0161582\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'CV score w/ postprocess: {log_loss_metric(df[targets].values, predictions_):.6}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
