{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../script/')\n",
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import optuna\n",
    "\n",
    "import utils\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "EPOCHS = 50000\n",
    "MODELNAME = \"LightGBM1019\"\n",
    "if not exists(f\"{MODELNAME}/scores\"):\n",
    "    os.makedirs(f\"{MODELNAME}/scores\")\n",
    "if not exists(f\"{MODELNAME}/weight\"):\n",
    "    os.makedirs(f\"{MODELNAME}/weight\")\n",
    "now = datetime.now()\n",
    "now = str(now)[5:17].replace(\" \", \"_\").replace(\":\", \"\")\n",
    "# writer = SummaryWriter(log_dir=f\"{MODELNAME}/tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "def split_data():\n",
    "    print(\"Split data\")\n",
    "    path_fold = \"../input/folds/train_folds.csv\"\n",
    "    if not exists(path_fold):\n",
    "        df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
    "        df.loc[:, \"kfold\"] = -1\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        targets = df.drop(\"sig_id\", axis=1).values\n",
    "\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "        for fold_, (tr_, va_) in enumerate(mskf.split(X=df, y=targets)):\n",
    "            df.loc[va_, \"kfold\"] = fold_\n",
    "        df.to_csv(path_fold, index=False)\n",
    "        print(f\"Created: {path_fold}\")\n",
    "    else:\n",
    "        print(\"Skipped: already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pca(df, n_comp_c=15, n_comp_g=50, use_diff=False, drop=False):\n",
    "    \"\"\"How to Extract\n",
    "        with open(f'{MODELNAME}/weight/scaler.npy', 'rb') as f:\n",
    "            mean_c = np.load(f)\n",
    "            var_c = np.load(f)\n",
    "            mean_g = np.load(f)\n",
    "            var_g = np.load(f)\n",
    "    \"\"\"\n",
    "    c_feats = [col for col in df.columns if col.startswith('c-')]\n",
    "    g_feats = [col for col in df.columns if col.startswith('g-')]\n",
    "\n",
    "    c_pca = PCA(n_components=n_comp_c, random_state=42).fit_transform(df[c_feats])\n",
    "    g_pca = PCA(n_components=n_comp_g, random_state=42).fit_transform(df[g_feats])\n",
    "    \n",
    "    cp = pd.DataFrame(c_pca, columns=[f'c_pca{i}' for i in range(n_comp_c)])\n",
    "    gp = pd.DataFrame(g_pca, columns=[f'g_pca{i}' for i in range(n_comp_g)])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cp[cp.columns] = scaler.fit_transform(cp)\n",
    "    mean_c, var_c = scaler.mean_, scaler.var_\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    gp[gp.columns] = scaler.fit_transform(gp)\n",
    "    mean_g, var_g = scaler.mean_, scaler.var_\n",
    "\n",
    "    with open(f'{MODELNAME}/weight/scaler.npy', 'wb') as f:\n",
    "        np.save(f, mean_c)\n",
    "        np.save(f, var_c)\n",
    "        np.save(f, mean_g)\n",
    "        np.save(f, var_g)\n",
    "\n",
    "    if use_diff:\n",
    "        for i in range(1, cp.shape[1]):\n",
    "            cp[f'c_pca{i}'] = cp[f'c_pca{i}'] - cp[f'c_pca0']\n",
    "        for i in range(1, gp.shape[1]):\n",
    "            gp[f'g_pca{i}'] = gp[f'g_pca{i}'] - gp[f'g_pca0']\n",
    "    \n",
    "    df = pd.concat([df, cp, gp], axis=1)\n",
    "    if drop:\n",
    "        df.drop(c_feats, axis=1, inplace=True)\n",
    "        df.drop(g_feats, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WIP] cp_time, cp_doseが同じグループごとに、ターゲットと相関の高い特徴値を選択した列を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_selective_feats(df, targets):\n",
    "    c_feats = [col for col in df.columns if col.startswith('c-')]\n",
    "    g_feats = [col for col in df.columns if col.startswith('g-')]\n",
    "    for i, target in enumerate(targets):\n",
    "        print(f'adding selective features... {i:>3}/{len(targets):>3}')\n",
    "        df[f'cfeat_for_{target}'] = np.nan\n",
    "        df[f'gfeat_for_{target}'] = np.nan\n",
    "        display(df)\n",
    "        for t in df['cp_time'].unique():\n",
    "            for d in df['cp_dose'].unique():\n",
    "                subset = df.query(f'cp_time == {t} and cp_dose == {d}')\n",
    "                \n",
    "                # [c features]\n",
    "                # Know which feature is the best in the subset\n",
    "                corr = np.zeros((len(c_feats), len(targets)))\n",
    "                for fi, feat in enumerate(c_feats):\n",
    "                    for ti, target_ in enumerate(targets):\n",
    "                        corr[fi,ti] = subset[target_].corr(subset[feat])\n",
    "                corr = pd.DataFrame(corr, index=c_feats, columns=targets)\n",
    "\n",
    "                # Insert values of selected feature\n",
    "                selected_feats = corr.idxmax()\n",
    "                df[f'cfeat_for_{target}'].iloc[subset.index] = df[selected_feats[target]].iloc[subset.index]\n",
    "                \n",
    "                # [g features]\n",
    "                # Know which feature is the best in the subset\n",
    "                corr = np.zeros((len(g_feats), len(targets)))\n",
    "                for fi, feat in enumerate(g_feats):\n",
    "                    for ti, target in enumerate(targets):\n",
    "                        corr[fi,ti] = subset[target].corr(subset[feat])\n",
    "                corr = pd.DataFrame(corr, index=g_feats, columns=targets)\n",
    "\n",
    "                # Insert values of selected feature\n",
    "                selected_feats = corr.idxmax()\n",
    "                df[f'gfeat_for_{target}'].iloc[subset.index] = df[selected_feats[target]].iloc[subset.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
    "    \n",
    "    # Label encoding\n",
    "    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n",
    "               \"cp_time\":{24:0, 48:1, 72:2},\n",
    "               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n",
    "    for col in ['cp_type', 'cp_time', 'cp_dose']:\n",
    "        df[col] = df[col].map(mapping[col])\n",
    "    \n",
    "    folds = pd.read_csv(\"../input/folds/train_folds.csv\")\n",
    "\n",
    "    # Create aux target\n",
    "    # `nsc_labels` means # of labels found in non-scored train set\n",
    "    non_scored_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
    "    targets_non_scored = non_scored_df.drop(\"sig_id\", axis=1).to_numpy().sum(axis=1)\n",
    "    non_scored_df.loc[:, \"nsc_labels\"] = targets_non_scored\n",
    "    drop_cols = [c for c in non_scored_df.columns if c not in (\"nsc_labels\", \"sig_id\")]\n",
    "    non_scored_df = non_scored_df.drop(drop_cols, axis=1)\n",
    "    folds = folds.merge(non_scored_df, on=\"sig_id\", how=\"left\")\n",
    "\n",
    "    # Feature engineering\n",
    "#     df = add_pca(df)\n",
    "    \n",
    "    targets = folds.drop([\"sig_id\", \"kfold\"], axis=1).columns\n",
    "    features = df.drop(\"sig_id\", axis=1).columns\n",
    "    df = df.merge(folds, on=\"sig_id\", how=\"left\")\n",
    "    \n",
    "    # TODO: Feature engineering (target related)\n",
    "    # df = add_selective_feats(df, targets)\n",
    "\n",
    "    return df, features, targets\n",
    "\n",
    "# split_data()\n",
    "df, features, targets = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, fold, params, hp_tune=False):\n",
    "    save_model = False if hp_tune else True\n",
    "    print(f'\\n[Fold No.{fold:>3}]')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    x_tr = train_df[features].to_numpy()\n",
    "    x_va = valid_df[features].to_numpy()\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    loss_tr, loss_va = [], []\n",
    "    for i, target in enumerate(targets):\n",
    "        print(f'Target No.{i:>3} / {len(targets):>3}',end='\\r')\n",
    "        y_tr = train_df[target].to_numpy()\n",
    "        y_va = valid_df[target].to_numpy()\n",
    "        dataset_tr = lgb.Dataset(x_tr, label=y_tr)\n",
    "        dataset_va = lgb.Dataset(x_va, label=y_va, reference=dataset_tr)\n",
    "\n",
    "        model = lgb.train(params, dataset_tr, EPOCHS, valid_sets=[dataset_tr, dataset_va],\n",
    "                          verbose_eval=False, early_stopping_rounds=100)\n",
    "\n",
    "        filename = f\"{MODELNAME}/weight/tgt{i}_{now}_fold{fold}.txt\"\n",
    "\n",
    "        preds_tr = model.predict(x_tr, num_iteration=model.best_iteration)\n",
    "        preds_va = model.predict(x_va, num_iteration=model.best_iteration)\n",
    "\n",
    "        loss_tr.append(log_loss(y_tr, preds_tr, labels=[0,1]).item())\n",
    "        loss_va.append(log_loss(y_va, preds_va, labels=[0,1]).item())\n",
    "\n",
    "        if save_model:\n",
    "            model.save_model(filename, num_iteration=model.best_iteration)\n",
    "\n",
    "    with open(f\"{MODELNAME}/scores/{now}.txt\", \"a\") as f:\n",
    "        f.write(f\"[fold{fold:>2}] {np.mean(loss_tr):.5}, {np.mean(loss_va):.5}\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"\\nmodel saved at:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"metric\": 'binary_logloss',\n",
    "          'objective': 'binary',\n",
    "          'num_leaves': 491,\n",
    "          'min_child_weight': 0.03,\n",
    "          'feature_fraction': 0.3,\n",
    "          'bagging_fraction': 0.4,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"verbose\": -1,\n",
    "          'reg_alpha': 0.4,\n",
    "          'reg_lambda': 0.6,\n",
    "          'random_state': 47,\n",
    "          \"force_col_wise\": True\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# importance = pd.DataFrame(model.feature_importance(), index=features, columns=['importance'])\n",
    "# importance = importance.sort_values(by=['importance'], ascending=False)\n",
    "# plt.figure(figsize=(15,80))\n",
    "# plt.title(f'feature_importance of experiment {now}')\n",
    "# plt.tight_layout()\n",
    "# barlist = plt.barh(importance['importance'].index, importance['importance'].values, height=.9, edgecolor='white')\n",
    "# for v, bar in zip(importance['importance'].values, barlist):\n",
    "#     v_ = np.clip(v / 256, 0, 1)\n",
    "#     bar.set_color((v_, .5, (1-v_)))\n",
    "# plt.savefig(f'{MODELNAME}/{now}_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold No.  0]\n",
      "Target No.  4 / 207\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-97ce54eb4f62>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         model = lgb.train(params, dataset_tr, EPOCHS, valid_sets=[dataset_tr, dataset_va],\n\u001b[1;32m---> 22\u001b[1;33m                           verbose_eval=False, early_stopping_rounds=100)\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{MODELNAME}/weight/tgt{i}_{now}_fold{fold}.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moa\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    250\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moa\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   2370\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   2371\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2372\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   2373\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for fold in range(4):\n",
    "    run_training(df, fold, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELNAME}/{now}_params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"nn_params\": {\"dropout\": trial.suggest_uniform(\"dropout\", 0.1, 0.8), \n",
    "                      \"num_layers\": trial.suggest_int(\"num_layers\", 1, 7),\n",
    "                      \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 2048),\n",
    "                      \"activation\": trial.suggest_categorical(\"activation\", [\"relu\", \"prelu\"]),\n",
    "                      \"batchnorm\": trial.suggest_categorical(\"batchnorm\", [True, False])},\n",
    "        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\"]),\n",
    "        \"optim_params\": {\"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3)},\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"scdl_params\": {\"threshold\": 0.00001},\n",
    "    }\n",
    "    loss_all = []\n",
    "    for fold_ in range(4):\n",
    "        loss_tmp = run_training(df, fold, params, save_model=False)\n",
    "        loss_all.append(loss_tmp)\n",
    "    return np.mean(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
