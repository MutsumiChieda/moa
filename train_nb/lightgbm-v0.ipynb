{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../script/')\n",
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import optuna\n",
    "\n",
    "import utils\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "EPOCHS = 50000\n",
    "MODELNAME = \"LightGBM1019\"\n",
    "if not exists(f\"{MODELNAME}/scores\"):\n",
    "    os.makedirs(f\"{MODELNAME}/scores\")\n",
    "if not exists(f\"{MODELNAME}/weight\"):\n",
    "    os.makedirs(f\"{MODELNAME}/weight\")\n",
    "now = datetime.now()\n",
    "now = str(now)[5:17].replace(\" \", \"_\").replace(\":\", \"\")\n",
    "# writer = SummaryWriter(log_dir=f\"{MODELNAME}/tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "def split_data():\n",
    "    print(\"Split data\")\n",
    "    path_fold = \"../input/folds/train_folds.csv\"\n",
    "    if not exists(path_fold):\n",
    "        df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
    "        df.loc[:, \"kfold\"] = -1\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        targets = df.drop(\"sig_id\", axis=1).values\n",
    "\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "        for fold_, (tr_, va_) in enumerate(mskf.split(X=df, y=targets)):\n",
    "            df.loc[va_, \"kfold\"] = fold_\n",
    "        df.to_csv(path_fold, index=False)\n",
    "        print(f\"Created: {path_fold}\")\n",
    "    else:\n",
    "        print(\"Skipped: already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_pca(df, n_comp_c=15, n_comp_g=50, use_diff=False, drop=False):\n",
    "    \"\"\"How to Extract\n",
    "        with open(f'{MODELNAME}/weight/scaler.npy', 'rb') as f:\n",
    "            mean_c = np.load(f)\n",
    "            var_c = np.load(f)\n",
    "            mean_g = np.load(f)\n",
    "            var_g = np.load(f)\n",
    "    \"\"\"\n",
    "    c_feats = [col for col in df.columns if col.startswith('c-')]\n",
    "    g_feats = [col for col in df.columns if col.startswith('g-')]\n",
    "\n",
    "    c_pca = PCA(n_components=n_comp_c, random_state=42).fit_transform(df[c_feats])\n",
    "    g_pca = PCA(n_components=n_comp_g, random_state=42).fit_transform(df[g_feats])\n",
    "    \n",
    "    cp = pd.DataFrame(c_pca, columns=[f'c_pca{i}' for i in range(n_comp_c)])\n",
    "    gp = pd.DataFrame(g_pca, columns=[f'g_pca{i}' for i in range(n_comp_g)])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cp[cp.columns] = scaler.fit_transform(cp)\n",
    "    cp[cp.columns] = cp[cp.columns].astype(np.float32)\n",
    "    mean_c, var_c = scaler.mean_, scaler.var_\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    gp[gp.columns] = scaler.fit_transform(gp)\n",
    "    gp[gp.columns] = gp[gp.columns].astype(np.float32)\n",
    "    mean_g, var_g = scaler.mean_, scaler.var_\n",
    "\n",
    "    with open(f'{MODELNAME}/weight/scaler.npy', 'wb') as f:\n",
    "        np.save(f, mean_c)\n",
    "        np.save(f, var_c)\n",
    "        np.save(f, mean_g)\n",
    "        np.save(f, var_g)\n",
    "\n",
    "    if use_diff:\n",
    "        for i in range(1, cp.shape[1]):\n",
    "            cp[f'c_pca{i}'] = cp[f'c_pca{i}'] - cp[f'c_pca0']\n",
    "        for i in range(1, gp.shape[1]):\n",
    "            gp[f'g_pca{i}'] = gp[f'g_pca{i}'] - gp[f'g_pca0']\n",
    "    \n",
    "    df = pd.concat([df, cp, gp], axis=1)\n",
    "    if drop:\n",
    "        df.drop(c_feats, axis=1, inplace=True)\n",
    "        df.drop(g_feats, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cp_time, cp_doseが同じグループごとに、ターゲットと相関の高い特徴値を選択した列を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_selective_feats(df, targets):\n",
    "    \n",
    "    if exists(f'{MODELNAME}/selected_features.pkl'):\n",
    "        print('Found existing file. Concatenating...')\n",
    "        feats = pd.read_pickle(f'{MODELNAME}/selected_features.pkl')\n",
    "        return pd.concat([df, feats.iloc[:, 1:]], axis=1), feats.columns[1:].values\n",
    "    \n",
    "    c_feats = [col for col in df.columns if col.startswith('c-')]\n",
    "    g_feats = [col for col in df.columns if col.startswith('g-')]\n",
    "    df['nan'] = np.nan\n",
    "    print(f'Adding selective features...')\n",
    "    for t in df['cp_time'].unique():\n",
    "        for d in df['cp_dose'].unique():\n",
    "            print(f'(group {t}{d})', end='    ')\n",
    "\n",
    "            subset = df.query(f'cp_time == {t} and cp_dose == {d}').copy()\n",
    "\n",
    "            # [c features]\n",
    "            # Know which feature is the best in the subset\n",
    "            print('Creating corr matrix for c features...', end='')\n",
    "            c_corr = np.zeros((len(c_feats), len(targets)))\n",
    "            for fi, feat in enumerate(c_feats):\n",
    "                for ti, target_ in enumerate(targets):\n",
    "                    c_corr[fi,ti] = subset[target_].corr(subset[feat])\n",
    "            c_corr = pd.DataFrame(c_corr, index=c_feats, columns=targets)\n",
    "\n",
    "            # [g features]\n",
    "            # Know which feature is the best in the subset\n",
    "            print('done. Then matrix for g features...')\n",
    "            g_corr = np.zeros((len(g_feats), len(targets)))\n",
    "            for fi, feat in enumerate(g_feats):\n",
    "                for ti, target in enumerate(targets):\n",
    "                    g_corr[fi,ti] = subset[target].corr(subset[feat])\n",
    "            g_corr = pd.DataFrame(g_corr, index=g_feats, columns=targets)\n",
    "            print('done.')\n",
    "            \n",
    "            for i, target in enumerate(targets):\n",
    "                print(f'Target No.{i:>3} / {len(targets):>3}', end='\\r')\n",
    "\n",
    "                # Insert values of selected feature\n",
    "                selected_feats = c_corr.idxmax().astype(str)\n",
    "                df.loc[subset.index, f'cfeat_for_{target}'] = df.loc[subset.index, selected_feats[target]]\n",
    "\n",
    "                # Insert values of selected feature\n",
    "                selected_feats = g_corr.idxmax().astype(str)\n",
    "                df.loc[subset.index, f'gfeat_for_{target}'] = df.loc[subset.index, selected_feats[target]]\n",
    "            print('\\n')\n",
    "    \n",
    "    added_features = []\n",
    "    print('\\nCompressing created columns...')\n",
    "    for i, target in enumerate(targets):\n",
    "        print(f'Target No.{i:>3} / {len(targets):>3}', end='\\r')\n",
    "        df[f'cfeat_for_{target}'] = df[f'cfeat_for_{target}'].astype(np.float16)\n",
    "        df[f'gfeat_for_{target}'] = df[f'gfeat_for_{target}'].astype(np.float16)\n",
    "        added_features.append(f'cfeat_for_{target}')\n",
    "        added_features.append(f'gfeat_for_{target}')\n",
    "        \n",
    "    df = df.drop(['nan'], axis=1)\n",
    "    return df, added_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing file. Concatenating...\n"
     ]
    }
   ],
   "source": [
    "def preprocess():\n",
    "    df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
    "\n",
    "    # Label encoding\n",
    "    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n",
    "               \"cp_time\":{24:0, 48:1, 72:2},\n",
    "               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n",
    "    for col in ['cp_type', 'cp_time', 'cp_dose']:\n",
    "        df[col] = df[col].map(mapping[col])\n",
    "\n",
    "    folds = pd.read_csv(\"../input/folds/train_folds.csv\")\n",
    "\n",
    "    # Create aux target\n",
    "    # `nsc_labels` means # of labels found in non-scored train set\n",
    "    non_scored_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
    "    targets_non_scored = non_scored_df.drop(\"sig_id\", axis=1).to_numpy().sum(axis=1)\n",
    "    non_scored_df.loc[:, \"nsc_labels\"] = targets_non_scored\n",
    "    drop_cols = [c for c in non_scored_df.columns if c not in (\"nsc_labels\", \"sig_id\")]\n",
    "    non_scored_df = non_scored_df.drop(drop_cols, axis=1)\n",
    "    folds = folds.merge(non_scored_df, on=\"sig_id\", how=\"left\")\n",
    "\n",
    "    # Feature engineering\n",
    "    # df = add_pca(df)\n",
    "\n",
    "    targets = folds.drop([\"sig_id\", \"kfold\"], axis=1).columns\n",
    "    features = df.drop(\"sig_id\", axis=1).columns\n",
    "    df = df.merge(folds, on=\"sig_id\", how=\"left\")\n",
    "\n",
    "    # Feature engineering (target related)\n",
    "    df, cols = add_selective_feats(df, targets)   \n",
    "    features = np.append(features.values, cols)\n",
    "\n",
    "    return df, features, targets\n",
    "\n",
    "# split_data()\n",
    "df, features, targets = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, fold, params, hp_tune=False):\n",
    "    save_model = False if hp_tune else True\n",
    "    print(f'\\n[Fold No.{fold:>3}]')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    x_tr = train_df[features].to_numpy()\n",
    "    x_va = valid_df[features].to_numpy()\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    loss_tr, loss_va = [], []\n",
    "    for i, target in enumerate(targets):\n",
    "        print(f'Target No.{i:>3} / {len(targets):>3}',end='\\r')\n",
    "        y_tr = train_df[target].to_numpy()\n",
    "        y_va = valid_df[target].to_numpy()\n",
    "        dataset_tr = lgb.Dataset(x_tr, label=y_tr)\n",
    "        dataset_va = lgb.Dataset(x_va, label=y_va, reference=dataset_tr)\n",
    "\n",
    "        model = lgb.train(params, dataset_tr, EPOCHS, valid_sets=[dataset_tr, dataset_va],\n",
    "                          verbose_eval=False, early_stopping_rounds=100)\n",
    "\n",
    "        filename = f\"{MODELNAME}/weight/tgt{i}_{now}_fold{fold}.txt\"\n",
    "\n",
    "        preds_tr = model.predict(x_tr, num_iteration=model.best_iteration)\n",
    "        preds_va = model.predict(x_va, num_iteration=model.best_iteration)\n",
    "\n",
    "        loss_tr.append(log_loss(y_tr, preds_tr, labels=[0,1]).item())\n",
    "        loss_va.append(log_loss(y_va, preds_va, labels=[0,1]).item())\n",
    "\n",
    "        # if save_model:\n",
    "        #     model.save_model(filename, num_iteration=model.best_iteration)\n",
    "    \n",
    "    mean_loss_tr, mean_loss_va = np.mean(loss_tr), np.mean(loss_va)\n",
    "    print(f\"[fold{fold:>2}] {np.mean(mean_loss_tr):.5}, {np.mean(mean_loss_va):.5}\")\n",
    "    with open(f\"{MODELNAME}/scores/{now}.txt\", \"a\") as f:\n",
    "        f.write(f\"[fold{fold:>2}] {np.mean(mean_loss_tr):.5}, {np.mean(mean_loss_va):.5}\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"\\nmodel saved at:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"metric\": 'binary_logloss',\n",
    "          'objective': 'binary',\n",
    "          'num_leaves': 491,\n",
    "          'min_child_weight': 0.03,\n",
    "          'feature_fraction': 0.3,\n",
    "          'bagging_fraction': 0.4,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"verbose\": -1,\n",
    "          'reg_alpha': 0.4,\n",
    "          'reg_lambda': 0.6,\n",
    "          'random_state': 47,\n",
    "          \"force_col_wise\": True\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold No.  0]\n",
      "[fold 0] 0.0073371, 0.017967\n",
      "\n",
      "\n",
      "model saved at: LightGBM1019/weight/tgt206_10-29_0151_fold0.txt\n",
      "Wall time: 24min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for fold in range(0,4):\n",
    "run_training(df, 0, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_importance(model, features, now, filename):\n",
    "    import matplotlib.pyplot as plt\n",
    "    importance = pd.DataFrame(model.feature_importance(), index=features, columns=['importance'])\n",
    "    importance = importance.sort_values(by=['importance'], ascending=False)\n",
    "    plt.figure(figsize=(15,80))\n",
    "    plt.title(f'feature_importance of experiment {now}')\n",
    "    plt.tight_layout()\n",
    "    barlist = plt.barh(importance['importance'].index, importance['importance'].values, height=.9, edgecolor='white')\n",
    "    for v, bar in zip(importance['importance'].values, barlist):\n",
    "        v_ = np.clip(v / 256, 0, 1)\n",
    "        bar.set_color((v_, .5, (1-v_)))\n",
    "    plt.savefig(filename)\n",
    "# show_importance(model, features, now, f'{MODELNAME}/{now}_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELNAME}/{now}_params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"nn_params\": {\"dropout\": trial.suggest_uniform(\"dropout\", 0.1, 0.8), \n",
    "                      \"num_layers\": trial.suggest_int(\"num_layers\", 1, 7),\n",
    "                      \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 2048),\n",
    "                      \"activation\": trial.suggest_categorical(\"activation\", [\"relu\", \"prelu\"]),\n",
    "                      \"batchnorm\": trial.suggest_categorical(\"batchnorm\", [True, False])},\n",
    "        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\"]),\n",
    "        \"optim_params\": {\"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3)},\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"scdl_params\": {\"threshold\": 0.00001},\n",
    "    }\n",
    "    loss_all = []\n",
    "    for fold_ in range(4):\n",
    "        loss_tmp = run_training(df, fold, params, save_model=False)\n",
    "        loss_all.append(loss_tmp)\n",
    "    return np.mean(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
