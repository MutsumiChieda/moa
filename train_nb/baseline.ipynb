{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../script/')\n",
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "import utils\n",
    "import models\n",
    "import train as trainer\n",
    "DEVICE = \"cuda\"\n",
    "EPOCHS = 10\n",
    "MODELNAME = \"Baseline1013\"\n",
    "if not exists(MODELNAME):\n",
    "    os.makedirs(f\"{MODELNAME}/tensorboard\")\n",
    "save_model = True\n",
    "now = datetime.now()\n",
    "now = str(now)[5:17].replace(\" \", \"_\").replace(\":\", \"\")\n",
    "writer = SummaryWriter(log_dir=f\"{MODELNAME}/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/folds/train.csv\")\n",
    "with open(\"../input/folds/targets\", \"r\") as f:\n",
    "    targets = f.read().split(\"\\n\")\n",
    "with open(\"../input/folds/features\", \"r\") as f:\n",
    "    features = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, num_layers=3, dropout=.2, hidden_size=256, activation=\"relu\", batchnorm=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(num_features if len(layers)==0 else hidden_size, hidden_size, bias=(not batchnorm)))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            if activation == \"relu\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == \"prelu\":\n",
    "                layers.append(nn.PReLU())\n",
    "            else:\n",
    "                raise RuntimeError(f'{activation} is not implemented')\n",
    "        layers.append(nn.Linear(hidden_size, num_targets))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, fold, params):\n",
    "    print(f'\\n[Fold No.{fold:>3}]')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    x_tr = train_df[features].to_numpy()\n",
    "    x_va = valid_df[features].to_numpy()\n",
    "\n",
    "    y_tr = train_df[targets].to_numpy()\n",
    "    y_va = valid_df[targets].to_numpy()\n",
    "\n",
    "    dataset_tr = utils.MoaDataset(x_tr, y_tr)\n",
    "    loader_tr = torch.utils.data.DataLoader(dataset_tr, batch_size=params['batch_size'], num_workers=2)\n",
    "    dataset_va = utils.MoaDataset(x_va, y_va)\n",
    "    loader_va = torch.utils.data.DataLoader(dataset_va, batch_size=params['batch_size'], num_workers=2)\n",
    "\n",
    "    model = BaseLine(num_features=x_tr.shape[1], num_targets=y_tr.shape[1], **params['nn_params'])\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    if params[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), **params[\"optim_params\"])\n",
    "    elif params[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), **params[\"optim_params\"])\n",
    "    else:\n",
    "        raise RuntimeError(f'{params[\"optimizer\"]} is not implemented')\n",
    "\n",
    "    if params[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, mode=\"min\", **params[\"scdl_params\"])\n",
    "    else: \n",
    "        print(\"Not Implemented: No scheduling will be applied\")\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda ep: 1**ep)\n",
    "\n",
    "    eng = utils.Engine(model, optimizer, device=DEVICE)\n",
    "\n",
    "    del df, train_df, valid_df, x_tr, x_va, y_tr, y_va\n",
    "    gc.collect()\n",
    "\n",
    "    print(f'Training state is shown in {MODELNAME}/tensorboard')\n",
    "    filename = f\"{MODELNAME}/{now}_fold{fold}.pt\"\n",
    "    \n",
    "    loss_best = np.inf\n",
    "    patience = 10\n",
    "    patience_cnt = 0\n",
    "    for ep in range(EPOCHS):\n",
    "        print(f'Ep.{ep:>3}/{EPOCHS:>3}, patience:{patience_cnt:>2}/{patience:>2}', end='\\r')\n",
    "        loss_tr = eng.train(loader_tr)\n",
    "        loss_va = eng.validate(loader_va)\n",
    "        scheduler.step(loss_va)\n",
    "        writer.add_scalar(f'{now}/train', loss_tr, ep)\n",
    "        writer.add_scalar(f'{now}/valid', loss_va, ep)\n",
    "        if loss_va < loss_best:\n",
    "            patience_cnt = 0\n",
    "            loss_best = loss_va\n",
    "            if save_model:\n",
    "                torch.save(model.model.state_dict(), filename)\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "        if patience_cnt > patience:\n",
    "            break\n",
    "    print(\"\\nmodel saved at:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"nn_params\": {\"dropout\": 0.2, \"num_layers\": 3, \"hidden_size\": 256, \"activation\": \"relu\", \"batchnorm\": True},\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"optim_params\": {\"lr\": 1e-2, \"momentum\": 0.0},\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"scdl_params\": {\"threshold\": 0.00001},\n",
    "    \"batch_size\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold No.  0]\n",
      "Training state is shown in Baseline1013/tensorboard\n",
      "Ep.  9/ 10, patience: 0/10\n",
      "model saved at: Baseline1013/10-15_0214_fold0.pt\n",
      "\n",
      "[Fold No.  1]\n",
      "Training state is shown in Baseline1013/tensorboard\n",
      "Ep.  9/ 10, patience: 0/10\n",
      "model saved at: Baseline1013/10-15_0214_fold1.pt\n",
      "\n",
      "[Fold No.  2]\n",
      "Training state is shown in Baseline1013/tensorboard\n",
      "Ep.  9/ 10, patience: 0/10\n",
      "model saved at: Baseline1013/10-15_0214_fold2.pt\n",
      "\n",
      "[Fold No.  3]\n",
      "Training state is shown in Baseline1013/tensorboard\n",
      "Ep.  9/ 10, patience: 0/10\n",
      "model saved at: Baseline1013/10-15_0214_fold3.pt\n",
      "\n",
      "[Fold No.  4]\n",
      "Training state is shown in Baseline1013/tensorboard\n",
      "Ep.  9/ 10, patience: 0/10\n",
      "model saved at: Baseline1013/10-15_0214_fold4.pt\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    run_training(df, fold, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELNAME}/{now}_params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
