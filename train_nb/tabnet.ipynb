{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../script/')\n",
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "import utils\n",
    "import models\n",
    "import train as trainer\n",
    "DEVICE = \"cuda\"\n",
    "EPOCHS = 1000\n",
    "MODELNAME = \"TabNet1017\"\n",
    "if not exists(f\"{MODELNAME}/scores\"):\n",
    "    os.makedirs(f\"{MODELNAME}/scores\")\n",
    "if not exists(f\"{MODELNAME}/weight\"):\n",
    "    os.makedirs(f\"{MODELNAME}/weight\")\n",
    "now = datetime.now()\n",
    "now = str(now)[5:17].replace(\" \", \"_\").replace(\":\", \"\")\n",
    "# writer = SummaryWriter(log_dir=f\"{MODELNAME}/tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../script/tabnet-develop\")\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from pytorch_tabnet import tab_network\n",
    "from pytorch_tabnet.multiclass_utils import unique_labels\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pytorch_tabnet.utils import (PredictDataset,\n",
    "                                  create_dataloaders,\n",
    "                                  create_explain_matrix)\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import io\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "class TabModel(BaseEstimator):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n",
    "                 lambda_sparse=1e-3, seed=0,\n",
    "                 clip_value=1, verbose=1,\n",
    "                 optimizer_fn=torch.optim.Adam,\n",
    "                 optimizer_params=dict(lr=2e-2),\n",
    "                 scheduler_params=None, scheduler_fn=None,\n",
    "                 mask_type=\"sparsemax\",\n",
    "                 input_dim=None, output_dim=None,\n",
    "                 device_name='auto'):\n",
    "        \"\"\" Class for TabNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "            device_name: str\n",
    "                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.cat_idxs = cat_idxs\n",
    "        self.cat_dims = cat_dims\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.device_name = device_name\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.batch_size = 1024\n",
    "\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        print(f\"Device used : {self.device}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n",
    "                          weights, batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define construct_loaders to use this base class')\n",
    "\n",
    "    def init_network(\n",
    "                     self,\n",
    "                     input_dim,\n",
    "                     output_dim,\n",
    "                     n_d,\n",
    "                     n_a,\n",
    "                     n_steps,\n",
    "                     gamma,\n",
    "                     cat_idxs,\n",
    "                     cat_dims,\n",
    "                     cat_emb_dim,\n",
    "                     n_independent,\n",
    "                     n_shared,\n",
    "                     epsilon,\n",
    "                     virtual_batch_size,\n",
    "                     momentum,\n",
    "                     device_name,\n",
    "                     mask_type,\n",
    "                     ):\n",
    "        self.network = tab_network.TabNet(\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            device_name=device_name,\n",
    "            mask_type=mask_type).to(self.device)\n",
    "\n",
    "        self.reducing_matrix = create_explain_matrix(\n",
    "            self.network.input_dim,\n",
    "            self.network.cat_emb_dim,\n",
    "            self.network.cat_idxs,\n",
    "            self.network.post_embed_dim)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n",
    "            weights=0, max_epochs=100, patience=10, batch_size=1024,\n",
    "            virtual_batch_size=128, num_workers=0, drop_last=False):\n",
    "        \"\"\"Train a neural network stored in self.network\n",
    "        Using train_dataloader for training data and\n",
    "        valid_dataloader for validation.\n",
    "        Parameters\n",
    "        ----------\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            weights : bool or dictionnary\n",
    "                0 for no balancing\n",
    "                1 for automated balancing\n",
    "                dict for custom weights per class\n",
    "            max_epochs : int\n",
    "                Maximum number of epochs during training\n",
    "            patience : int\n",
    "                Number of consecutive non improving epoch before early stopping\n",
    "            batch_size : int\n",
    "                Training batch size\n",
    "            virtual_batch_size : int\n",
    "                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n",
    "            num_workers : int\n",
    "                Number of workers used in torch.utils.data.DataLoader\n",
    "            drop_last : bool\n",
    "                Whether to drop last batch during training\n",
    "        \"\"\"\n",
    "        # update model name\n",
    "\n",
    "        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                               weights, max_epochs, patience, batch_size,\n",
    "                               virtual_batch_size, num_workers, drop_last)\n",
    "\n",
    "        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n",
    "                                                                    y_train,\n",
    "                                                                    X_valid,\n",
    "                                                                    y_valid,\n",
    "                                                                    self.updated_weights,\n",
    "                                                                    self.batch_size,\n",
    "                                                                    self.num_workers,\n",
    "                                                                    self.drop_last)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "\n",
    "        self.optimizer = self.optimizer_fn(self.network.parameters(),\n",
    "                                           **self.optimizer_params)\n",
    "\n",
    "        if self.scheduler_fn:\n",
    "            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "        self.learning_rates = []\n",
    "        self.metrics_train = []\n",
    "        self.metrics_valid = []\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Will train until validation stopping metric\",\n",
    "                  f\"hasn't improved in {self.patience} rounds.\")\n",
    "            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n",
    "            print('---------------------------------------')\n",
    "            print(msg_epoch)\n",
    "\n",
    "        total_time = 0\n",
    "        while (self.epoch < self.max_epochs and\n",
    "               self.patience_counter < self.patience):\n",
    "            starting_time = time.time()\n",
    "            # updates learning rate history\n",
    "            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n",
    "\n",
    "            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n",
    "\n",
    "            # leaving it here, may be used for callbacks later\n",
    "            self.losses_train.append(fit_metrics['train']['loss_avg'])\n",
    "            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n",
    "            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n",
    "            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n",
    "\n",
    "            stopping_loss = fit_metrics['valid']['stopping_loss']\n",
    "            if stopping_loss < self.best_cost:\n",
    "                self.best_cost = stopping_loss\n",
    "                self.patience_counter = 0\n",
    "                # Saving model\n",
    "                self.best_network = deepcopy(self.network)\n",
    "                has_improved = True\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                has_improved=False\n",
    "            self.epoch += 1\n",
    "            total_time += time.time() - starting_time\n",
    "            if self.verbose > 0:\n",
    "                if self.epoch % self.verbose == 0:\n",
    "                    separator = \"|\"\n",
    "                    msg_epoch = f\"| {self.epoch:<5} | \"\n",
    "                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n",
    "                    msg_epoch += f\" {has_improved}\"\n",
    "                    print(msg_epoch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopping occured at epoch {self.epoch}\")\n",
    "            print(f\"Training done in {total_time:.3f} seconds.\")\n",
    "            print('---------------------------------------')\n",
    "\n",
    "        self.history = {\"train\": {\"loss\": self.losses_train,\n",
    "                                  \"metric\": self.metrics_train,\n",
    "                                  \"lr\": self.learning_rates},\n",
    "                        \"valid\": {\"loss\": self.losses_valid,\n",
    "                                  \"metric\": self.metrics_valid}}\n",
    "        # load best models post training\n",
    "        self.load_best_model()\n",
    "\n",
    "        # compute feature importance once the best model is defined\n",
    "        self._compute_feature_importances(train_dataloader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saving model with two distinct files.\n",
    "        \"\"\"\n",
    "        saved_params = {}\n",
    "        for key, val in self.get_params().items():\n",
    "            if isinstance(val, type):\n",
    "                # Don't save torch specific params\n",
    "                continue\n",
    "            else:\n",
    "                saved_params[key] = val\n",
    "\n",
    "        # Create folder\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save models params\n",
    "        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(saved_params, f)\n",
    "\n",
    "        # Save state_dict\n",
    "        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n",
    "        shutil.make_archive(path, 'zip', path)\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Successfully saved model at {path}.zip\")\n",
    "        return f\"{path}.zip\"\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(filepath) as z:\n",
    "                with z.open(\"model_params.json\") as f:\n",
    "                    loaded_params = json.load(f)\n",
    "                with z.open(\"network.pt\") as f:\n",
    "                    try:\n",
    "                        saved_state_dict = torch.load(f)\n",
    "                    except io.UnsupportedOperation:\n",
    "                        # In Python <3.7, the returned file object is not seekable (which at least\n",
    "                        # some versions of PyTorch require) - so we'll try buffering it in to a\n",
    "                        # BytesIO instead:\n",
    "                        saved_state_dict = torch.load(io.BytesIO(f.read()))\n",
    "        except KeyError:\n",
    "            raise KeyError(\"Your zip file is missing at least one component\")\n",
    "\n",
    "        self.__init__(**loaded_params)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=1024,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "        self.network.load_state_dict(saved_state_dict)\n",
    "        self.network.eval()\n",
    "        return\n",
    "\n",
    "    def fit_epoch(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates and updates network for one epoch.\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with valid set\n",
    "        \"\"\"\n",
    "        train_metrics = self.train_epoch(train_dataloader)\n",
    "        valid_metrics = self.predict_epoch(valid_dataloader)\n",
    "\n",
    "        fit_metrics = {'train': train_metrics,\n",
    "                       'valid': valid_metrics}\n",
    "\n",
    "        return fit_metrics\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_batch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_batch to use this base class')\n",
    "\n",
    "    def load_best_model(self):\n",
    "        if self.best_network is not None:\n",
    "            self.network = self.best_network\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem or the last class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict to use this base class')\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Return local explanation\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            M_explain: matrix\n",
    "                Importance per sample, per columns.\n",
    "            masks: matrix\n",
    "                Sparse matrix showing attention masks used by network.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            for key, value in masks.items():\n",
    "                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n",
    "                                            self.reducing_matrix)\n",
    "\n",
    "            if batch_nb == 0:\n",
    "                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                             self.reducing_matrix)\n",
    "                res_masks = masks\n",
    "            else:\n",
    "                res_explain = np.vstack([res_explain,\n",
    "                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                                        self.reducing_matrix)])\n",
    "                for key, value in masks.items():\n",
    "                    res_masks[key] = np.vstack([res_masks[key], value])\n",
    "        return res_explain, res_masks\n",
    "\n",
    "    def _compute_feature_importances(self, loader):\n",
    "        self.network.eval()\n",
    "        feature_importances_ = np.zeros((self.network.post_embed_dim))\n",
    "        for data, targets in loader:\n",
    "            data = data.to(self.device).float()\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n",
    "\n",
    "        feature_importances_ = csc_matrix.dot(feature_importances_,\n",
    "                                              self.reducing_matrix)\n",
    "        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n",
    "        \n",
    "        \n",
    "class TabNetRegressor(TabModel):\n",
    "\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n",
    "                          batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(weights, int):\n",
    "            if weights == 1:\n",
    "                raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "        if isinstance(weights, dict):\n",
    "            raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "\n",
    "        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n",
    "                                                                y_train,\n",
    "                                                                X_valid,\n",
    "                                                                y_valid,\n",
    "                                                                weights,\n",
    "                                                                batch_size,\n",
    "                                                                num_workers,\n",
    "                                                                drop_last)\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                          weights, max_epochs, patience,\n",
    "                          batch_size, virtual_batch_size, num_workers, drop_last):\n",
    "\n",
    "        if loss_fn is None:\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        if len(y_train.shape) == 1:\n",
    "            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n",
    "                                if doing single regression.\"\"\")\n",
    "        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n",
    "        self.output_dim = y_train.shape[1]\n",
    "\n",
    "        self.updated_weights = weights\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        # Initialize counters and histories.\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        self.best_cost = np.inf\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.train()\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            batch_outs = self.train_batch(data, targets)\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n",
    "        total_loss = total_loss / len(train_loader)\n",
    "        epoch_metrics = {'loss_avg': total_loss,\n",
    "                         'stopping_loss': total_loss,\n",
    "                         }\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step(total_loss)\n",
    "        return epoch_metrics\n",
    "\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        data = data.to(self.device).float()\n",
    "\n",
    "        targets = targets.to(self.device).float()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "\n",
    "        loss = self.loss_fn(output, targets)\n",
    "        \n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "\n",
    "        loss.backward()\n",
    "        if self.clip_value:\n",
    "            clip_grad_norm_(self.network.parameters(), self.clip_value)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        self.network.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in loader:\n",
    "            batch_outs = self.predict_batch(data, targets)\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "\n",
    "        total_loss = total_loss / len(loader)\n",
    "        epoch_metrics = {'total_loss': total_loss,\n",
    "                         'stopping_loss': stopping_loss}\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        data = data.to(self.device).float()\n",
    "        targets = targets.to(self.device).float()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "       \n",
    "        loss = self.loss_fn(output, targets)\n",
    "        #print(self.loss_fn, loss)\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "        #print(loss)\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        results = []\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            output, M_loss = self.network(data)\n",
    "            predictions = output.cpu().detach().numpy()\n",
    "            results.append(predictions)\n",
    "        res = np.vstack(results)\n",
    "        return res\n",
    "\n",
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "    \"\"\"\n",
    "    :param predicted:   The predicted probabilities as floats between 0-1\n",
    "    :param actual:      The binary labels. Either 0 or 1.\n",
    "    :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "    :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "    \"\"\"\n",
    "    p1 = actual * np.log(predicted+eps)\n",
    "    p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "    loss = p0 + p1\n",
    "\n",
    "    return -loss.mean()\n",
    "    \n",
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data\n",
      "Skipped: already exists\n"
     ]
    }
   ],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "def split_data():\n",
    "    print(\"Split data\")\n",
    "    path_fold = \"../input/folds/train_folds.csv\"\n",
    "    if not exists(path_fold):\n",
    "        df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
    "        df.loc[:, \"kfold\"] = -1\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        targets = df.drop(\"sig_id\", axis=1).values\n",
    "\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "        for fold_, (tr_, va_) in enumerate(mskf.split(X=df, y=targets)):\n",
    "            df.loc[va_, \"kfold\"] = fold_\n",
    "        df.to_csv(path_fold, index=False)\n",
    "        print(f\"Created: {path_fold}\")\n",
    "    else:\n",
    "        print(\"Skipped: already exists\")\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
    "    \n",
    "    # # Label encoding\n",
    "    # mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n",
    "    #            \"cp_time\":{24:0, 48:1, 72:2},\n",
    "    #            \"cp_dose\":{\"D1\":0, \"D2\":1}}\n",
    "    # for col in ['cp_type', 'cp_time', 'cp_dose']:\n",
    "    #     df[col] = df[col].map(mapping[col])\n",
    "    df = utils.process_data(df)\n",
    "    \n",
    "    folds = pd.read_csv(\"../input/folds/train_folds.csv\")\n",
    "\n",
    "    # Create aux target\n",
    "    # `nsc_labels` means # of labels found in non-scored train set\n",
    "    non_scored_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
    "    targets_non_scored = non_scored_df.drop(\"sig_id\", axis=1).to_numpy().sum(axis=1)\n",
    "    non_scored_df.loc[:, \"nsc_labels\"] = targets_non_scored\n",
    "    drop_cols = [c for c in non_scored_df.columns if c not in (\"nsc_labels\", \"sig_id\")]\n",
    "    non_scored_df = non_scored_df.drop(drop_cols, axis=1)\n",
    "    folds = folds.merge(non_scored_df, on=\"sig_id\", how=\"left\")\n",
    "\n",
    "    targets = folds.drop([\"sig_id\", \"kfold\"], axis=1).columns\n",
    "    features = df.drop(\"sig_id\", axis=1).columns\n",
    "    df = df.merge(folds, on=\"sig_id\", how=\"left\")\n",
    "    \n",
    "    return df, features, targets\n",
    "\n",
    "split_data()\n",
    "df, features, targets = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, fold, params, hp_tune=False):\n",
    "\n",
    "    ## Cardinaliry of Label Encoded features\n",
    "    # cols = ['cp_type', 'cp_time', 'cp_dose']\n",
    "    # cat_dims = [len(np.unique(df[col])) for col in cols]\n",
    "    # cat_idx = [df.columns.get_loc(col) for col in cols]\n",
    "\n",
    "    save_model = False if hp_tune else True\n",
    "    print(f'\\n[Fold No.{fold:>3}]')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    x_tr = train_df[features].to_numpy()\n",
    "    x_va = valid_df[features].to_numpy()\n",
    "\n",
    "    y_tr = train_df[targets].to_numpy()\n",
    "    y_va = valid_df[targets].to_numpy()\n",
    "\n",
    "    # for hp tuning, see https://www.kaggle.com/gogo827jz/moa-stacked-tabnet-baseline-tensorflow-2-0\n",
    "    # to get the clue.\n",
    "\n",
    "    model = TabNetRegressor(n_d=32, n_a=32, n_steps=3, gamma=1.3, \n",
    "                            optimizer_fn=torch.optim.AdamW,\n",
    "                            optimizer_params=dict(lr=1e-3, amsgrad=True), \n",
    "                            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                            scheduler_params={\"patience\":3, \"mode\":\"min\"},\n",
    "#                             cat_dims=cat_dims, cat_emb_dim=len(cat_dims), cat_idxs=cat_idx, \n",
    "                            mask_type=\"sparsemax\", device_name=DEVICE)\n",
    "\n",
    "    del df, train_df, valid_df\n",
    "    gc.collect()\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    filename = f\"{MODELNAME}/weight/{now}_fold{fold}\"\n",
    "\n",
    "    model.fit(X_train=x_tr, y_train=y_tr, X_valid=x_va, y_valid=y_va, \n",
    "              max_epochs=EPOCHS, patience=50, \n",
    "              batch_size=1024, virtual_batch_size=256,\n",
    "              num_workers=2, drop_last=False, \n",
    "              loss_fn=nn.functional.binary_cross_entropy_with_logits)\n",
    "\n",
    "    model.load_best_model()\n",
    "    preds_tr = torch.as_tensor(model.predict(x_tr)).detach().cpu()\n",
    "    preds_va = torch.as_tensor(model.predict(x_va)).detach().cpu()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = criterion(preds_tr, torch.as_tensor(y_tr).type_as(preds_tr))\n",
    "    loss_va = criterion(preds_va, torch.as_tensor(y_va).type_as(preds_va))\n",
    "    # writer.add_scalars(f'{now}/fold{fold}', {'train':loss_tr, 'valid':loss_va}, ep)\n",
    "    with open(f\"{MODELNAME}/scores/{now}.txt\", \"a\") as f:\n",
    "        f.write(f\"[fold{fold:>2}] {loss_tr.item():.5}, {loss_va.item():.5}\\n\")\n",
    "\n",
    "    if save_model:\n",
    "        model.save_model(filename)\n",
    "\n",
    "    print(\"\\nmodel saved at:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No variable params for now \n",
    "params = {\n",
    "#     \"nn_params\": {\"dropout\": 0.2, \"num_layers\": 3, \"hidden_size\": 512, \"activation\": \"prelu\", \"batchnorm\": True},\n",
    "#     \"optimizer\": \"SGD\",\n",
    "#     \"optim_params\": {\"lr\": 1e-2, \"momentum\": 0.3},\n",
    "#     \"scheduler\": \"ReduceLROnPlateau\",\n",
    "#     \"scdl_params\": {\"threshold\": 0.00001}\n",
    "}\n",
    "# 0.02355, 0.03 on momentum:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold No.  0]\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.87609 |   0.70131 |   4.1        True\n",
      "| 2     |  0.75697 |   0.67765 |   7.3        True\n",
      "| 3     |  0.66001 |   0.63323 |   10.4       True\n",
      "| 4     |  0.58061 |   0.57568 |   13.6       True\n",
      "| 5     |  0.50008 |   0.47733 |   16.7       True\n",
      "| 6     |  0.40880 |   0.38319 |   19.9       True\n",
      "| 7     |  0.32470 |   0.30326 |   23.1       True\n",
      "| 8     |  0.25261 |   0.22705 |   26.2       True\n",
      "| 9     |  0.18837 |   0.16746 |   29.4       True\n",
      "| 10    |  0.13233 |   0.11053 |   32.5       True\n",
      "| 11    |  0.08710 |   0.07306 |   35.7       True\n",
      "| 12    |  0.06040 |   0.05206 |   38.8       True\n",
      "| 13    |  0.04522 |   0.04030 |   42.0       True\n",
      "| 14    |  0.03832 |   0.03482 |   45.2       True\n",
      "| 15    |  0.03464 |   0.03164 |   48.3       True\n",
      "| 16    |  0.03269 |   0.02977 |   51.5       True\n",
      "| 17    |  0.03142 |   0.02870 |   54.7       True\n",
      "| 18    |  0.03057 |   0.02803 |   57.9       True\n",
      "| 19    |  0.02992 |   0.02749 |   61.1       True\n",
      "| 20    |  0.02953 |   0.02740 |   64.2       True\n",
      "| 21    |  0.02901 |   0.02698 |   67.3       True\n",
      "| 22    |  0.02876 |   0.02646 |   70.5       True\n",
      "| 23    |  0.02857 |   0.02636 |   73.6       True\n",
      "| 24    |  0.02830 |   0.02628 |   76.7       True\n",
      "| 25    |  0.02812 |   0.02586 |   79.9       True\n",
      "| 26    |  0.02787 |   0.02571 |   83.1       True\n",
      "| 27    |  0.02768 |   0.02562 |   86.3       True\n",
      "| 28    |  0.02756 |   0.02555 |   89.4       True\n",
      "| 29    |  0.02733 |   0.02536 |   92.6       True\n",
      "| 30    |  0.02722 |   0.02527 |   95.7       True\n",
      "| 31    |  0.02728 |   0.02514 |   98.9       True\n",
      "| 32    |  0.02699 |   0.02489 |   102.0      True\n",
      "| 33    |  0.02684 |   0.02480 |   105.2      True\n",
      "| 34    |  0.02679 |   0.02476 |   108.3      True\n",
      "| 35    |  0.02663 |   0.02468 |   111.5      True\n",
      "| 36    |  0.02659 |   0.02460 |   114.6      True\n",
      "| 37    |  0.02648 |   0.02450 |   117.7      True\n",
      "| 38    |  0.02654 |   0.02461 |   120.9      False\n",
      "| 39    |  0.02637 |   0.02444 |   124.0      True\n",
      "| 40    |  0.02639 |   0.02445 |   127.1      False\n",
      "| 41    |  0.02626 |   0.02439 |   130.3      True\n",
      "| 42    |  0.02618 |   0.02423 |   133.4      True\n",
      "| 43    |  0.02608 |   0.02403 |   136.5      True\n",
      "| 44    |  0.02599 |   0.02413 |   139.6      False\n",
      "| 45    |  0.02593 |   0.02394 |   142.8      True\n",
      "| 46    |  0.02586 |   0.02390 |   146.0      True\n",
      "| 47    |  0.02579 |   0.02388 |   149.1      True\n",
      "| 48    |  0.02577 |   0.02376 |   152.3      True\n",
      "| 49    |  0.02563 |   0.02375 |   155.5      True\n",
      "| 50    |  0.02561 |   0.02385 |   158.7      False\n",
      "| 51    |  0.02559 |   0.02372 |   161.9      True\n",
      "| 52    |  0.02551 |   0.02366 |   165.0      True\n",
      "| 53    |  0.02550 |   0.02357 |   168.2      True\n",
      "| 54    |  0.02544 |   0.02364 |   171.3      False\n",
      "| 55    |  0.02533 |   0.02370 |   174.5      False\n",
      "| 56    |  0.02543 |   0.02380 |   177.7      False\n",
      "| 57    |  0.02531 |   0.02366 |   180.8      False\n",
      "| 58    |  0.02525 |   0.02371 |   184.0      False\n",
      "| 59    |  0.02519 |   0.02349 |   187.1      True\n",
      "| 60    |  0.02516 |   0.02359 |   190.3      False\n",
      "| 61    |  0.02524 |   0.02362 |   193.5      False\n",
      "| 62    |  0.02523 |   0.02361 |   196.6      False\n",
      "| 63    |  0.02512 |   0.02353 |   199.8      False\n",
      "| 64    |  0.02509 |   0.02343 |   202.9      True\n",
      "| 65    |  0.02498 |   0.02338 |   206.2      True\n",
      "| 66    |  0.02502 |   0.02336 |   209.4      True\n",
      "| 67    |  0.02500 |   0.02334 |   212.6      True\n",
      "| 68    |  0.02495 |   0.02325 |   215.8      True\n",
      "| 69    |  0.02492 |   0.02328 |   219.0      False\n",
      "| 70    |  0.02490 |   0.02321 |   222.2      True\n",
      "| 71    |  0.02488 |   0.02330 |   225.3      False\n",
      "| 72    |  0.02492 |   0.02317 |   228.5      True\n",
      "| 73    |  0.02477 |   0.02318 |   231.6      False\n",
      "| 74    |  0.02480 |   0.02313 |   234.8      True\n",
      "| 75    |  0.02460 |   0.02300 |   238.0      True\n",
      "| 76    |  0.02468 |   0.02310 |   241.1      False\n",
      "| 77    |  0.02463 |   0.02309 |   244.2      False\n",
      "| 78    |  0.02462 |   0.02313 |   247.4      False\n",
      "| 79    |  0.02459 |   0.02299 |   250.6      True\n",
      "| 80    |  0.02447 |   0.02298 |   253.7      True\n",
      "| 81    |  0.02451 |   0.02302 |   256.9      False\n",
      "| 82    |  0.02441 |   0.02293 |   260.0      True\n",
      "| 83    |  0.02453 |   0.02309 |   263.2      False\n",
      "| 84    |  0.02444 |   0.02291 |   266.3      True\n",
      "| 85    |  0.02435 |   0.02297 |   269.4      False\n",
      "| 86    |  0.02435 |   0.02295 |   272.6      False\n",
      "| 87    |  0.02432 |   0.02288 |   275.7      True\n",
      "| 88    |  0.02429 |   0.02284 |   278.9      True\n",
      "| 89    |  0.02433 |   0.02279 |   282.0      True\n",
      "| 90    |  0.02428 |   0.02279 |   285.2      True\n",
      "| 91    |  0.02428 |   0.02277 |   288.4      True\n",
      "| 92    |  0.02418 |   0.02284 |   291.5      False\n",
      "| 93    |  0.02420 |   0.02271 |   294.7      True\n",
      "| 94    |  0.02406 |   0.02269 |   297.9      True\n",
      "| 95    |  0.02410 |   0.02266 |   301.0      True\n",
      "| 96    |  0.02406 |   0.02268 |   304.2      False\n",
      "| 97    |  0.02404 |   0.02263 |   307.3      True\n",
      "| 98    |  0.02402 |   0.02255 |   310.5      True\n",
      "| 99    |  0.02395 |   0.02264 |   313.7      False\n",
      "| 100   |  0.02394 |   0.02261 |   316.9      False\n",
      "| 101   |  0.02392 |   0.02259 |   320.0      False\n",
      "| 102   |  0.02385 |   0.02245 |   323.1      True\n",
      "| 103   |  0.02393 |   0.02245 |   326.3      False\n",
      "| 104   |  0.02389 |   0.02253 |   329.5      False\n",
      "| 105   |  0.02389 |   0.02248 |   332.6      False\n",
      "| 106   |  0.02380 |   0.02242 |   335.8      True\n",
      "| 107   |  0.02383 |   0.02244 |   338.9      False\n",
      "| 108   |  0.02377 |   0.02247 |   342.1      False\n",
      "| 109   |  0.02380 |   0.02233 |   345.3      True\n",
      "| 110   |  0.02374 |   0.02248 |   348.4      False\n",
      "| 111   |  0.02371 |   0.02236 |   351.6      False\n",
      "| 112   |  0.02371 |   0.02231 |   354.8      True\n",
      "| 113   |  0.02368 |   0.02230 |   357.9      True\n",
      "| 114   |  0.02369 |   0.02231 |   361.1      False\n",
      "| 115   |  0.02364 |   0.02228 |   364.3      True\n",
      "| 116   |  0.02368 |   0.02227 |   367.4      True\n",
      "| 117   |  0.02355 |   0.02228 |   370.6      False\n",
      "| 118   |  0.02357 |   0.02219 |   373.8      True\n",
      "| 119   |  0.02353 |   0.02217 |   376.9      True\n",
      "| 120   |  0.02354 |   0.02222 |   380.1      False\n",
      "| 121   |  0.02350 |   0.02214 |   383.2      True\n",
      "| 122   |  0.02351 |   0.02217 |   386.4      False\n",
      "| 123   |  0.02347 |   0.02215 |   389.5      False\n",
      "| 124   |  0.02345 |   0.02214 |   392.6      False\n",
      "| 125   |  0.02336 |   0.02211 |   395.8      True\n",
      "| 126   |  0.02338 |   0.02210 |   399.0      True\n",
      "| 127   |  0.02335 |   0.02210 |   402.1      True\n",
      "| 128   |  0.02324 |   0.02205 |   405.3      True\n",
      "| 129   |  0.02334 |   0.02211 |   408.5      False\n",
      "| 130   |  0.02329 |   0.02201 |   411.6      True\n",
      "| 131   |  0.02329 |   0.02196 |   414.7      True\n",
      "| 132   |  0.02329 |   0.02196 |   417.9      True\n",
      "| 133   |  0.02327 |   0.02200 |   421.1      False\n",
      "| 134   |  0.02325 |   0.02192 |   424.2      True\n",
      "| 135   |  0.02330 |   0.02198 |   427.3      False\n",
      "| 136   |  0.02326 |   0.02198 |   430.5      False\n",
      "| 137   |  0.02319 |   0.02195 |   433.7      False\n",
      "| 138   |  0.02326 |   0.02197 |   436.9      False\n",
      "| 139   |  0.02326 |   0.02198 |   440.0      False\n",
      "| 140   |  0.02326 |   0.02199 |   443.2      False\n",
      "| 141   |  0.02325 |   0.02192 |   446.4      True\n",
      "| 142   |  0.02326 |   0.02195 |   449.6      False\n",
      "| 143   |  0.02329 |   0.02200 |   452.7      False\n",
      "| 144   |  0.02325 |   0.02198 |   455.9      False\n",
      "| 145   |  0.02327 |   0.02202 |   459.1      False\n",
      "| 146   |  0.02328 |   0.02197 |   462.2      False\n",
      "| 147   |  0.02330 |   0.02196 |   465.4      False\n",
      "| 148   |  0.02327 |   0.02195 |   468.6      False\n",
      "| 149   |  0.02331 |   0.02197 |   471.8      False\n",
      "| 150   |  0.02320 |   0.02195 |   474.9      False\n",
      "| 151   |  0.02327 |   0.02197 |   478.1      False\n",
      "| 152   |  0.02324 |   0.02199 |   481.2      False\n",
      "| 153   |  0.02328 |   0.02199 |   484.4      False\n",
      "| 154   |  0.02325 |   0.02197 |   487.5      False\n",
      "| 155   |  0.02325 |   0.02195 |   490.7      False\n",
      "| 156   |  0.02328 |   0.02194 |   493.9      False\n",
      "| 157   |  0.02328 |   0.02195 |   497.1      False\n",
      "| 158   |  0.02322 |   0.02197 |   500.3      False\n",
      "| 159   |  0.02326 |   0.02198 |   503.4      False\n",
      "| 160   |  0.02323 |   0.02194 |   506.6      False\n",
      "| 161   |  0.02330 |   0.02194 |   509.7      False\n",
      "| 162   |  0.02324 |   0.02197 |   513.0      False\n",
      "| 163   |  0.02326 |   0.02194 |   516.1      False\n",
      "| 164   |  0.02324 |   0.02198 |   519.2      False\n",
      "| 165   |  0.02324 |   0.02197 |   522.4      False\n",
      "| 166   |  0.02328 |   0.02196 |   525.6      False\n",
      "| 167   |  0.02328 |   0.02193 |   528.7      False\n",
      "| 168   |  0.02323 |   0.02200 |   531.9      False\n",
      "| 169   |  0.02327 |   0.02198 |   535.2      False\n",
      "| 170   |  0.02322 |   0.02193 |   538.3      False\n",
      "| 171   |  0.02330 |   0.02195 |   541.4      False\n",
      "| 172   |  0.02326 |   0.02197 |   544.6      False\n",
      "| 173   |  0.02324 |   0.02197 |   547.7      False\n",
      "| 174   |  0.02331 |   0.02196 |   550.9      False\n",
      "| 175   |  0.02328 |   0.02198 |   554.0      False\n",
      "| 176   |  0.02328 |   0.02197 |   557.2      False\n",
      "| 177   |  0.02325 |   0.02200 |   560.3      False\n",
      "| 178   |  0.02327 |   0.02195 |   563.5      False\n",
      "| 179   |  0.02327 |   0.02197 |   566.6      False\n",
      "| 180   |  0.02320 |   0.02197 |   569.8      False\n",
      "| 181   |  0.02318 |   0.02194 |   573.0      False\n",
      "| 182   |  0.02328 |   0.02198 |   576.1      False\n",
      "| 183   |  0.02329 |   0.02191 |   579.3      True\n",
      "| 184   |  0.02330 |   0.02195 |   582.6      False\n",
      "| 185   |  0.02315 |   0.02197 |   585.7      False\n",
      "| 186   |  0.02324 |   0.02197 |   588.8      False\n",
      "| 187   |  0.02325 |   0.02197 |   591.9      False\n",
      "| 188   |  0.02328 |   0.02200 |   595.1      False\n",
      "| 189   |  0.02323 |   0.02194 |   598.2      False\n",
      "| 190   |  0.02325 |   0.02197 |   601.3      False\n",
      "| 191   |  0.02326 |   0.02197 |   604.4      False\n",
      "| 192   |  0.02318 |   0.02198 |   607.6      False\n",
      "| 193   |  0.02318 |   0.02194 |   610.7      False\n",
      "| 194   |  0.02324 |   0.02197 |   613.8      False\n",
      "| 195   |  0.02329 |   0.02201 |   617.0      False\n",
      "| 196   |  0.02331 |   0.02198 |   620.2      False\n",
      "| 197   |  0.02322 |   0.02195 |   623.3      False\n",
      "| 198   |  0.02324 |   0.02195 |   626.4      False\n",
      "| 199   |  0.02330 |   0.02196 |   629.6      False\n",
      "| 200   |  0.02324 |   0.02197 |   632.8      False\n",
      "| 201   |  0.02321 |   0.02197 |   635.9      False\n",
      "| 202   |  0.02325 |   0.02193 |   639.1      False\n",
      "| 203   |  0.02327 |   0.02196 |   642.3      False\n",
      "| 204   |  0.02321 |   0.02196 |   645.4      False\n",
      "| 205   |  0.02332 |   0.02196 |   648.5      False\n",
      "| 206   |  0.02328 |   0.02197 |   651.6      False\n",
      "| 207   |  0.02321 |   0.02195 |   654.8      False\n",
      "| 208   |  0.02325 |   0.02196 |   657.9      False\n",
      "| 209   |  0.02325 |   0.02197 |   661.0      False\n",
      "| 210   |  0.02327 |   0.02193 |   664.2      False\n",
      "| 211   |  0.02330 |   0.02200 |   667.3      False\n",
      "| 212   |  0.02326 |   0.02197 |   670.5      False\n",
      "| 213   |  0.02326 |   0.02195 |   673.7      False\n",
      "| 214   |  0.02329 |   0.02200 |   676.8      False\n",
      "| 215   |  0.02322 |   0.02199 |   680.0      False\n",
      "| 216   |  0.02333 |   0.02194 |   683.2      False\n",
      "| 217   |  0.02329 |   0.02193 |   686.4      False\n",
      "| 218   |  0.02331 |   0.02197 |   689.6      False\n",
      "| 219   |  0.02326 |   0.02199 |   692.7      False\n",
      "| 220   |  0.02322 |   0.02198 |   695.9      False\n",
      "| 221   |  0.02326 |   0.02196 |   699.0      False\n",
      "| 222   |  0.02323 |   0.02197 |   702.1      False\n",
      "| 223   |  0.02324 |   0.02197 |   705.3      False\n",
      "| 224   |  0.02333 |   0.02200 |   708.4      False\n",
      "| 225   |  0.02331 |   0.02195 |   711.6      False\n",
      "| 226   |  0.02329 |   0.02200 |   714.7      False\n",
      "| 227   |  0.02322 |   0.02197 |   717.9      False\n",
      "| 228   |  0.02329 |   0.02195 |   721.2      False\n",
      "| 229   |  0.02324 |   0.02196 |   724.3      False\n",
      "| 230   |  0.02320 |   0.02198 |   727.5      False\n",
      "| 231   |  0.02330 |   0.02195 |   730.6      False\n",
      "| 232   |  0.02325 |   0.02196 |   733.7      False\n",
      "| 233   |  0.02321 |   0.02193 |   736.9      False\n",
      "Early stopping occured at epoch 233\n",
      "Training done in 736.868 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at TabNet1017/weight/10-17_1225_fold0.zip\n",
      "\n",
      "model saved at: TabNet1017/weight/10-17_1225_fold0\n",
      "\n",
      "[Fold No.  1]\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.87673 |   0.70089 |   3.2        True\n",
      "| 2     |  0.75612 |   0.67463 |   6.3        True\n",
      "| 3     |  0.65259 |   0.62458 |   9.6        True\n",
      "| 4     |  0.56665 |   0.54464 |   12.8       True\n",
      "| 5     |  0.48641 |   0.46606 |   16.0       True\n",
      "| 6     |  0.40244 |   0.37420 |   19.1       True\n",
      "| 7     |  0.30902 |   0.28250 |   22.3       True\n",
      "| 8     |  0.23115 |   0.20563 |   25.5       True\n",
      "| 9     |  0.16632 |   0.14478 |   28.7       True\n",
      "| 10    |  0.11640 |   0.09953 |   31.9       True\n",
      "| 11    |  0.08032 |   0.06734 |   35.1       True\n",
      "| 12    |  0.05703 |   0.04922 |   38.2       True\n",
      "| 13    |  0.04483 |   0.03978 |   41.4       True\n",
      "| 14    |  0.03830 |   0.03438 |   44.6       True\n",
      "| 15    |  0.03456 |   0.03173 |   47.8       True\n",
      "| 16    |  0.03264 |   0.02991 |   51.0       True\n",
      "| 17    |  0.03135 |   0.02892 |   54.2       True\n",
      "| 18    |  0.03052 |   0.02803 |   57.4       True\n",
      "| 19    |  0.02975 |   0.02752 |   60.5       True\n",
      "| 20    |  0.02937 |   0.02717 |   63.7       True\n",
      "| 21    |  0.02897 |   0.02674 |   66.9       True\n",
      "| 22    |  0.02862 |   0.02637 |   70.1       True\n",
      "| 23    |  0.02836 |   0.02596 |   73.2       True\n",
      "| 24    |  0.02814 |   0.02606 |   76.4       False\n",
      "| 25    |  0.02804 |   0.02573 |   79.6       True\n",
      "| 26    |  0.02787 |   0.02562 |   82.7       True\n",
      "| 27    |  0.02773 |   0.02551 |   85.9       True\n",
      "| 28    |  0.02749 |   0.02531 |   89.0       True\n",
      "| 29    |  0.02743 |   0.02541 |   92.2       False\n",
      "| 30    |  0.02722 |   0.02541 |   95.4       False\n",
      "| 31    |  0.02709 |   0.02507 |   98.6       True\n",
      "| 32    |  0.02695 |   0.02498 |   101.7      True\n",
      "| 33    |  0.02685 |   0.02487 |   104.9      True\n",
      "| 34    |  0.02665 |   0.02481 |   108.0      True\n",
      "| 35    |  0.02658 |   0.02476 |   111.2      True\n",
      "| 36    |  0.02647 |   0.02466 |   114.3      True\n",
      "| 37    |  0.02635 |   0.02460 |   117.5      True\n",
      "| 38    |  0.02636 |   0.02447 |   120.7      True\n",
      "| 39    |  0.02633 |   0.02437 |   123.9      True\n",
      "| 40    |  0.02623 |   0.02418 |   127.0      True\n",
      "| 41    |  0.02612 |   0.02445 |   130.1      False\n",
      "| 42    |  0.02614 |   0.02417 |   133.3      True\n",
      "| 43    |  0.02601 |   0.02413 |   136.5      True\n",
      "| 44    |  0.02604 |   0.02407 |   139.7      True\n",
      "| 45    |  0.02590 |   0.02405 |   142.8      True\n",
      "| 46    |  0.02590 |   0.02396 |   146.0      True\n",
      "| 47    |  0.02584 |   0.02400 |   149.1      False\n",
      "| 48    |  0.02564 |   0.02391 |   152.3      True\n",
      "| 49    |  0.02564 |   0.02390 |   155.4      True\n",
      "| 50    |  0.02558 |   0.02372 |   158.6      True\n",
      "| 51    |  0.02559 |   0.02387 |   161.8      False\n",
      "| 52    |  0.02552 |   0.02374 |   164.9      False\n",
      "| 53    |  0.02558 |   0.02369 |   168.1      True\n",
      "| 54    |  0.02538 |   0.02358 |   171.3      True\n",
      "| 55    |  0.02534 |   0.02358 |   174.4      False\n",
      "| 56    |  0.02528 |   0.02367 |   177.5      False\n",
      "| 57    |  0.02535 |   0.02355 |   180.7      True\n",
      "| 58    |  0.02523 |   0.02352 |   183.9      True\n",
      "| 59    |  0.02510 |   0.02345 |   187.0      True\n",
      "| 60    |  0.02505 |   0.02355 |   190.2      False\n",
      "| 61    |  0.02510 |   0.02349 |   193.4      False\n",
      "| 62    |  0.02498 |   0.02344 |   196.6      True\n",
      "| 63    |  0.02500 |   0.02337 |   199.7      True\n",
      "| 64    |  0.02496 |   0.02331 |   202.9      True\n",
      "| 65    |  0.02497 |   0.02328 |   206.1      True\n",
      "| 66    |  0.02487 |   0.02326 |   209.2      True\n",
      "| 67    |  0.02474 |   0.02316 |   212.4      True\n",
      "| 68    |  0.02486 |   0.02313 |   215.6      True\n",
      "| 69    |  0.02475 |   0.02310 |   218.8      True\n",
      "| 70    |  0.02475 |   0.02329 |   221.9      False\n",
      "| 71    |  0.02468 |   0.02309 |   225.1      True\n",
      "| 72    |  0.02465 |   0.02306 |   228.3      True\n",
      "| 73    |  0.02472 |   0.02308 |   231.4      False\n",
      "| 74    |  0.02466 |   0.02304 |   234.6      True\n",
      "| 75    |  0.02465 |   0.02301 |   237.8      True\n",
      "| 76    |  0.02465 |   0.02294 |   240.9      True\n",
      "| 77    |  0.02463 |   0.02294 |   244.1      False\n",
      "| 78    |  0.02463 |   0.02290 |   247.3      True\n",
      "| 79    |  0.02460 |   0.02303 |   250.4      False\n",
      "| 80    |  0.02454 |   0.02290 |   253.6      True\n",
      "| 81    |  0.02461 |   0.02294 |   256.7      False\n",
      "| 82    |  0.02450 |   0.02292 |   259.9      False\n",
      "| 83    |  0.02452 |   0.02300 |   263.0      False\n",
      "| 84    |  0.02449 |   0.02297 |   266.2      False\n",
      "| 85    |  0.02447 |   0.02289 |   269.4      True\n",
      "| 86    |  0.02440 |   0.02286 |   272.5      True\n",
      "| 87    |  0.02438 |   0.02283 |   275.6      True\n",
      "| 88    |  0.02438 |   0.02288 |   278.8      False\n",
      "| 89    |  0.02435 |   0.02287 |   282.0      False\n",
      "| 90    |  0.02439 |   0.02281 |   285.2      True\n",
      "| 91    |  0.02428 |   0.02280 |   288.4      True\n",
      "| 92    |  0.02438 |   0.02278 |   291.6      True\n",
      "| 93    |  0.02432 |   0.02287 |   294.7      False\n",
      "| 94    |  0.02426 |   0.02288 |   297.9      False\n",
      "| 95    |  0.02420 |   0.02296 |   301.0      False\n",
      "| 96    |  0.02415 |   0.02289 |   304.2      False\n",
      "| 97    |  0.02419 |   0.02281 |   307.4      False\n",
      "| 98    |  0.02424 |   0.02281 |   310.5      False\n",
      "| 99    |  0.02420 |   0.02283 |   313.7      False\n",
      "| 100   |  0.02421 |   0.02282 |   316.9      False\n",
      "| 101   |  0.02418 |   0.02284 |   320.1      False\n",
      "| 102   |  0.02413 |   0.02283 |   323.2      False\n",
      "| 103   |  0.02407 |   0.02281 |   326.4      False\n",
      "| 104   |  0.02414 |   0.02284 |   329.6      False\n",
      "| 105   |  0.02417 |   0.02281 |   332.7      False\n",
      "| 106   |  0.02411 |   0.02280 |   335.8      False\n",
      "| 107   |  0.02407 |   0.02287 |   339.0      False\n",
      "| 108   |  0.02419 |   0.02283 |   342.2      False\n",
      "| 109   |  0.02413 |   0.02286 |   345.3      False\n",
      "| 110   |  0.02410 |   0.02278 |   348.5      True\n",
      "| 111   |  0.02403 |   0.02283 |   351.7      False\n",
      "| 112   |  0.02416 |   0.02282 |   354.8      False\n",
      "| 113   |  0.02414 |   0.02288 |   358.0      False\n",
      "| 114   |  0.02420 |   0.02289 |   361.1      False\n",
      "| 115   |  0.02412 |   0.02289 |   364.3      False\n",
      "| 116   |  0.02413 |   0.02280 |   367.4      False\n",
      "| 117   |  0.02414 |   0.02282 |   370.5      False\n",
      "| 118   |  0.02409 |   0.02284 |   373.7      False\n",
      "| 119   |  0.02410 |   0.02280 |   376.8      False\n",
      "| 120   |  0.02418 |   0.02286 |   380.0      False\n",
      "| 121   |  0.02414 |   0.02286 |   383.1      False\n",
      "| 122   |  0.02407 |   0.02284 |   386.2      False\n",
      "| 123   |  0.02414 |   0.02283 |   389.4      False\n",
      "| 124   |  0.02407 |   0.02283 |   392.5      False\n",
      "| 125   |  0.02414 |   0.02285 |   395.6      False\n",
      "| 126   |  0.02403 |   0.02285 |   398.8      False\n",
      "| 127   |  0.02418 |   0.02285 |   401.9      False\n",
      "| 128   |  0.02413 |   0.02283 |   405.1      False\n",
      "| 129   |  0.02410 |   0.02290 |   408.2      False\n",
      "| 130   |  0.02409 |   0.02286 |   411.4      False\n",
      "| 131   |  0.02419 |   0.02287 |   414.5      False\n",
      "| 132   |  0.02415 |   0.02282 |   417.7      False\n",
      "| 133   |  0.02408 |   0.02284 |   420.8      False\n",
      "| 134   |  0.02408 |   0.02280 |   424.0      False\n",
      "| 135   |  0.02407 |   0.02279 |   427.1      False\n",
      "| 136   |  0.02407 |   0.02282 |   430.3      False\n",
      "| 137   |  0.02412 |   0.02280 |   433.4      False\n",
      "| 138   |  0.02410 |   0.02278 |   436.5      False\n",
      "| 139   |  0.02412 |   0.02283 |   439.7      False\n",
      "| 140   |  0.02414 |   0.02280 |   442.9      False\n",
      "| 141   |  0.02414 |   0.02285 |   446.0      False\n",
      "| 142   |  0.02403 |   0.02283 |   449.2      False\n",
      "| 143   |  0.02415 |   0.02279 |   452.4      False\n",
      "| 144   |  0.02412 |   0.02285 |   455.6      False\n",
      "| 145   |  0.02421 |   0.02283 |   458.7      False\n",
      "| 146   |  0.02408 |   0.02286 |   461.9      False\n",
      "| 147   |  0.02422 |   0.02285 |   465.0      False\n",
      "| 148   |  0.02415 |   0.02284 |   468.2      False\n",
      "| 149   |  0.02409 |   0.02279 |   471.4      False\n",
      "| 150   |  0.02413 |   0.02281 |   474.6      False\n",
      "| 151   |  0.02413 |   0.02281 |   477.8      False\n",
      "| 152   |  0.02409 |   0.02281 |   480.9      False\n",
      "| 153   |  0.02409 |   0.02283 |   484.1      False\n",
      "| 154   |  0.02417 |   0.02282 |   487.3      False\n",
      "| 155   |  0.02411 |   0.02282 |   490.4      False\n",
      "| 156   |  0.02415 |   0.02285 |   493.6      False\n",
      "| 157   |  0.02414 |   0.02287 |   496.8      False\n",
      "| 158   |  0.02415 |   0.02281 |   499.9      False\n",
      "| 159   |  0.02406 |   0.02284 |   503.1      False\n",
      "| 160   |  0.02413 |   0.02285 |   506.3      False\n",
      "Early stopping occured at epoch 160\n",
      "Training done in 506.258 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at TabNet1017/weight/10-17_1225_fold1.zip\n",
      "\n",
      "model saved at: TabNet1017/weight/10-17_1225_fold1\n",
      "\n",
      "[Fold No.  2]\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.87443 |   0.70019 |   3.2        True\n",
      "| 2     |  0.76082 |   0.67648 |   6.4        True\n",
      "| 3     |  0.65586 |   0.61426 |   9.5        True\n",
      "| 4     |  0.55888 |   0.53846 |   12.7       True\n",
      "| 5     |  0.46801 |   0.44306 |   15.8       True\n",
      "| 6     |  0.37787 |   0.35251 |   19.0       True\n",
      "| 7     |  0.29224 |   0.26632 |   22.2       True\n",
      "| 8     |  0.21724 |   0.19098 |   25.4       True\n",
      "| 9     |  0.15539 |   0.13532 |   28.5       True\n",
      "| 10    |  0.10876 |   0.09417 |   31.7       True\n",
      "| 11    |  0.07679 |   0.06602 |   34.9       True\n",
      "| 12    |  0.05575 |   0.04846 |   38.1       True\n",
      "| 13    |  0.04326 |   0.03870 |   41.2       True\n",
      "| 14    |  0.03686 |   0.03337 |   44.4       True\n",
      "| 15    |  0.03389 |   0.03097 |   47.5       True\n",
      "| 16    |  0.03210 |   0.02950 |   50.7       True\n",
      "| 17    |  0.03105 |   0.02840 |   53.8       True\n",
      "| 18    |  0.03010 |   0.02773 |   57.0       True\n",
      "| 19    |  0.02948 |   0.02733 |   60.1       True\n",
      "| 20    |  0.02903 |   0.02676 |   63.3       True\n",
      "| 21    |  0.02865 |   0.02650 |   66.5       True\n",
      "| 22    |  0.02823 |   0.02620 |   69.6       True\n",
      "| 23    |  0.02809 |   0.02610 |   72.8       True\n",
      "| 24    |  0.02783 |   0.02563 |   75.9       True\n",
      "| 25    |  0.02757 |   0.02534 |   79.1       True\n",
      "| 26    |  0.02750 |   0.02542 |   82.3       False\n",
      "| 27    |  0.02736 |   0.02529 |   85.4       True\n",
      "| 28    |  0.02716 |   0.02519 |   88.6       True\n",
      "| 29    |  0.02706 |   0.02519 |   91.7       True\n",
      "| 30    |  0.02694 |   0.02509 |   94.9       True\n",
      "| 31    |  0.02699 |   0.02494 |   98.1       True\n",
      "| 32    |  0.02681 |   0.02486 |   101.3      True\n",
      "| 33    |  0.02676 |   0.02481 |   104.5      True\n",
      "| 34    |  0.02653 |   0.02457 |   107.6      True\n",
      "| 35    |  0.02646 |   0.02454 |   110.8      True\n",
      "| 36    |  0.02642 |   0.02453 |   114.0      True\n",
      "| 37    |  0.02636 |   0.02455 |   117.2      False\n",
      "| 38    |  0.02630 |   0.02439 |   120.4      True\n",
      "| 39    |  0.02620 |   0.02432 |   123.6      True\n",
      "| 40    |  0.02616 |   0.02422 |   126.8      True\n",
      "| 41    |  0.02606 |   0.02433 |   130.0      False\n",
      "| 42    |  0.02599 |   0.02421 |   133.1      True\n",
      "| 43    |  0.02602 |   0.02443 |   136.3      False\n",
      "| 44    |  0.02585 |   0.02415 |   139.5      True\n",
      "| 45    |  0.02568 |   0.02392 |   142.6      True\n",
      "| 46    |  0.02566 |   0.02401 |   145.8      False\n",
      "| 47    |  0.02569 |   0.02394 |   149.0      False\n",
      "| 48    |  0.02550 |   0.02384 |   152.1      True\n",
      "| 49    |  0.02542 |   0.02371 |   155.3      True\n",
      "| 50    |  0.02535 |   0.02364 |   158.5      True\n",
      "| 51    |  0.02530 |   0.02359 |   161.6      True\n",
      "| 52    |  0.02525 |   0.02361 |   164.8      False\n",
      "| 53    |  0.02524 |   0.02352 |   168.0      True\n",
      "| 54    |  0.02520 |   0.02347 |   171.1      True\n",
      "| 55    |  0.02512 |   0.02348 |   174.4      False\n",
      "| 56    |  0.02504 |   0.02347 |   177.6      True\n",
      "| 57    |  0.02501 |   0.02351 |   180.7      False\n",
      "| 58    |  0.02506 |   0.02352 |   183.8      False\n",
      "| 59    |  0.02497 |   0.02343 |   186.9      True\n",
      "| 60    |  0.02491 |   0.02339 |   190.1      True\n",
      "| 61    |  0.02483 |   0.02332 |   193.3      True\n",
      "| 62    |  0.02487 |   0.02326 |   196.4      True\n",
      "| 63    |  0.02477 |   0.02328 |   199.6      False\n",
      "| 64    |  0.02470 |   0.02334 |   202.8      False\n",
      "| 65    |  0.02469 |   0.02322 |   205.9      True\n",
      "| 66    |  0.02464 |   0.02310 |   209.1      True\n",
      "| 67    |  0.02460 |   0.02309 |   212.3      True\n",
      "| 68    |  0.02465 |   0.02305 |   215.5      True\n",
      "| 69    |  0.02450 |   0.02309 |   218.7      False\n",
      "| 70    |  0.02454 |   0.02311 |   221.9      False\n",
      "| 71    |  0.02447 |   0.02302 |   225.2      True\n",
      "| 72    |  0.02444 |   0.02308 |   228.6      False\n",
      "| 73    |  0.02440 |   0.02304 |   231.8      False\n",
      "| 74    |  0.02446 |   0.02298 |   235.0      True\n",
      "| 75    |  0.02441 |   0.02300 |   238.1      False\n",
      "| 76    |  0.02441 |   0.02296 |   241.3      True\n",
      "| 77    |  0.02433 |   0.02306 |   244.5      False\n",
      "| 78    |  0.02434 |   0.02294 |   247.7      True\n",
      "| 79    |  0.02435 |   0.02300 |   250.9      False\n",
      "| 80    |  0.02429 |   0.02306 |   254.0      False\n",
      "| 81    |  0.02432 |   0.02297 |   257.2      False\n",
      "| 82    |  0.02426 |   0.02299 |   260.4      False\n",
      "| 83    |  0.02433 |   0.02298 |   263.5      False\n",
      "| 84    |  0.02430 |   0.02297 |   266.7      False\n",
      "| 85    |  0.02424 |   0.02297 |   269.9      False\n",
      "| 86    |  0.02413 |   0.02293 |   273.0      True\n",
      "| 87    |  0.02419 |   0.02286 |   276.2      True\n",
      "| 88    |  0.02416 |   0.02289 |   279.4      False\n",
      "| 89    |  0.02416 |   0.02289 |   282.7      False\n",
      "| 90    |  0.02416 |   0.02288 |   285.8      False\n",
      "| 91    |  0.02417 |   0.02288 |   289.0      False\n",
      "| 92    |  0.02415 |   0.02294 |   292.1      False\n",
      "| 93    |  0.02414 |   0.02297 |   295.3      False\n",
      "| 94    |  0.02415 |   0.02295 |   298.4      False\n",
      "| 95    |  0.02413 |   0.02293 |   301.6      False\n",
      "| 96    |  0.02422 |   0.02295 |   304.7      False\n",
      "| 97    |  0.02414 |   0.02290 |   307.9      False\n",
      "| 98    |  0.02415 |   0.02289 |   311.0      False\n",
      "| 99    |  0.02413 |   0.02297 |   314.2      False\n",
      "| 100   |  0.02412 |   0.02290 |   317.3      False\n",
      "| 101   |  0.02411 |   0.02290 |   320.4      False\n",
      "| 102   |  0.02416 |   0.02292 |   323.6      False\n",
      "| 103   |  0.02418 |   0.02293 |   326.7      False\n",
      "| 104   |  0.02415 |   0.02291 |   329.9      False\n",
      "| 105   |  0.02418 |   0.02288 |   333.1      False\n",
      "| 106   |  0.02416 |   0.02294 |   336.3      False\n",
      "| 107   |  0.02413 |   0.02293 |   339.5      False\n",
      "| 108   |  0.02411 |   0.02293 |   342.6      False\n",
      "| 109   |  0.02414 |   0.02294 |   345.8      False\n",
      "| 110   |  0.02418 |   0.02288 |   349.0      False\n",
      "| 111   |  0.02413 |   0.02295 |   352.1      False\n",
      "| 112   |  0.02412 |   0.02289 |   355.3      False\n",
      "| 113   |  0.02414 |   0.02291 |   358.4      False\n",
      "| 114   |  0.02412 |   0.02294 |   361.6      False\n",
      "| 115   |  0.02414 |   0.02289 |   364.7      False\n",
      "| 116   |  0.02412 |   0.02290 |   367.9      False\n",
      "| 117   |  0.02417 |   0.02286 |   371.1      False\n",
      "| 118   |  0.02420 |   0.02293 |   374.3      False\n",
      "| 119   |  0.02419 |   0.02292 |   377.7      False\n",
      "| 120   |  0.02419 |   0.02288 |   381.3      False\n",
      "| 121   |  0.02409 |   0.02294 |   384.6      False\n",
      "| 122   |  0.02416 |   0.02296 |   387.8      False\n",
      "| 123   |  0.02419 |   0.02297 |   391.1      False\n",
      "| 124   |  0.02428 |   0.02289 |   394.4      False\n",
      "| 125   |  0.02418 |   0.02292 |   397.8      False\n",
      "| 126   |  0.02420 |   0.02294 |   401.1      False\n",
      "| 127   |  0.02426 |   0.02293 |   404.4      False\n",
      "| 128   |  0.02415 |   0.02289 |   407.7      False\n",
      "| 129   |  0.02409 |   0.02293 |   410.9      False\n",
      "| 130   |  0.02412 |   0.02296 |   414.2      False\n",
      "| 131   |  0.02416 |   0.02296 |   417.5      False\n",
      "| 132   |  0.02418 |   0.02291 |   420.8      False\n",
      "| 133   |  0.02420 |   0.02293 |   424.1      False\n",
      "| 134   |  0.02413 |   0.02291 |   427.4      False\n",
      "| 135   |  0.02414 |   0.02293 |   430.6      False\n",
      "| 136   |  0.02414 |   0.02290 |   433.8      False\n",
      "| 137   |  0.02406 |   0.02293 |   437.0      False\n",
      "Early stopping occured at epoch 137\n",
      "Training done in 436.986 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at TabNet1017/weight/10-17_1225_fold2.zip\n",
      "\n",
      "model saved at: TabNet1017/weight/10-17_1225_fold2\n",
      "\n",
      "[Fold No.  3]\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 50 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     |  0.87600 |   0.70527 |   3.2        True\n",
      "| 2     |  0.76961 |   0.69138 |   6.4        True\n",
      "| 3     |  0.67848 |   0.64492 |   9.6        True\n",
      "| 4     |  0.58584 |   0.56616 |   12.8       True\n",
      "| 5     |  0.48885 |   0.46369 |   16.0       True\n",
      "| 6     |  0.39647 |   0.37549 |   19.2       True\n",
      "| 7     |  0.31876 |   0.29704 |   22.4       True\n",
      "| 8     |  0.24939 |   0.22487 |   25.6       True\n",
      "| 9     |  0.18343 |   0.15977 |   28.8       True\n",
      "| 10    |  0.12724 |   0.11058 |   32.0       True\n",
      "| 11    |  0.08888 |   0.07644 |   35.2       True\n",
      "| 12    |  0.06198 |   0.05272 |   38.4       True\n",
      "| 13    |  0.04577 |   0.04053 |   41.6       True\n",
      "| 14    |  0.03823 |   0.03553 |   44.8       True\n",
      "| 15    |  0.03449 |   0.03189 |   48.0       True\n",
      "| 16    |  0.03253 |   0.03028 |   51.2       True\n",
      "| 17    |  0.03126 |   0.02934 |   54.3       True\n",
      "| 18    |  0.03041 |   0.02841 |   57.5       True\n",
      "| 19    |  0.02974 |   0.02777 |   60.7       True\n",
      "| 20    |  0.02929 |   0.02718 |   63.9       True\n",
      "| 21    |  0.02893 |   0.02695 |   67.0       True\n",
      "| 22    |  0.02854 |   0.02668 |   70.2       True\n",
      "| 23    |  0.02828 |   0.02635 |   73.4       True\n",
      "| 24    |  0.02792 |   0.02587 |   76.6       True\n",
      "| 25    |  0.02780 |   0.02594 |   79.7       False\n",
      "| 26    |  0.02760 |   0.02587 |   82.9       True\n",
      "| 27    |  0.02746 |   0.02554 |   86.1       True\n",
      "| 28    |  0.02740 |   0.02525 |   89.3       True\n",
      "| 29    |  0.02715 |   0.02526 |   92.5       False\n",
      "| 30    |  0.02701 |   0.02540 |   95.6       False\n",
      "| 31    |  0.02686 |   0.02514 |   98.8       True\n",
      "| 32    |  0.02690 |   0.02484 |   102.0      True\n",
      "| 33    |  0.02677 |   0.02507 |   105.1      False\n",
      "| 34    |  0.02667 |   0.02490 |   108.3      False\n",
      "| 35    |  0.02659 |   0.02481 |   111.5      True\n",
      "| 36    |  0.02651 |   0.02461 |   114.6      True\n",
      "| 37    |  0.02637 |   0.02470 |   117.8      False\n",
      "| 38    |  0.02632 |   0.02460 |   120.9      True\n",
      "| 39    |  0.02623 |   0.02460 |   124.1      False\n",
      "| 40    |  0.02625 |   0.02451 |   127.3      True\n",
      "| 41    |  0.02610 |   0.02450 |   130.5      True\n",
      "| 42    |  0.02604 |   0.02448 |   133.7      True\n",
      "| 43    |  0.02602 |   0.02447 |   136.8      True\n",
      "| 44    |  0.02601 |   0.02441 |   140.0      True\n",
      "| 45    |  0.02585 |   0.02443 |   143.2      False\n",
      "| 46    |  0.02577 |   0.02427 |   146.4      True\n",
      "| 47    |  0.02580 |   0.02419 |   149.5      True\n",
      "| 48    |  0.02569 |   0.02412 |   152.8      True\n",
      "| 49    |  0.02571 |   0.02407 |   156.0      True\n",
      "| 50    |  0.02551 |   0.02412 |   159.1      False\n",
      "| 51    |  0.02552 |   0.02401 |   162.4      True\n",
      "| 52    |  0.02549 |   0.02397 |   165.5      True\n",
      "| 53    |  0.02537 |   0.02377 |   168.7      True\n",
      "| 54    |  0.02539 |   0.02373 |   171.9      True\n",
      "| 55    |  0.02528 |   0.02363 |   175.2      True\n",
      "| 56    |  0.02521 |   0.02373 |   178.4      False\n",
      "| 57    |  0.02526 |   0.02353 |   181.7      True\n",
      "| 58    |  0.02517 |   0.02359 |   184.9      False\n",
      "| 59    |  0.02521 |   0.02361 |   188.2      False\n",
      "| 60    |  0.02513 |   0.02354 |   191.4      False\n",
      "| 61    |  0.02509 |   0.02358 |   194.7      False\n",
      "| 62    |  0.02515 |   0.02364 |   197.9      False\n",
      "| 63    |  0.02498 |   0.02366 |   201.1      False\n",
      "| 64    |  0.02508 |   0.02343 |   204.4      True\n",
      "| 65    |  0.02501 |   0.02350 |   207.7      False\n",
      "| 66    |  0.02505 |   0.02357 |   210.9      False\n",
      "| 67    |  0.02492 |   0.02344 |   214.1      False\n",
      "| 68    |  0.02485 |   0.02329 |   217.4      True\n",
      "| 69    |  0.02482 |   0.02337 |   220.6      False\n",
      "| 70    |  0.02480 |   0.02341 |   223.9      False\n",
      "| 71    |  0.02474 |   0.02324 |   227.2      True\n",
      "| 72    |  0.02475 |   0.02340 |   230.4      False\n",
      "| 73    |  0.02471 |   0.02338 |   233.7      False\n",
      "| 74    |  0.02467 |   0.02323 |   236.9      True\n",
      "| 75    |  0.02460 |   0.02332 |   240.1      False\n",
      "| 76    |  0.02460 |   0.02323 |   243.4      True\n",
      "| 77    |  0.02458 |   0.02329 |   246.6      False\n",
      "| 78    |  0.02454 |   0.02323 |   249.9      False\n",
      "| 79    |  0.02459 |   0.02334 |   253.1      False\n",
      "| 80    |  0.02449 |   0.02322 |   256.4      True\n",
      "| 81    |  0.02449 |   0.02319 |   259.6      True\n",
      "| 82    |  0.02451 |   0.02310 |   262.9      True\n",
      "| 83    |  0.02449 |   0.02317 |   266.1      False\n",
      "| 84    |  0.02441 |   0.02321 |   269.4      False\n",
      "| 85    |  0.02441 |   0.02317 |   272.7      False\n",
      "| 86    |  0.02444 |   0.02319 |   275.9      False\n",
      "| 87    |  0.02429 |   0.02326 |   279.1      False\n",
      "| 88    |  0.02434 |   0.02304 |   282.4      True\n",
      "| 89    |  0.02441 |   0.02310 |   285.6      False\n",
      "| 90    |  0.02424 |   0.02306 |   288.9      False\n",
      "| 91    |  0.02436 |   0.02314 |   292.2      False\n",
      "| 92    |  0.02432 |   0.02323 |   295.5      False\n",
      "| 93    |  0.02435 |   0.02319 |   298.7      False\n",
      "| 94    |  0.02438 |   0.02316 |   302.0      False\n",
      "| 95    |  0.02437 |   0.02311 |   305.2      False\n",
      "| 96    |  0.02436 |   0.02324 |   308.5      False\n",
      "| 97    |  0.02430 |   0.02314 |   311.9      False\n",
      "| 98    |  0.02429 |   0.02315 |   315.1      False\n",
      "| 99    |  0.02442 |   0.02316 |   318.4      False\n",
      "| 100   |  0.02436 |   0.02320 |   321.6      False\n",
      "| 101   |  0.02432 |   0.02315 |   324.9      False\n",
      "| 102   |  0.02431 |   0.02321 |   328.2      False\n",
      "| 103   |  0.02433 |   0.02316 |   331.4      False\n",
      "| 104   |  0.02427 |   0.02323 |   334.7      False\n",
      "| 105   |  0.02437 |   0.02316 |   337.9      False\n",
      "| 106   |  0.02430 |   0.02320 |   341.1      False\n",
      "| 107   |  0.02435 |   0.02315 |   344.4      False\n",
      "| 108   |  0.02432 |   0.02317 |   347.6      False\n",
      "| 109   |  0.02429 |   0.02312 |   350.9      False\n",
      "| 110   |  0.02437 |   0.02315 |   354.1      False\n",
      "| 111   |  0.02431 |   0.02317 |   357.3      False\n",
      "| 112   |  0.02429 |   0.02319 |   360.6      False\n",
      "| 113   |  0.02434 |   0.02313 |   363.8      False\n",
      "| 114   |  0.02433 |   0.02324 |   367.1      False\n",
      "| 115   |  0.02434 |   0.02311 |   370.3      False\n",
      "| 116   |  0.02433 |   0.02323 |   373.5      False\n",
      "| 117   |  0.02437 |   0.02314 |   376.8      False\n",
      "| 118   |  0.02434 |   0.02323 |   380.0      False\n",
      "| 119   |  0.02438 |   0.02321 |   383.3      False\n",
      "| 120   |  0.02429 |   0.02316 |   386.5      False\n",
      "| 121   |  0.02430 |   0.02318 |   389.8      False\n",
      "| 122   |  0.02430 |   0.02322 |   393.0      False\n",
      "| 123   |  0.02429 |   0.02321 |   396.3      False\n",
      "| 124   |  0.02435 |   0.02321 |   399.5      False\n",
      "| 125   |  0.02427 |   0.02316 |   402.8      False\n",
      "| 126   |  0.02440 |   0.02316 |   406.0      False\n",
      "| 127   |  0.02437 |   0.02315 |   409.3      False\n",
      "| 128   |  0.02432 |   0.02319 |   412.5      False\n",
      "| 129   |  0.02438 |   0.02320 |   415.8      False\n",
      "| 130   |  0.02432 |   0.02321 |   419.0      False\n",
      "| 131   |  0.02437 |   0.02309 |   422.3      False\n",
      "| 132   |  0.02431 |   0.02318 |   425.5      False\n",
      "| 133   |  0.02432 |   0.02316 |   428.7      False\n",
      "| 134   |  0.02433 |   0.02311 |   432.0      False\n",
      "| 135   |  0.02430 |   0.02312 |   435.2      False\n",
      "| 136   |  0.02442 |   0.02317 |   438.4      False\n",
      "| 137   |  0.02428 |   0.02319 |   441.7      False\n",
      "| 138   |  0.02428 |   0.02316 |   444.9      False\n",
      "Early stopping occured at epoch 138\n",
      "Training done in 444.929 seconds.\n",
      "---------------------------------------\n",
      "Successfully saved model at TabNet1017/weight/10-17_1225_fold3.zip\n",
      "\n",
      "model saved at: TabNet1017/weight/10-17_1225_fold3\n"
     ]
    }
   ],
   "source": [
    "for fold in range(4):\n",
    "    run_training(df, fold, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELNAME}/{now}_params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"nn_params\": {\"dropout\": trial.suggest_uniform(\"dropout\", 0.1, 0.8), \n",
    "                      \"num_layers\": trial.suggest_int(\"num_layers\", 1, 7),\n",
    "                      \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 2048),\n",
    "                      \"activation\": trial.suggest_categorical(\"activation\", [\"relu\", \"prelu\"]),\n",
    "                      \"batchnorm\": trial.suggest_categorical(\"batchnorm\", [True, False])},\n",
    "        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\"]),\n",
    "        \"optim_params\": {\"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3)},\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"scdl_params\": {\"threshold\": 0.00001},\n",
    "    }\n",
    "    loss_all = []\n",
    "    for fold_ in range(4):\n",
    "        loss_tmp = run_training(df, fold, params, save_model=False)\n",
    "        loss_all.append(loss_tmp)\n",
    "    return np.mean(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
