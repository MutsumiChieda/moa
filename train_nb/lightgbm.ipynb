{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../script/')\n",
    "import os\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "import optuna\n",
    "\n",
    "import utils\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "EPOCHS = 50000\n",
    "MODELNAME = \"LightGBM1017\"\n",
    "if not exists(f\"{MODELNAME}/scores\"):\n",
    "    os.makedirs(f\"{MODELNAME}/scores\")\n",
    "if not exists(f\"{MODELNAME}/weight\"):\n",
    "    os.makedirs(f\"{MODELNAME}/weight\")\n",
    "now = datetime.now()\n",
    "now = str(now)[5:17].replace(\" \", \"_\").replace(\":\", \"\")\n",
    "# writer = SummaryWriter(log_dir=f\"{MODELNAME}/tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data\n",
      "Skipped: already exists\n"
     ]
    }
   ],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "def split_data():\n",
    "    print(\"Split data\")\n",
    "    path_fold = \"../input/folds/train_folds.csv\"\n",
    "    if not exists(path_fold):\n",
    "        df = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\n",
    "        df.loc[:, \"kfold\"] = -1\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        targets = df.drop(\"sig_id\", axis=1).values\n",
    "\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "        for fold_, (tr_, va_) in enumerate(mskf.split(X=df, y=targets)):\n",
    "            df.loc[va_, \"kfold\"] = fold_\n",
    "        df.to_csv(path_fold, index=False)\n",
    "        print(f\"Created: {path_fold}\")\n",
    "    else:\n",
    "        print(\"Skipped: already exists\")\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    df = pd.read_csv(\"../input/lish-moa/train_features.csv\")\n",
    "    \n",
    "    # Label encoding\n",
    "    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n",
    "               \"cp_time\":{24:0, 48:1, 72:2},\n",
    "               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n",
    "    for col in ['cp_type', 'cp_time', 'cp_dose']:\n",
    "        df[col] = df[col].map(mapping[col])\n",
    "    \n",
    "    folds = pd.read_csv(\"../input/folds/train_folds.csv\")\n",
    "\n",
    "    # Create aux target\n",
    "    # `nsc_labels` means # of labels found in non-scored train set\n",
    "    non_scored_df = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\n",
    "    targets_non_scored = non_scored_df.drop(\"sig_id\", axis=1).to_numpy().sum(axis=1)\n",
    "    non_scored_df.loc[:, \"nsc_labels\"] = targets_non_scored\n",
    "    drop_cols = [c for c in non_scored_df.columns if c not in (\"nsc_labels\", \"sig_id\")]\n",
    "    non_scored_df = non_scored_df.drop(drop_cols, axis=1)\n",
    "    folds = folds.merge(non_scored_df, on=\"sig_id\", how=\"left\")\n",
    "\n",
    "    targets = folds.drop([\"sig_id\", \"kfold\"], axis=1).columns\n",
    "    features = df.drop(\"sig_id\", axis=1).columns\n",
    "    df = df.merge(folds, on=\"sig_id\", how=\"left\")\n",
    "    \n",
    "    return df, features, targets\n",
    "\n",
    "split_data()\n",
    "df, features, targets = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, fold, params, hp_tune=False):\n",
    "\n",
    "    save_model = False if hp_tune else True\n",
    "    print(f'\\n[Fold No.{fold:>3}]')\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    x_tr = train_df[features].to_numpy()\n",
    "    x_va = valid_df[features].to_numpy()\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    loss_tr, loss_va = [], []\n",
    "    for i, target in enumerate(targets):\n",
    "        print(f'Target No.{i:>3} / {len(targets):>3}',end='\\r')\n",
    "        y_tr = train_df[target].to_numpy()\n",
    "        y_va = valid_df[target].to_numpy()\n",
    "        dataset_tr = lgb.Dataset(x_tr, label=y_tr)\n",
    "        dataset_va = lgb.Dataset(x_va, label=y_va, reference=dataset_tr)\n",
    "\n",
    "        model = lgb.train(params, dataset_tr, EPOCHS, valid_sets=[dataset_tr, dataset_va],\n",
    "                          verbose_eval=False, early_stopping_rounds=100)\n",
    "\n",
    "        filename = f\"{MODELNAME}/weight/tgt{i}_{now}_fold{fold}.txt\"\n",
    "\n",
    "        preds_tr = model.predict(x_tr, num_iteration=model.best_iteration)\n",
    "        preds_va = model.predict(x_va, num_iteration=model.best_iteration)\n",
    "\n",
    "        loss_tr.append(log_loss(y_tr, preds_tr, labels=[0,1]).item())\n",
    "        loss_va.append(log_loss(y_va, preds_va, labels=[0,1]).item())\n",
    "\n",
    "        if save_model:\n",
    "            model.save_model(filename, num_iteration=model.best_iteration)\n",
    "\n",
    "    with open(f\"{MODELNAME}/scores/{now}.txt\", \"a\") as f:\n",
    "        f.write(f\"[fold{fold:>2}] {np.mean(loss_tr):.5}, {np.mean(loss_va):.5}\\n\")\n",
    "    print()\n",
    "\n",
    "    print(\"\\nmodel saved at:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"metric\": 'binary_logloss',\n",
    "          'objective': 'binary',\n",
    "          'num_leaves': 491,\n",
    "          'min_child_weight': 0.03,\n",
    "          'feature_fraction': 0.3,\n",
    "          'bagging_fraction': 0.4,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"verbose\": -1,\n",
    "          'reg_alpha': 0.4,\n",
    "          'reg_lambda': 0.6,\n",
    "          'random_state': 47,\n",
    "          \"force_col_wise\": True\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold No.  0]\n",
      "Target No.206 / 207\n",
      "\n",
      "model saved at: LightGBM1017/weight/tgt206_10-18_1159_fold0.txt\n",
      "\n",
      "[Fold No.  1]\n",
      "Target No.206 / 207\n",
      "\n",
      "model saved at: LightGBM1017/weight/tgt206_10-18_1159_fold1.txt\n",
      "\n",
      "[Fold No.  2]\n",
      "Target No.206 / 207\n",
      "\n",
      "model saved at: LightGBM1017/weight/tgt206_10-18_1159_fold2.txt\n",
      "\n",
      "[Fold No.  3]\n",
      "Target No.206 / 207\n",
      "\n",
      "model saved at: LightGBM1017/weight/tgt206_10-18_1159_fold3.txt\n"
     ]
    }
   ],
   "source": [
    "for fold in range(4):\n",
    "    run_training(df, fold, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODELNAME}/{now}_params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"nn_params\": {\"dropout\": trial.suggest_uniform(\"dropout\", 0.1, 0.8), \n",
    "                      \"num_layers\": trial.suggest_int(\"num_layers\", 1, 7),\n",
    "                      \"hidden_size\": trial.suggest_int(\"hidden_size\", 16, 2048),\n",
    "                      \"activation\": trial.suggest_categorical(\"activation\", [\"relu\", \"prelu\"]),\n",
    "                      \"batchnorm\": trial.suggest_categorical(\"batchnorm\", [True, False])},\n",
    "        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\"]),\n",
    "        \"optim_params\": {\"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3)},\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"scdl_params\": {\"threshold\": 0.00001},\n",
    "    }\n",
    "    loss_all = []\n",
    "    for fold_ in range(4):\n",
    "        loss_tmp = run_training(df, fold, params, save_model=False)\n",
    "        loss_all.append(loss_tmp)\n",
    "    return np.mean(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
